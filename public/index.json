[{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211119132529/\n相关话题：https://www.cnsre.cn/tags/k3s/\n双十一薅了几个云厂商的羊毛，一开始搭建了k3s的单机版，后面就想着能不能搭建一个k3s集群，然后参考了这位大佬的文章，就试着用 WireGuard 来进行组网。它轻量、便捷、高效，而且数据全程加密传输，是依托公网组建虚拟局域网的优秀选择。\n环境介绍 服务器介绍\n云厂商 公网IP地址 内网IP地址 虚拟网络IP地址 操作系统 内核版本 腾讯云1 42.xx.xx.12 10.0.16.8 192.168.1.1 CentOS Linux release 7.9.2009 (Core) 5.15.2-1 腾讯云2 122.xx.xxx.111 10.0.0.6 192.168.1.2 CentOS Linux release 7.9.2009 (Core) 5.15.2-1 阿里云 122.xx.xx.155 172.17.0.3 192.168.1.3 CentOS Linux release 7.9.2009 (Core) 5.15.2-1 搭建前准备 在搭建跨云的 k3s 集群前，我们需要把 WireGuard 安装好，WireGuard 对内核是有要求的，所以内核已经要升级到 5.15.2-1.el7.elrepo.x86_64\n在所有节点开启 IP 地址转发 1 2 3 echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026#34;net.ipv4.conf.all.proxy_arp = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p /etc/sysctl.conf 所有节点开启修改主机名称 1 2 3 4 5 6 # 腾讯云1执行 hostnamectl set-hostname k3s-master # 腾讯云2执行 hostnamectl set-hostname k3s-node1 # 阿里云执行 hostnamectl set-hostname k3s-node2 升级内核 几个服务器默认的内核都是 3.10 的，安装WireGuard 需要吧内核升级到比较高的版本。\n升级内核前 先升级软件包（非必要）\n1 yum update -y 添加 iptables 规则，允许本机的 NAT 转换：\n所有节点都要执行 1 2 3 4 iptables -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT iptables -A FORWARD -i wg0 -o wg0 -m conntrack --ctstate NEW -j ACCEPT iptables -t nat -A POSTROUTING -s 192.168.1.1/24 -o eth0 -j MASQUERADE wg0:为你定义的虚拟网卡\n192.168.1.1: 为你的虚拟IP地址段\neth0:为你的物理网卡\n升级内核 所有节点都要执行 方法1 方法2 方法3 方法1：\n直接下载RPM包进行安装。\n如果你想安装其他内核，你也可以在 这里下载\n1 2 wget https://pan.cnsre.cn/d/Package/Linux/kernel/kernel-ml-5.15.2-1.el7.elrepo.x86_64.rpm rpm -ivh kernel-ml-5.15.2-1.el7.elrepo.x86_64.rpm 方法 2：\n利用包管理工具更新\n1 2 3 4 5 6 7 8 9 10 # 载入公钥 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 升级安装 elrepo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-5.el7.elrepo.noarch.rpm # 载入 elrepo-kernel 元数据 yum --disablerepo=\\* --enablerepo=elrepo-kernel repolist # 安装最新版本的内核 yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y # 删除旧版本工具包 yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y 方法3：\n通过源码包编译安装.\n这种方式可定制性强，但也比较复杂，有需要的可自行查找资料安装，下面只给出各系统版本内核源码包的下载地址\n修改默认内核版本 1 2 3 4 5 6 7 8 9 10 11 12 # 查看当前实际启动顺序 grub2-editenv list # 查看内核插入顺序 grep \u0026#34;^menuentry\u0026#34; /boot/grub2/grub.cfg | cut -d \u0026#34;\u0026#39;\u0026#34; -f2 # 设置默认启动 grub2-set-default \u0026#39;CentOS Linux (5.15.2-1.el7.elrepo.x86_64) 7 (Core)\u0026#39; # 重新创建内核配置 grub2-mkconfig -o /boot/grub2/grub.cfg # 重启服务器 reboot # 验证当前内核版本 uname -r 内核版本一定要是比较高的，不然启动WireGuard会有如下报错。\n1 2 3 4 5 [#] ip link add wg0 type wireguard RTNETLINK answers: Operation not supported Unable to access interface: Protocol not supported [#] ip link delete dev wg0 Cannot find device \u0026#34;wg0\u0026#34; 安装 WireGuard 所有节点执行 安装流程非常简单，我这里是直接将 CentOS 内核更新到目前最新的 5.15.2 版本，其中就已经包含了 WireGuard 的内核模块，只需要安装 wireguard-tools 这个 yum 包就行了。\n1 2 yum install epel-release https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm yum install yum-plugin-elrepo kmod-wireguard wireguard-tools -y 配置 WireGuard wireguard-tools 包提供了我们所需的工具 wg 和 wg-quick，可以使用它们来分别完成手动部署和自动部署。\n先按照官方文档描述的形式，生成腾讯云1用于加密解密的密钥\n1 wg genkey | tee privatekey | wg pubkey \u0026gt; publickey 然后在当前目录下就生成了 privatekey 和 publickey 两个文件\n密钥是配置到本机的，而公钥是配置到其它机器里的。 1 2 3 cat privatekey publickey EMWcI01iqM4zkb7xfbaaxxxxxxxxDo2GJUA= 0ay8WfGOIHndWklSIVBqrsp5LDWxxxxxxxxxxxxxxQ= 现在我们需要与上述主机对等联网的 腾讯云2 阿里云 ，其公网IP（这边需要填写的是能与主机通信的IP）是 122.xx.xxx.111，122.xx.xx.155\n我们首先依照上面的流程安装 WireGuard 并生成好 腾讯云2 阿里云 的密钥。\n然后编写 腾讯云1 完整的配置文件，以供 wg-quick 使用，在主机A的 /etc/wireguard/wg0.conf 中写入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Interface] PrivateKey = EMWcI01iqM4zkb7xfbaaxxxxxxxxDo2GJUA= Address = 192.168.1.1 ListenPort = 5418 [Peer] PublicKey = 腾讯云2 publickey EndPoint = 122.xx.xxx.111:5418 AllowedIPs = 192.168.1.2/32 [Peer] PublicKey = 阿里云publickey EndPoint = 122.xx.xx.155:5418 AllowedIPs = 192.168.1.3/32 配置说明 Interface: 小节是属于腾讯云1（也就是本机）的配置.\nAddress: 是分配给 腾讯云1 虚拟IP，\nListenPort: 是主机之间通讯使用的端口，是 UDP 协议的。\nPeer: 是属于需要通信的 腾讯云2 、阿里云 的信息，有多少需要通信的主机，就添加多少个 Peer 小节。\nEndPoint: 是 腾讯云2 、阿里云的公网IP与 WireGuard 监听的 UDP 端口，这个 IP 不一定是公网，\n如果你的机器通过内网也能通信，直接用内网 IP 也可以，当然要注意这个IP需要所有加入局域网的主机都能通信才行。 AllowedIPs: 是指本机发起连接的哪些IP应该将流量转发到这个节点去，比如我们给主机B分配了内网IP 192.168.1.2，那么在主机A上发送到 192.168.1.2 的数据包，都应该转发到这个 EndPoint 上，它其实起的是一个过滤作用。而且多个 Peer 时，这里配置的IP地址不能有冲突。\n各个节点生产的 privatekey 和publickey 分别如下\n1 2 3 4 5 6 7 8 9 10 11 12 # master 节点 [root@k3s-master ~]# cat privatekey publickey EMWcI01iqM4zkb7xfbaaxxxxxxxxDo2GJUA= 0ay8WfGOIHndWklSIVBqrsp5LDWxxxxxxxxxxxxxxQ= # node1 节点 [root@k3s-node1 ~]# cat privatekey publickey QGdNkzpnIkuvUU+00C6XYxxxxxxxxxK0D82qJVc= 3izpVbZgPhlM+S5szOogTDTxxxxxxxxxuKuDGn4= # node2 节点 [root@k3s-node2 ~]# cat privatekey publickey WOOObkWINkW/hqaAME9r+xxxxxxxxxm+r2Q= 0f0dn60+tBUfYgzw7rIihKbqxxxxxxxxa6Wo= 各节点配置 master配置 node1配置 node2配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # master 节点 cat /etc/wireguard/wg0.conf [Interface] PrivateKey = EMWcI01iqM4zkb7xfbaaxxxxxxxxDo2GJUA= Address = 192.168.1.1 ListenPort = 5418 [Peer] PublicKey = 3izpVbZgPhlM+S5szOogTDTxxxxxxxxxuKuDGn4= EndPoint = 122.xx.xxx.111:5418 AllowedIPs = 192.168.1.2/32 [Peer] PublicKey = 0f0dn60+tBUfYgzw7rIihKbqxxxxxxxxa6Wo= EndPoint = 122.xx.xx.155:5418 AllowedIPs = 192.168.1.3/32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # node1 节点 cat /etc/wireguard/wg0.conf [Interface] PrivateKey = QGdNkzpnIkuvUU+00C6XYxxxxxxxxxK0D82qJVc= Address = 192.168.1.2 ListenPort = 5418 [Peer] PublicKey = 0ay8WfGOIHndWklSIVBqrsp5LDWxxxxxxxxxxxxxxQ= EndPoint = 42.xxx.xx.16:5418 AllowedIPs = 192.168.1.1/32 [Peer] PublicKey = 0f0dn60+tBUfYgzw7rIihKbqxxxxxxxxa6Wo= EndPoint = 122.xx.xx.155:5418 AllowedIPs = 192.168.1.3/32 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # node2 节点 cat /etc/wireguard/wg0.conf [Interface] PrivateKey = WOOObkWINkW/hqaAME9r+xxxxxxxxxm+r2Q= Address = 192.168.1.3 ListenPort = 5418 [Peer] PublicKey = 0ay8WfGOIHndWklSIVBqrsp5LDWxxxxxxxxxxxxxxQ= EndPoint = 42.xxx.xx.16:5418 AllowedIPs = 192.168.1.1/32 [Peer] PublicKey = 3izpVbZgPhlM+S5szOogTDTxxxxxxxxxuKuDGn4= EndPoint = 122.xx.xx.155:5418 AllowedIPs = 192.168.1.2/32 启动 WireGuard 配置文件写好后，使用 wg-quick 工具来创建虚拟网卡，\n1 wg-quick up wg0 上面命令中的 wg0 对应的是 /etc/wireguard/wg0.conf 这个配置文件，其自动创建的网卡设备，名字就是 wg0，这对应关系自不必多言。\n将腾讯云2 、阿里云 的网卡设备都安装配置好后，就能使用 wg 命令来观察组网情况了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@k3s-master ~]# wg interface: wg0 public key: 0ay8WfGOIHndWklSIVBqrsp5LDWxxxxxxxxxxxxxxQ= private key: (hidden) listening port: 5418 peer: 0f0dn60+tBUfYgzw7rIihKbqxxxxxxxxa6Wo= endpoint: 122.xx.xx.155:5418 allowed ips: 192.168.1.3/32 latest handshake: 3 minutes, 3 seconds ago transfer: 35.40 KiB received, 47.46 KiB sent peer: 3izpVbZgPhlM+S5szOogTDTxxxxxxxxxuKuDGn4= endpoint: 122.xx.xxx.111:5418 allowed ips: 192.168.1.2/32 latest handshake: 5 minutes, 6 seconds ago transfer: 24.84 KiB received, 35.21 KiB sent 可以看到列出了对等联网的节点信息，还有通信测量数据。然后可以通过 ping 其他主机的虚拟IP或者 ssh 其他主机的IP地址，来检查网络通信是否正常。\n自动化 系统重启后，WireGuard 创建的网卡设备就会丢失，有自动化的脚本\n1 systemctl enable wg-quick@wg0 使用上述命令生成systemd守护脚本，开机会自动运行up指令。\n配置热重载 wg-quick并未提供重载相关的指令，但是提供了 strip 指令，可以将 conf 文件转换为 wg 指令可以识别的格式。\n1 wg syncconf wg0 \u0026lt;(wg-quick strip wg0) 即可实现热重载。\n完成 WireGuard 的安装配置以后，我们就可以接下来安装 k3s 的集群了。\n安装 K3S 集群 master节点安装 1 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --node-external-ip 42.xx.xx.12--advertise-address 42.xx.xx.12--node-ip 192.168.1.1 --flannel-iface wg0 参数说明：\n--node-external-ip 42.xxx.xx.16 为节点设置外部IP，阿里云VPC的外网IP并未直接绑定到虚拟机网卡上，所以我要设置这个参数，避免k3s组件在设置loadbalance时，将内网IP当作公网IP使用。 --advertise-address 42.xxx.xx.16 用于设置kubectl工具以及子节点进行通讯使用的地址，可以是IP，也可以是域名，在创建apiserver证书时会将此设置到有效域中。 --node-ip 10.20.30.1 如果不设置这个参数，那么第一张网卡设备上的IP就会被选中，所以这个IP常是内网IP。但我自行组建了虚拟局域网，所以需要指定虚拟局域网的IP（也就是WireGuard的IP）。 --flannel-iface wg0 wg0是WireGuard创建的网卡设备，我需要使用虚拟局域网来进行节点间的通信，所以这里需要指定为wg0。 另外就是，由于WireGuard的所有流量都是加密传输的，通过它来进行节点间的通信，就已经能够保证通信安全，也就没有必要改用其它的CNI驱动，使用默认的就可以了。\n在主节点执行上述命令后，一分钟不到就可以看到脚本提示安装完成。通过命令查看下主控端的运行情况\n1 systemctl status k3s 如果运行正常，那么就看看容器的运行状态是否正常\n1 kubectl get pods -A -A 参数用于查看所有命名空间，如果容器都处于 running 状态，那么安装就成功了，接下来要可以添加被控节点。\nAgent 安装 有了上述安装主控的经验，安装work节点更加简单，参数需要一定的调整\n腾讯云2执行 阿里云执行 腾讯云2执行\n1 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.1.1:6443 K3S_TOKEN=K107xxxxxxxxxxxxxxxx2cf95048d6a3cd85f15717edfbe5::server:xxxxxxxxxxxxxxxxxxxx4da1b7e701f67e sh -s - --node-external-ip 122.xx.xxx.111 --node-ip 192.168.1.2 --flannel-iface wg0 阿里云执行\n1 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.1.1:6443 K3S_TOKEN=K10720eda8a278bdc7b9b6d787c9676a92119bb2cf95048d6a3cd85f15717edfbe5::server:e98b986e8202885cb54da1b7e701f67e sh -s - --node-external-ip 122.xx.xx.155 --node-ip 192.168.1.3 --flannel-iface wg0 参数不必过多解释\nK3S_Token 去 cat /var/lib/rancher/k3s/server/node-token 获取即可。 K3S_URL 需要设置master的通信地址端口，端口默认是6443，IP地址就是虚拟网域的IP，这样流量就会通过WireGuard加密传输。 node-external-ip 为节点公网地址 node-ip 节点虚拟IP地址\n执行完稍等一会，安装成功后，查看服务运行状态。 1 systemctl status k3s-agent 如果有报错就根据报错查找解决方案。\n都安装好以后 在master节点检查。\n1 kubectl get nodes -o wide 至此 多云 K3S 集群已经搭建完毕。\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211119132529/\n相关话题：https://www.cnsre.cn/tags/k3s/\n","description":"保姆级教学。手把手教把多个云厂商的服务器快速搭建一个k3s集群|多云搭建K3S集群|公网搭建K3S集群|跨云搭建K3S集群","id":0,"section":"posts","tags":["k3s","kubernetes","wireguard"],"title":"多云搭建 K3S 集群","uri":"https://www.cnsre.cn/posts/211119132529/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/240909092827/\n相关话题：https://www.cnsre.cn/tags/eks/\n如何解决 Amazon EKS 中 Fargate 无法访问公网的问题 背景介绍 在使用 EKS + Fargate 创建服务的测试过程中，遇到了 Fargate 中的 Pod 无法访问公网的问题。该案例中，Fargate 和 EC2 实例使用了相同的子网配置，然而 Fargate Pod 无法访问公网，而 EC2 Pod 可以正常访问。本文将深入分析该问题的原因，并提供相应的解决方案。\n案例描述 我们在 AWS 环境中配置了一个 EKS 集群，其中既包含 EC2 节点的 Pod，也包含 Fargate 节点的 Pod。EC2 节点的 Pod 可以正常访问公网，但 Fargate 节点的 Pod 却出现了无法解析域名、访问超时的情况。\n关键问题 在我们尝试使用 Fargate Pod 访问公网时，使用 kubectl exec 进入 Fargate Pod 后，尝试执行 apt-get update 以更新系统包，但出现了如下错误：\n1 0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] ^C 这表明 Fargate Pod 无法访问外部网络，而 EC2 节点的 Pod 则可以正常执行命令、连接公网。\n分析与调查 经过 AWS 技术支持的深入分析，问题的根源在于 安全组配置。Fargate 使用的 Pod 默认通过 CoreDNS 解析域名，当我们使用 dig 命令解析域名时，返回了超时错误。这说明 Fargate Pod 的 DNS 请求未能正确处理。\n在检查我们的 EC2 节点时，发现该节点的安全组仅放行了来自 Fargate 的 TCP 请求。然而，DNS 请求通常使用 UDP 协议，因此在 Fargate Pod 尝试进行 DNS 解析时，由于安全组未放行 UDP 流量，导致了超时。\n解决方案 为了解决这一问题，AWS 技术支持建议修改 EC2 节点的安全组配置。通过放行来自 集群安全组 的所有流量，包括 TCP 和 UDP 请求，确保 Fargate Pod 能够正常进行 DNS 解析和公网访问。\n步骤如下： 登录 AWS 控制台，进入 EC2 管理页面。 选择对应的 EC2 实例，并查看其附加的安全组。 修改安全组规则，确保放行来自 Fargate 的所有流量，特别是 UDP 53 端口，用于 DNS 请求。 保存更改后，重新测试 Fargate Pod 的公网访问能力。 结果验证 在修改安全组后，我们的 Fargate Pod 可以正常访问公网，使用 apt-get update 命令成功连接到外部源。这说明问题已彻底解决。\nFargate Pod 成功连接公网输出 1 2 3 4 5 root@awslinux-fargate-pod:/# apt-get update Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB] Fetched 114 kB in 1s (150 kB/s) Reading package lists... Done 通过这些配置修改，Fargate Pod 的网络访问问题得到了彻底解决，我们的生产环境也恢复了正常运行。\n常见问题与预防 为什么 Fargate 和 EC2 的网络行为不同？ Fargate 和 EC2 节点虽然可以共享同一个子网，但它们在网络层面的实现存在差异。Fargate 使用 AWS 提供的托管网络，因此其默认行为与 EC2 节点有所不同。在配置安全组时，特别需要注意两者的网络规则是否兼容。\n如何避免类似问题？ 为了避免类似的网络问题，建议在配置 EKS 集群时，仔细检查所有节点（包括 Fargate 和 EC2）的 安全组配置 和 网络规则。确保放行集群内部所需的所有流量，尤其是 UDP 53 端口，以支持 DNS 请求的正常运行。\n注意：\n为了能够保证Fargate 和EC2 node 的通信，尽量打通集群安全组，和 EC2 node 的安全组的通讯问题。即 EC2 node 安全组全部放行可以访问集群安全组，集群安全组全部放行可以访问EC2 node。 结论 通过本文，我们深入探讨了 Amazon EKS 环境中 Fargate Pod 无法访问公网的问题及其解决方案。这一问题主要是由于 安全组未放行 UDP 流量 导致的，通过适当的安全组配置调整，问题得以顺利解决。如果您在使用 AWS EKS 时遇到类似的网络问题，建议从安全组配置入手，确保所有必要的流量得到了正确放行。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/240909092827/\n相关话题：https://www.cnsre.cn/tags/eks/\n","description":"解决Amazon EKS 中 Fargate 无法访问公网的问题","id":1,"section":"posts","tags":["fargate","eks","aws","kubernetes"],"title":"如何解决 Amazon EKS 中 Fargate 无法访问公网的问题","uri":"https://www.cnsre.cn/posts/240909092827/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/240906085714/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n实现Kubernetes跨Namespace Service访问的最佳实践 在Kubernetes运维场景中，经常会遇到跨Namespace（名称空间）的Service访问需求。为了实现不同Namespace下的Pod能够通过Service名称进行跨Namespace访问，我们可以利用Kubernetes的ExternalName类型Service来解决这个问题。\n在本文中，我将展示如何配置和实现这种跨Namespace的通信。我们将通过一个完整的示例来详细说明配置过程和关键步骤。\n1. 场景需求 假设我们有两个不同Namespace下的两个Service和Pod：\nNamespace A：ServiceA (PodA) Namespace B：ServiceB (PodB) 我们希望实现PodA通过ServiceB的名称进行通信，而不是通过Service的IP地址。由于Service的IP地址可能会在重启时发生变化，而Service的名称则保持不变，这样可以确保通信的稳定性。\n2. 创建Service和Pod 首先，我们需要为两个Namespace分别创建对应的Pod和Service。\n在myns Namespace中创建Pod和Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion: v1 kind: Namespace metadata: name: myns --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deploy1 namespace: myns spec: replicas: 2 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 ports: - name: http containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp-clusterip1 namespace: myns spec: type: ClusterIP selector: app: myapp release: v1 ports: - name: http port: 80 targetPort: 80 在mytest Namespace中创建Pod和Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 apiVersion: v1 kind: Namespace metadata: name: mytest --- apiVersion: apps/v1 kind: Deployment metadata: name: myapp-deploy2 namespace: mytest spec: replicas: 2 selector: matchLabels: app: myapp release: v2 template: metadata: labels: app: myapp release: v2 spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 ports: - name: http containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: myapp-clusterip2 namespace: mytest spec: type: ClusterIP selector: app: myapp release: v2 ports: - name: http port: 80 targetPort: 80 3. 检查创建结果 运行以下命令来应用这些YAML文件并检查Service和Pod的状态：\n1 2 3 4 5 6 kubectl apply -f deply_service_myns.yaml kubectl apply -f deply_service_mytest.yaml kubectl get svc -n myns -o wide kubectl get svc -n mytest -o wide kubectl get pod -A -o wide 4. 跨Namespace Service通信问题 在默认情况下，PodA无法通过Service名称直接访问Namespace B中的Service。下面是PodA尝试访问PodB的日志输出：\n1 2 3 # kubectl exec -it -n myns myapp-deploy1-36ds6g873r8-asy2us sh / # ping myapp-clusterip2 ping: bad address \u0026#39;myapp-clusterip2\u0026#39; 这是因为默认的Kubernetes DNS解析只支持在同一Namespace内解析Service名称。\n5. 解决方案：使用ExternalName类型的Service 为了解决这个问题，我们可以在每个Namespace中创建一个ExternalName类型的Service。ExternalName允许通过指定的FQDN（完全限定域名）来指向其他Namespace中的Service。\n以下是实现跨Namespace通信的Service配置：\n在myns Namespace中创建ExternalName Service 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: myapp-clusterip1-externalname namespace: myns spec: type: ExternalName externalName: myapp-clusterip2.mytest.svc.cluster.local ports: - name: http port: 80 targetPort: 80 在mytest Namespace中创建ExternalName Service 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: myapp-clusterip2-externalname namespace: mytest spec: type: ExternalName externalName: myapp-clusterip1.myns.svc.cluster.local ports: - name: http port: 80 targetPort: 80 运行以下命令来应用这些新的Service配置：\n1 kubectl apply -f svc_ExternalName_visit.yaml 6. 验证跨Namespace通信 现在我们可以再次尝试从PodA访问PodB，应该可以成功解析并进行通信：\n1 2 kubectl exec -it -n myns myapp-deploy1-36ds6g873r8-asy2us sh / # ping myapp-clusterip2.mytest.svc.cluster.local 这样，PodA就可以通过Service名称访问不同Namespace中的PodB了。\n7. 总结 通过使用Kubernetes的ExternalName类型Service，我们成功实现了跨Namespace的Service名称解析和通信。此方法避免了直接使用IP地址通信的局限性，确保了即使Service的IP地址发生变化，通信依然可以通过名称进行。\n这种跨Namespace通信的场景在多租户、多环境隔离的Kubernetes集群中尤为常见。如果你的集群架构中需要不同Namespace之间的Service互访，ExternalName是一个有效的解决方案。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/240906085714/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"本文介绍了如何在Kubernetes集群中，使用Service名称实现跨命名空间（namespace）间的Pod互相通信。通过具体示例和详细步骤，展示了如何配置和使用ExternalName类型的Service来解决命名空间间的通信问题，从而避免Service IP变动导致的通信中断。","id":2,"section":"posts","tags":["kubernetes"],"title":"实现Kubernetes跨Namespace Service访问的最佳实践","uri":"https://www.cnsre.cn/posts/240906085714/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/231208094411/\n相关话题：https://www.cnsre.cn/tags/阿里云/\n申请前须知 社区镜像前，请先了解以下注意事项：\n只有通过企业认证的才可以申请使用社区镜像发布功能。 加密镜像不允许发布为社区镜像。 社区镜像完全公开，在镜像所属地域下，所有的阿里云账号均可使用。 社区镜像不支持共享、导出与复制。 申请流程 申请企业认证 因为只有通过企业认证的用户才有权限申请发布社区镜像，所以要先进行社区认证。\n本文主要以申请社区镜像发布权限为主，不再介绍企业认证方法。 没有社区认证的用户可以参考链接 阿里云企业认证\n创建自定义镜像 在申请发布社区镜像之前，首先要把镜像制作好。\n发布社区镜像的镜像来源主要有控制台创建自定义镜像、OpenAPI、镜像构建Image Builder、Packer等。本地以控制台创建自动以镜像为例。\n登录到控制台，按照下图进行操作。进行镜像的制作。\n点击创建自定义镜像后，出现如下界面\n点击创建后，需要等待一段时间。创建时间主要看磁盘和数据量的大小决定的。\n收集镜像信息 创建完自动以镜像后，需要收集一些镜像的信息，以备申请权限时使用。\n具体信息如下：\n**镜像名称：**Wrodpress **操作系统及版本：**CentOS 7.9 64位 UEFI版 镜像大小： 40GB **应用场景：**发布的社区镜像将应用于CloudIaC模板中,用来创建对应的云服务。后续还会推出更多应用和镜像。 **如何引流、用户群：**通过官网CloudIaC模板引流使用社区镜像。主要用户群是学生和高校。 **计划发布、更新：**在多个地域发布社区镜像,并长期更新维护。 **镜像制作方式：**通过控制台创建自定义镜像的方式制作镜像。 提交工单申请权限 准备完以上信息后，就可以提工单申请了。我这边也给大家准备了申请话术供大家参考：\n尊敬的阿里云支持团队， 我云霁科技的账户管理员雪文龙，我代表杭州云霁科技有限公司。 我们近期开发了一款基于CentOS 7.9 定制了WordPress的系统镜像，经过内部测试和优化，我们相信这个镜像对阿里云社区的用户会有很大的价值。 为了分享我们的工作，并让更多的用户受益，我们希望能够将这个自定义系统镜像发布到阿里云社区镜像中。在此，我们诚恳地申请获得发布镜像的权限。 以下是有关我们自定义系统镜像的一些基本信息： 1. 发布者认证名称： Yunjikeji Ampere Computing 2. 镜像名称：Wrodpress 3. 操作系统及版本：CentOS 7.9 64位 UEFI版 4. 镜像大小： 40GB 5. 应用场景：发布的社区镜像将应用于CloudIaC模板中,用来创建对应的云服务。后续还会推出更多应用和镜像。 6. 如何引流、用户群：通过官网CloudIaC模板引流使用社区镜像。主要用户群是学生和高校。 7. 计划发布、更新：在多个地域发布社区镜像,并长期更新维护。 8. 镜像制作方式：通过控制台创建自定义镜像的方式制作镜像。 我们相信这个系统镜像将为阿里云社区的用户提供更丰富的选择，并有助于推动云计算社区的发展。 我们理解阿里云社区对镜像的质量和安全性有严格的要求，因此我们已经采取了一系列措施来确保我们的镜像符合最高标准。 在此，我们真诚地期待能够得到您的支持和批准，以便我们的系统镜像能够为更多的用户服务。 谢谢您的时间和考虑。 针对提交工单申请权限的一些疑问：\n提交工单申请权限审核的时间是多久?\n申请权限的审核时间没有的到具体的回复。不过在催工单的情况下，我是从申请到得到权限用了24个小时。 提交工单申请权限审核的流程是什么?\n申请权限的流程。准备申请发布镜像的材料 \u0026ndash; 提交申请工单 \u0026ndash; 审核团队核对提交申请材料 \u0026ndash; 补充申请材料（如果需要）\u0026ndash; 审核镜像 \u0026ndash; 开通权限。因为没有公布具体的审核流程，但是从沟通上我猜想可以这样去理解 有没有一些快捷的方式可以尽快通过审核？\n有的。值得一提的是，在我后续申请到了权限以后，我也可以发布没有进行审核的镜像发布到社区镜像中。所以大家在申请社区镜像权限的时候，可以选择申请一个尽量符合规则标准的镜像去申请。 发布社区镜像 经历24小时的来回拉扯。得到了回复 您好 : 后台已经审核完毕，您在配置发布镜像看下呢。\n拿到权限。发布测试。\n进入镜像功能页面。\n登录ECS管理控制台。 在左侧导航栏，选择实例与镜像 \u0026gt; 镜像。 在顶部菜单栏左上角处，选择地域。 在自定义镜像页签，找到待发布为社区镜像的可用自定义镜像，然后在操作列选择... \u0026gt;发布为社区镜像。\n在发布为社区镜像对话框，完成社区镜像的发布。\n发布成功后，您可以在镜像页面，单击社区镜像页签，查看已发布的社区镜像信息。\n相关问答 提交工单申请权限审核的时间是多久?\n申请权限的审核时间没有的到具体的回复。不过在催工单的情况下，我是从申请到得到权限用了24个小时。 提交工单申请权限审核的流程是什么?\n申请权限的流程。准备申请发布镜像的材料 \u0026ndash; 提交申请工单 \u0026ndash; 审核团队核对提交申请材料 \u0026ndash; 补充申请材料（如果需要）\u0026ndash; 审核镜像 \u0026ndash; 开通权限。因为没有公布具体的审核流程，但是从沟通上我猜想可以这样去理解 有没有一些快捷的方式可以尽快通过审核？\n有的。值得一提的是，在我后续申请到了权限以后，我也可以发布没有进行审核的镜像发布到社区镜像中。所以大家在申请社区镜像权限的时候，可以选择申请一个尽量符合规则标准的镜像去申请。 参考链接 https://help.aliyun.com/zh/ecs/user-guide/overview-12\nhttps://help.aliyun.com/zh/pnp/l6821168?spm=5176.22414175.sslink.1.638a753bYT0ivM\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/postsid/\n相关话题：https://www.cnsre.cn/tags/阿里云/\n","description":"在阿里云中无法发布镜像到社区，让更多的人使用？","id":3,"section":"posts","tags":["阿里云"],"title":"阿里云申请发布社区镜像权限","uri":"https://www.cnsre.cn/posts/231208094411/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/230506425079/\n相关话题：https://www.cnsre.cn/tags/lambda/\n利用Lambda轻松实现EC2实例监控 最近新增了一些服务器，因为每个服务器的基础监控都是要做的。我就想，如何能够快速便捷的方式把这些基础指标都监控上呢？本文将详细介绍如何通过Lambda自动为所有EC2实例添加CloudWatch磁盘告警,并在磁盘利用率超过阈值时,通过SNS主题发送通知。\n准备工作 在开始前,我们需要准备:\nAWS账号:开通一个AWS账号。 SNS主题:在SNS上创建一个主题,并添加订阅者。订阅方式可以是邮件、短信等。 VPC ID:登录VPC控制台获取VPC ID。 Lambda：需要创建一个Lambda。 创建SNS主题 接下来,我们需要在AWS SNS上创建一个主题,并将订阅者添加到主题中。这些订阅者将会接收磁盘使用超过阈值时的通知。我们可以选择电子邮件、短信等通知方式。\n登录到 Amazon SNS 控制台。 单击左侧面板上的“主题”,然后单击“创建主题”。 输入主题名称和主题显示名称,单击“创建主题”。 在主题页面上,单击“订阅”,选择希望接收通知的通知方式。 根据选定的订阅类型输入相关信息,单击“创建订阅”。 在主题页面上,您将看到已创建的订阅。\n举个例子,如果我们选择“邮件”作为订阅方式: 在“电子邮件订阅”下,输入接收通知的电子邮件地址。 单击“创建订阅”。 您应会收到一封来自AWS通知的验证电子邮件。请按照电子邮件中的说明进行验证。 一旦验证通过,该订阅将变为“已验证”状态。 现在,当CloudWatch告警触发并调用SNS主题时,验证的电子邮件地址将收到有关警报的电子邮件通知。我们也可以添加多个订阅以接收通过不同渠道的通知,例如电子邮件、短信等。 添加订阅者后,我们可以在SNS主题的“概述”页查看订阅的详细信息和状态。在测试Lambda函数时,这有助于确保订阅设置正确和通知能正常发送。\n创建Lambda函数 接下来,我们可以在Lambda控制台创建一个新的Lambda函数。\n访问 Amazon Lambda 控制台,点击“创建函数”。 选择“从头开始” ,输入函数名称,如“create_cloudwatch_alarm_for_ec2_disk_usage”。 选择Python作为运行环境,选择 创建具有基本 Lambda 权限的新角色。 点击“创建函数”。 编写Lambda函数代码 代码我已经写好了，各位大佬收好。代码功能是为所有EC2实例设置CloudWatch磁盘告警,当磁盘利用率超过阈值时,通过SNS主题发送通知。\n连接EC2客户端。 获取VPC中运行的EC2实例列表。 遍历实例创建CloudWatch磁盘使用百分比告警。默认阈值是80%。 告警触发SNS主题“you_sns_arn”。 将下面代码复制后进行部分修改。然后替换掉原有代码后，然后保存并部署。\n下面是具体代码内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \u0026#39;\u0026#39;\u0026#39; Author : Wenlong Xue Date : 2023-02-27 13:35 LastEditors : Wenlong Xue LastEditTime : 2023-02-27 17:52 Description : 为 VPC 中的 EC2 实例创建磁盘使用百分比的 CloudWatch 告警，并在磁盘使用超过阈值时发送通知到 SNS 主题 \u0026#39;\u0026#39;\u0026#39; import boto3 def lambda_handler(event, context): # 连接到 EC2 客户端 ec2 = boto3.client(\u0026#39;ec2\u0026#39;) # 获取 VPC 中运行中的实例列表 response = ec2.describe_instances( Filters=[ { \u0026#39;Name\u0026#39;: \u0026#39;vpc-id\u0026#39;, \u0026#39;Values\u0026#39;: [\u0026#39;vpc-xxxxxx\u0026#39;] # 修改VPC ID }, { \u0026#39;Name\u0026#39;: \u0026#39;instance-state-name\u0026#39;, \u0026#39;Values\u0026#39;: [\u0026#39;running\u0026#39;] } ] ) # 遍历实例并为磁盘使用百分比创建 CloudWatch 告警 for reservation in response[\u0026#39;Reservations\u0026#39;]: for instance in reservation[\u0026#39;Instances\u0026#39;]: # 获取实例 ID 和 Name 标签 instance_id = instance[\u0026#39;InstanceId\u0026#39;] instance_name = \u0026#39;\u0026#39; for tag in instance[\u0026#39;Tags\u0026#39;]: if tag[\u0026#39;Key\u0026#39;] == \u0026#39;Name\u0026#39;: instance_name = tag[\u0026#39;Value\u0026#39;] break # 创建磁盘使用百分比的 CloudWatch 告警 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) cloudwatch.put_metric_alarm( AlarmName=\u0026#39;{}-磁盘使用百分比\u0026#39;.format(instance_name), AlarmDescription=\u0026#39;{} - 磁盘使用百分比\u0026#39;.format(instance_name), ActionsEnabled=True, AlarmActions=[\u0026#39;you_sns_arn\u0026#39;], # 修改sns主题,如:arn:aws-cn:sns:cn-north-1:xxxxx:HighDiskUsed MetricName=\u0026#39;disk_used_percent\u0026#39;, # 根据情况修改 Namespace=\u0026#39;CWAgent\u0026#39;, # 根据情况修改 Dimensions=[ { \u0026#39;Name\u0026#39;: \u0026#39;InstanceId\u0026#39;, \u0026#39;Value\u0026#39;: instance_id } ], Statistic=\u0026#39;Maximum\u0026#39;, Period=300, EvaluationPeriods=1, Threshold=80.0, ComparisonOperator=\u0026#39;GreaterThanOrEqualToThreshold\u0026#39; ) 给IAM角色添加权限 因为 创建具有基本 Lambda 权限的新角色 的权限并不完整，所以我们要把代码中所要用到的权限给到这个新角色。\n首先,Lambda中找到这个觉得然后给他添加对应的权限。\n找到创建的Lambda 函数。 选择函数汇总的配置\u0026ndash;权限。 找到执行角色，并点击角色名称连接。 点击添加权限，选择附加策略。 添加 CloudWatchFullAccess 和 AmazonEC2ReadOnlyAccess 权限 最后权限如下：\n注意\n⚠️ 若提示确少什么权限就补充什么权限。 测试函数 现在,我们已经创建Lambda函数和SNS主题,并将订阅者添加到主题。我们可以测试Lambda函数功能。\n我们可以手动运行测试来观察 CloudWatch告警项有没有被添加。 我们也可以进行添加触发器。如：固定时间段运行一次，来遍历新加的服务器。然后添加告警项。 常见问题及解决方案 Q1: 没收到SNS通知怎么办?\nA1: 确认订阅邮箱是否正确。检查AWS账号是否有权限向SNS发送通知。\nQ2: 如何修改磁盘使用阈值?\nA2: 编辑Lambda函数代码,更改\u0026quot;Threshold\u0026quot;的值。\nQ3: CloudWatch的告警项中显示数据不足怎么办?\nA3: 查看CloudWatch中原本磁盘的指标名称，并修改Lambda函数代码对应的值：MetricName=\u0026lsquo;disk_used_percent\u0026rsquo;；Namespace=\u0026lsquo;CWAgent\u0026rsquo; 参考资料 AWS Lambda文档: https://docs.aws.amazon.com/lambda/ AWS CloudWatch文档: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/ AWS SNS文档: https://docs.aws.amazon.com/sns/index.html 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/230506425079/\n相关话题：https://www.cnsre.cn/tags/lambda/\n","description":"本文介绍如何使用 AWS Lambda 来自动添加 CloudWatch 磁盘告警并通过 SNS 发送通知。","id":4,"section":"posts","tags":["lambda","aws","cloudwatch","sns","python"],"title":"如何使用 Lambda 自动添加CloudWatch所有实例磁盘告警及 SNS 通知","uri":"https://www.cnsre.cn/posts/230506425079/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/230410659053/\n相关话题：https://www.cnsre.cn/tags/lambda/\nLambda函数检查S3文件夹是否存在 作为 AWS 中最常用的对象存储服务，S3 可以用于存储各种类型的文件，包括网站文件、媒体文件、备份文件等等。在 S3 中存储的文件可以通过不同的方式访问，例如在 Web 应用程序中、通过移动应用程序或直接使用 AWS SDK 访问等。\n在进行 S3 存储时，如果我们需要将存储的日志同步到另一个桶或区域中，则可以使用 AWS 的 S3 日志同步任务功能。通过将日志同步到其他存储桶或区域中，我们可以更方便地对日志进行分析、监控和管理。\n但是，如果 S3 日志同步任务出现故障，我们可能无法及时获取相关的日志信息。因此，为了确保日志同步任务的正常运行，我们需要对任务进行监控。在本文中，我们将介绍如何使用 AWS Lambda 监控 S3 日志同步任务。\n介绍 AWS Lambda 是一种无服务器计算服务，可使您在云中运行代码，而无需自己管理服务器。通过使用 Lambda，您可以将代码上传到云中，然后 Lambda 会根据需要自动扩展和缩减计算资源，以满足您的应用程序的请求。Lambda 还支持许多编程语言和库，使您能够编写功能强大的应用程序和服务。\n在本文中，我们将使用 Lambda 编写一个函数，该函数将定期检查 S3 存储桶中的文件夹是否存在。如果不存在任何文件夹，则 Lambda 将向指定的 SNS 主题发送一条消息，以便管理员可以及时采取措施。通过使用 Lambda 监控 S3 存储桶中的文件夹，我们可以确保日志同步任务的正常运行。\n1. 准备工作 在开始之前，我们需要先准备好以下工作：\n一个S3桶，用于存储我们要检查的文件夹。 一个SNS主题，用于发送消息提醒。 2. 创建Lambda函数 在AWS控制台上创建一个Python Lambda函数，名称为s3-folder-exist-checker，并使用以下代码：\nimport boto3 from datetime import datetime, timedelta from dateutil import tz def lambda_handler(event, context): print(\u0026#39;Lambda 函数已启动.\u0026#39;) s3 = boto3.resource(\u0026#39;s3\u0026#39;) bucket_name = \u0026#39;my_s3_bucket_name\u0026#39; local_tz = tz.gettz(\u0026#39;Asia/Shanghai\u0026#39;) now = datetime.now() date_prefix = now.strftime(\u0026#39;%Y/%m/%d/\u0026#39;) folder_prefixes = [\u0026#39;my_prefixes/\u0026#39; + date_prefix, \u0026#39;my_prefixes/\u0026#39; + date_prefix, \u0026#39;RGC-Prod-3in1oven/\u0026#39; + date_prefix] folder_prefixes = [prefix + \u0026#39;/\u0026#39; if not prefix.endswith(\u0026#39;/\u0026#39;) else prefix for prefix in folder_prefixes] # 确保每个前缀以斜杠结尾 print(\u0026#39;正在检查以下 S3 文件夹：\u0026#39;, folder_prefixes) sns = boto3.client(\u0026#39;sns\u0026#39;) topic_arn = \u0026#39;arn:aws-cn:sns:cn-north-1:1234567890:s3-logs-monitoring\u0026#39; for prefix in folder_prefixes: resp = s3.meta.client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter=\u0026#39;/\u0026#39;) subfolders = [p[\u0026#39;Prefix\u0026#39;] for p in resp.get(\u0026#39;CommonPrefixes\u0026#39;, [])] if len(subfolders) \u0026gt; 0: print(f\u0026#34;子文件夹 \u0026#39;{prefix}\u0026#39; 存在:\u0026#34;) for folder in subfolders: print(f\u0026#34;发现子文件夹: {folder}\u0026#34;) else: message = f\u0026#34;S3桶\u0026#39;{bucket_name}中\u0026#39;{prefix}\u0026#39;下不存在新增文件夹,即日志同步S3桶任务失败.请检查.\u0026#39;\u0026#34; sns.publish(TopicArn=topic_arn, Message=message) print(f\u0026#34;已发送 SNS 消息: {message}\u0026#34;) print(\u0026#39;Lambda 函数已完成.\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;S3 文件夹存在性检查已完成.\u0026#39; } 接下来是对 Lambda 函数中的一些细节进行讲解。首先，我们定义了 S3 的资源，并且指定了 S3 桶的名称：\ns3 = boto3.resource(\u0026#39;s3\u0026#39;) bucket_name = \u0026#39;my_bucket_name\u0026#39; 接着，我们获取当前的时间，并且根据当前时间生成一个目录前缀。我们使用 dateutil 模块中的 tz.gettz 函数来获取一个本地的时区信息。为了确保时区的准确性，我们建议在使用 Lambda 函数时，显式地设置时区信息：\nlocal_tz = tz.gettz(\u0026#39;Asia/Shanghai\u0026#39;) now = datetime.now(local_tz) date_prefix = now.strftime(\u0026#39;%Y/%m/%d/\u0026#39;) 在 Lambda 函数中，我们使用了 list_objects_v2 方法来列举指定的文件夹。具体来说，我们使用了 CommonPrefixes 参数，该参数可以返回指定前缀下的子文件夹列表。如果返回的子文件夹列表为空，则说明指定的文件夹不存在。如果子文件夹列表不为空，则说明文件夹存在，并且我们可以将每个子文件夹的路径打印出来：\nresp = s3.meta.client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter=\u0026#39;/\u0026#39;) subfolders = [p[\u0026#39;Prefix\u0026#39;] for p in resp.get(\u0026#39;CommonPrefixes\u0026#39;, [])] if len(subfolders) \u0026gt; 0: print(f\u0026#34;子文件夹 \u0026#39;{prefix}\u0026#39; 存在:\u0026#34;) for folder in subfolders: print(f\u0026#34;发现子文件夹: {folder}\u0026#34;) 如果子文件夹列表为空，则说明文件夹不存在。在这种情况下，我们可以使用 SNS 服务发送一条消息来通知管理员：\nif len(subfolders) == 0: message = f\u0026#34;S3桶\u0026#39;{bucket_name}中\u0026#39;{prefix}\u0026#39;下不存在新增文件夹,即日志同步S3桶任务失败.请检查.\u0026#39;\u0026#34; sns.publish(TopicArn=topic_arn, Message=message) print(f\u0026#34;已发送 SNS 消息: {message}\u0026#34;) 最后，我们返回了一个包含状态码和消息的字典，以便可以在 Lambda 函数执行过程中监控执行状态：\nreturn { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;S3 文件夹存在性检查已完成.\u0026#39; } 确保您的 Lambda 函数有权限访问 S3 和 SNS\n在 AWS Lambda 控制台中，创建一个新的 Lambda 函数。在函数代码中将 Python 代码粘贴到代码编辑器中。请确保您选择了正确的运行时环境，并设置以下环境变量：\nBUCKET_NAME：您的 S3 桶名称 SNS_TOPIC_ARN：SNS 主题的 ARN 配置 Lambda 函数的基本设置和高级设置，包括内存和超时。\n在 Lambda 控制台中，测试 Lambda 函数，以确保 Lambda 函数能够访问 S3 桶和 SNS 主题。为了测试该函数，您可以创建一个测试事件，该事件需要一个空的 JSON 对象，例如：\n最后，您需要在 Amazon CloudWatch 中设置 CloudWatch Events 规则以定期触发 Lambda 函数。这样您的 Lambda 函数就能在您预定的时间检查 S3 文件夹是否存在并发送通知。\n总结 在这篇文章中，我们介绍了一个使用 AWS Lambda、S3 和 SNS 的自动化任务，该任务定期检查 S3 文件夹是否存在并发送通知。我们解释了如何编写 Python 代码来实现此任务，并提供了一个详细的代码示例。我们还介绍了如何在 AWS Lambda 和 Amazon SNS 控制台上配置 Lambda 函数和 SNS 主题，并在 Amazon CloudWatch 中创建定期触发器来触发 Lambda 函数。最后，我们提供了一些最佳实践和注意事项，以确保您的 Lambda 函数和 SNS 主题能够正常工作。\n希望这篇文章对您有所帮助！如果您有任何疑问或建议，请在下面的评论区留言。\n参考文献 AWS Lambda 文档 AWS S3 文档 AWS SNS 文档 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/230410659053/\n相关话题：https://www.cnsre.cn/tags/lambda/\n","description":"使用 Lambda 函数检查当前日期（上海时区）的 S3 文件夹是否存在，如果不存在任何文件夹，则发送 SNS 消息。","id":5,"section":"posts","tags":["lambda","python","aws"],"title":"Lambda函数检查S3文件夹是否存在","uri":"https://www.cnsre.cn/posts/230410659053/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/230129126154/\n相关话题：https://www.cnsre.cn/tags/aws/\n背景需求 因为项目的安全性。为了避免项目的服务器暴露在公网中。很多时候我们会使用跳板机或者是一些三方的远程工具，来进行一些安全性比较高的方式来进行远程项目的服务器，但是往往越安全的方式就越麻烦。那有没有一种既安全，有便捷的连接方式呢？当然有，今天就介绍下AWS Session Manager。\n前置需求 一台 EC2 服务器（需要开启 SSM 远程服务并分配权限） 一个 AWS 账户 安装配置 SSM Agent 在 Amazon Linux 2 中安装SSM Agent x86_64\n1 sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm ARM64\n1 sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_arm64/amazon-ssm-agent.rpm 在 CentOS 7.x 上安装 SSM Agent x86_64\n1 sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm ARM64\n1 sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_arm64/amazon-ssm-agent.rpm 启动服务并设置为开机启动 1 2 sudo systemctl status amazon-ssm-agent sudo systemctl enable amazon-ssm-agent 如其他系统安装请参考 在适用于 Linux 的 EC2 实例上手动安装 SSM Agent 给 EC2 分配对应权限 在角色管理中选择创建角色。\n如下如图所示选择对应选项。\n搜索AmazonSSMFullAccess 选择下一步\n填入角色名称，然后创建角色\n在 AWS EC2 控制台中找到对应的服务器，然后点击操作``安全 修改IAM角色\n在框内搜索刚才创建的角色名称。然后确定选择。\nAWS 中的远程链接方式 Session Manager 安装完以后可以在AWS 中我们可以直接通过控制台去链接 Linux 服务器，如下图。在选中实例以后 在右上角中选择连接。然后再次进行远程登录服务。\n如遇到无法远程的情况请按照下面的方法将安装ssm服务。\n在 SSM 中进行远程链接 在控制台 Amazon Systems Manager 中选择 Session Manager\n选择启动会话。\n选择对应的实例 然后点击启动会话。\n然后就进入到了系统界面。\n如何通过IAM让不同的 aws 用户拥有不同的远程服务器权限？下篇文章将会介绍 Session Manager 的进阶用法 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/230129126154/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"因为一些项目的原因，不能开放 SSH 端口。这就需要用其他的方式来远程AWS中的EC2，通过SSM中的Session Manager能完美解决这个问题。","id":6,"section":"posts","tags":["aws","ssm"],"title":"AWS 中的另外一种远程工具 AWS Session Manager","uri":"https://www.cnsre.cn/posts/230129126154/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/221207116004/\n相关话题：https://www.cnsre.cn/tags/k3s/\n去年双十一买的服务器，但是创建了 K3S 集群。今天登录的时候发现出现了以下错误。简单记录下。\n故障现象 登录服务器执行相关命令出现以下错误\n1 2 [root@k3s-master ~]# kubectl get pods error: You must be logged in to the server (Unauthorized) 执行 doker 命令判断大概问题\n1 2 [root@k3s-master ~]# docker run -it ubuntu /bin/echo \u0026#34;cnsre\u0026#34; cnsre docker 命令可以执行那大概率应该是 k3s 的问题,查看 k3s 服务日志\n1 2 [root@k3s-master ~]# journalctl -r -u k3s 1466 authentication.go:63] \u0026#34;Unable to authenticate the request\u0026#34; err=\u0026#34;[x509: certificate has expired or is not yet valid: current time 发现有以上错误，那确定是证书的问题了。\n解决方法 对于K3S 来说解决证书的问题其实很简单。\n可以通过重启K3S 服务的来解决问题\n1 sudo systemctl restart k3s 验证 执行命令验证问题\n[root@k3s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION k3s-node1 Ready \u0026lt;none\u0026gt; 370d v1.21.5+k3s2 k3s-node2 Ready \u0026lt;none\u0026gt; 370d v1.21.5+k3s2 k3s-node3 Ready \u0026lt;none\u0026gt; 370d v1.21.5+k3s2 k3s-master Ready control-plane,master 370d v1.21.5+k3s2 问题解决。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/221207116004/\n相关话题：https://www.cnsre.cn/tags/k3s/\n","description":"k3s 集群提示 \"error You must be logged in to the server (Unauthorized)\"","id":7,"section":"posts","tags":["k3s","故障集"],"title":"k3s 证书过期修改","uri":"https://www.cnsre.cn/posts/221207116004/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/221205544069/\n相关话题：https://www.cnsre.cn/tags/aws/\n躺了好久，诈尸了。因为换了工作，所以比较忙一直没有时间去更新博客的内容（主要还是因为懒🤔）\n话不多说 直接上干货。\n需求背景 最近在看费用的时候发现有很大一部分费用都是 cloudwatch log中存储了大量的数据，是因为ec2 将日志传输到了存储到了cloudwatch中。这个存储的多的查询日志的时候收费特别的高。另外一个是因为数据分析用途，大数据分析的同事如果想那到数据的话，还是存储在 S3 中是比较划算和方便的，一个是拿取数据比较方便，另外一个是S3 可以最归档存储，后面的大量数据可以分层储存，以此来降低费用。\n如果你也想将你的cloudwatch 中日志组中的日志存储到S3中的话可以参考下这篇文章。\n前置条件 创建 一个 S3 桶，并修改权限 创建 lambda 函数 有一个Cloudwatch 日志组并且有一天以上的日志 给 lambda分配所需的权限 创建 S3 桶并修改权限 国内S3桶权限配置 国外S3桶权限配置 国内S3桶权限配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.cn-north-1.amazonaws.com.cn\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws-cn:s3:::\u0026lt;bucket name\u0026gt;\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.cn-north-1.amazonaws.com.cn\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws-cn:s3:::\u0026lt;bucket name\u0026gt;/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } } ] } 国外S3桶权限配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::\u0026lt;bucket name\u0026gt;\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.us-west-2.amazonaws.com\u0026#34; } }, { \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34; , \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::\u0026lt;bucket name\u0026gt;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } }, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;logs.us-west-2.amazonaws.com\u0026#34; } } ] } S3 桶权限文档链接\n创建 lambda 函数 创建 lambda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import boto3 import logging import time import datetime import json logger = logging.getLogger() logger.setLevel(logging.INFO) def export_s3_logs(bucket_name, log_group_name, log_stream_name, days_of_logs=1, timeout=1000): \u0026#39;\u0026#39;\u0026#39; today = datetime.datetime.combine(datetime.datetime.utcnow(), datetime.datetime.min.time()) day_end = today day_start = today - datetime.timedelta(days=days_of_logs) \u0026#39;\u0026#39;\u0026#39; today = datetime.datetime.combine(datetime.datetime.utcnow() + datetime.timedelta(hours=8), datetime.datetime.min.time()) # UTC+8 day_end = today - datetime.timedelta(hours=8) # UTC day_start = today - datetime.timedelta(days=days_of_logs, hours=8) # UTC #print(day_start) ts_start = \u0026#39;{0:.0f}\u0026#39;.format(((day_start - datetime.datetime(1970, 1, 1)).total_seconds())*1000) ts_end = \u0026#39;{0:.0f}\u0026#39;.format(((day_end - datetime.datetime(1970, 1, 1)).total_seconds())*1000) the_date = \u0026#39;/\u0026#39;.join([str(today.year), \u0026#39;0\u0026#39;+str(today.month)[-2:], \u0026#39;0\u0026#39;+str(today.day)[-2:]]) #folder_name = \u0026#39;/\u0026#39;.join([log_group_name, log_stream_name, the_date]) folder_name = \u0026#39;/\u0026#39;.join([log_group_name,the_date]) client = boto3.client(\u0026#39;logs\u0026#39;) #print (ts_start, ts_end)#, day_start, day_end,the_date task_id = client.create_export_task( logGroupName=log_group_name, #logStreamNamePrefix=log_stream_name, fromTime=int(ts_start), to=int(ts_end), destination=bucket_name, destinationPrefix=folder_name )[\u0026#39;taskId\u0026#39;] i = 1 while i\u0026lt;timeout: response = client.describe_export_tasks( taskId=task_id ) status = response[\u0026#39;exportTasks\u0026#39;][0][\u0026#39;status\u0026#39;] if status == \u0026#39;COMPLETED\u0026#39;: result = True break elif status != \u0026#39;RUNNING\u0026#39;: result = False break i += 1 time.sleep(interval) return result def lambda_handler(event, context): region = \u0026#39;cn-northwest-1\u0026#39; # 日志组所在区域 bucket_name = \u0026#39;\u0026lt;bucket name\u0026gt;\u0026#39; #同区域内的S3桶名称 log_group_name = \u0026#39;\u0026lt;log group name\u0026gt;\u0026#39; #日志组名称 log_stream_name = \u0026#39;1\u0026#39; #默认即可 log_export_days = 1 #默认即可 export_s3_logs(bucket_name, log_group_name, log_stream_name, log_export_days) 给 lambda 分配权限 AmazonS3的读写权限 CloudWatchLogsFullAccess 验证桶内的文件 最后会以日期的目录将日志归档起来，以方便日后对归档文件进行梳理。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/221205544069/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"使用 Lambda 函数将 cloudwatchlog 中的日志归档到S3 桶中","id":8,"section":"posts","tags":["aws","lambda","cloudwatch"],"title":"使用 Lambda 函数将 CloudWatch Log 中的日志归档到 S3 桶中","uri":"https://www.cnsre.cn/posts/221205544069/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/220125450139/\n相关话题：https://www.cnsre.cn/tags/eks/\n前言 之前在 aws 中创建了 eks，在数据存储这一块中，我选择了使用 AWS 的 EFS 具体部署配置参考Amazon EKS 中 EFS 持久性存储。文章中的动态供给是 AWS 官方给的示例，使用的是root用户启动的容器。在我后面的测试中发现，我在使用非root用户启动容器的时候，发现使用静态供给是有权限并且没有报错的。但是在使用静载供给的时候出现了 Operation not permitted 的报错。\n问题描述 我根据efs dynamic_provisioning 创建了 dynamic provisioning\nroot用户的容器运行没有问题，但是非root用户容器运行时提示 “Operation not permitted”\npod配置清单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 apiVersion: v1 kind: Service metadata: name: wordpress-mysql labels: app: wordpress spec: ports: - port: 3306 selector: app: wordpress tier: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim labels: app: wordpress spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: wordpress-mysql labels: app: wordpress spec: selector: matchLabels: app: wordpress tier: mysql strategy: type: Recreate template: metadata: labels: app: wordpress tier: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim StorageClass配置清单：\n1 2 3 4 5 6 7 8 9 10 11 12 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: fs-xxxxx directoryPerms: \u0026#34;700\u0026#34; gidRangeStart: \u0026#34;1000\u0026#34; # optional gidRangeEnd: \u0026#34;2000\u0026#34; # optional basePath: \u0026#34;/dynamic_provisioning\u0026#34; # optional 分析和检查 该报错是由于采用了dynamic provisioning PV部署方式，这种模式的实现需要利用 efs-ap：access point访问点 模式做 EFS 挂载。从 EFS 的角度来讲，EFS 的 access point 模式挂载的 EFS 卷，客户端不可修改 uid/gid ，只拥有使用权（读写）详情点击查看。从自己的pod环境也可以看到，客户端挂载目录/dynamic_provisioning 的uid跟gid都是一个随机数字。 ls -l /dynamic_provisioning可以看到是 `1018 （不同环境uid会不同）。\nEFS-AP模式指的是access point访问点模式。关于访问点的介绍：\nEFS Access Points：\nAn access point applies an operating system user, group, and file system path to any file system request made using the access point. The access point\u0026rsquo;s operating system user and group override any identity information provided by the NFS client.\n简单来讲，EFS-AP也就是access point访问点挂载模式下，efs客户端的路径user/gid是不可被修改的。的客户端用户只有使用权（读写），但是不可以修改owner。因此遇到的报错是该配置的预期表现。\nEFS-AP模式的配置是在storageclass中定义的：provisioningMode: efs-ap，比如：\n1 2 3 4 5 6 7 8 9 10 11 12 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc-dynamic provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;------------------------------EFS访问点挂载模式 fileSystemId: fs-xxxxxx directoryPerms: \u0026#34;700\u0026#34; gidRangeStart: \u0026#34;1000\u0026#34; # optional gidRangeEnd: \u0026#34;2000\u0026#34; # optional basePath: \u0026#34;/dynamic_provisioning\u0026#34; # optional 目前AWS EFS的 dynamic provisioning 模式的实现就是使用 storageclass 的 efs-sc-dynamic 模式。\n这种模式的弊端已经在 github 中有issue在跟踪,详情点击查看，但是由于该模式也有一定的设计意义 详情点击查看，所以目前还没有明确的结论。\n临时解决方法 使用静态模式创建 可以创建EKS pv/pvc时使用static模式部署PV，不会使用access point模式挂载EFS卷，那么可以顺利修改uid/gid。\n详情参考Amazon EKS 中 EFS 持久性存储\n在pod中指定 uid 和 gid 在创建pod之前，先创建 pvc在创建完pvc后查看uid 和gid\n1 2 3 4 5 [root@ip-10-0-100-206 ~]# ls -l /efs/dynamic_provisioning/ total 12 drwxr-xr-x 5 1015 1015 6144 Jan 20 02:44 pvc-40b922c7-8d4d-47d9-8783-60d25abe123 drwxr-xr-x 5 1017 1017 6144 Jan 20 04:22 pvc-4ee000a8-7ab2-4ffc-8fd3-72ef31b7123 drwx------ 5 1014 1014 6144 Jan 20 01:08 pvc-f6622cb3-7c24-4172-a427-d4b9a996122 将输出内容的pvc的uid gid 记下并在pod的yaml清单中指定uid已经gid让pod拥有该目录的权限。\npod配置清单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 apiVersion: v1 kind: Service metadata: name: wordpress-mysql labels: app: wordpress spec: ports: - port: 3306 selector: app: wordpress tier: mysql clusterIP: None --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim labels: app: wordpress spec: accessModes: - ReadWriteMany resources: requests: storage: 5Gi --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: wordpress-mysql labels: app: wordpress spec: selector: matchLabels: app: wordpress tier: mysql strategy: type: Recreate template: metadata: labels: app: wordpress tier: mysql spec: securityContext: fsGroup: 1014 runAsUser: 1014 runAsGroup: 1014 containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim 检查 1 2 3 4 5 6 7 8 9 10 kubectl get pv|grep mysql pvc-f6622cb3-7c24-4172-a427-d4b9a9962cd8 5Gi RWX Delete Bound default/mysql-pv-claim efs-sc 5d23h kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-f6622cb3-7c24-4172-a427-d4b9a9962cd8 5Gi RWX efs-sc 5d23h kubectl get pod NAME READY STATUS RESTARTS AGE wordpress-mysql-6f6455f449-52zrp 1/1 Running 0 5d7h 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/220125450139/\n相关话题：https://www.cnsre.cn/tags/eks/\n","description":"aws eks使用efs dynamic provisioning 挂载到非root容器提示 Operation not permitted的问题。","id":9,"section":"posts","tags":["efs","eks","故障集"],"title":"eks使用efs dynamic provisioning 创建非root容器提示 Operation not permitted","uri":"https://www.cnsre.cn/posts/220125450139/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/220110850573/\n相关话题：https://www.cnsre.cn/tags/eks/\n学习目标 在 EKS 中部署 Amazon EFS CSI 驱动程序到 验证 EFS 并验证它是否正常工作 创建基于 EFS 的静态、动态存储 前提条件 EKS 集群 AWS CLI 如果你没有安装请查看安装、更新和卸载 AWS CLI。在安装 AWS CLI 后，还要对其进行配置。 kubectl 如果没有安装 请查看安装 kubectl。 创建 IAM 策略 创建 IAM 策略并将其分配给 IAM 角色。该策略将允许 Amazon EFS 驱动程序与文件系统交互。\n1.从 查看下方 IAM 策略文档或者查看策略文档。\n推荐使用 查看 策略文档。获取策略文档。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticfilesystem:DescribeAccessPoints\u0026#34;, \u0026#34;elasticfilesystem:DescribeFileSystems\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticfilesystem:CreateAccessPoint\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;aws:RequestTag/efs.csi.aws.com/cluster\u0026#34;: \u0026#34;true\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;elasticfilesystem:DeleteAccessPoint\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ResourceTag/efs.csi.aws.com/cluster\u0026#34;: \u0026#34;true\u0026#34; } } } ] } 2.在IAM-策略 中创建策略\n在Identity and Access Management (IAM)中点击 策略 然后在下一步中点击 创建策略\n点击 json 然后将 IAM 策略 填入， 然后点击 下一步:标签\n在下一步的标签中，可以根据自己的情况自行填写,然后点击 下一步:审核\n名称中填写 AmazonEKS_EFS_CSI_Driver_Policy\n你可以将 AmazonEKS_EFS_CSI_Driver_Policy 更改为其他名称，但如果更改，请确保在后续步骤中也做出相应更改。 将 efs 策略 附件到 eks node 角色中 将我们刚才创建的 efs 策略 AmazonEKS_EFS_CSI_Driver_Policy 附加在 eks_node 的角色中，确保eks node拥有efs的权限。\n如果你之前创建了 eks 那么在你的角色中会有一个名为 eksctl-\u0026lt;eks-name\u0026gt;-nodegrou-NodeInstanceRole-xxxxxxxxx 的角色。 在角色中搜索 node 然后点击 eksctl-\u0026lt;eks-name\u0026gt;-nodegrou-NodeInstanceRole-xxxxxxxxx\n在角色中点击 附加策略\n搜索之前创建的 EFS 策略 也就是 AmazonEKS_EFS_CSI_Driver_Policy 然后选中，点击最下方的附加策略。\n安装 Amazon EFS 驱动程序 使用 Helm 或 yaml 清单安装 Amazon EFS CSI 驱动程序。\n这边不详细说 helm 部署方式主要介绍 yaml 清单部署\n一定要修改镜像地址为你所在的地区 Amazon EKS 附加组件容器镜像地址 yaml 清单部署 因为github网络的问题。如果再执行的时候部署没有反应，请终止运行，多运行几次尝试部署 1 kubectl apply -k \u0026#34;github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.3\u0026#34; 输出如下：\n1 2 3 4 5 6 7 serviceaccount/efs-csi-controller-sa created serviceaccount/efs-csi-node-sa created clusterrole.rbac.authorization.k8s.io/efs-csi-external-provisioner-role created clusterrolebinding.rbac.authorization.k8s.io/efs-csi-provisioner-binding created deployment.apps/efs-csi-controller created daemonset.apps/efs-csi-node created csidriver.storage.k8s.io/efs.csi.aws.com created 检查驱动运行是否正常\n1 2 3 4 5 kubectl get pods -A|grep efs kube-system efs-csi-controller-56f6dc4c76-2lvqf 3/3 Running 0 3m32s kube-system efs-csi-controller-56f6dc4c76-dxkwl 3/3 Running 0 3m32s kube-system efs-csi-node-9ttxp 3/3 Running 0 3m32s kube-system efs-csi-node-hsn94 3/3 Running 0 3m32s 虽然这边显示运行正常，但是还是要修改镜像地址。不然在创建pv，pvc以后在pod中挂载会出现错误。（后面会单独记录这个错误） 修改 efs-csi-node 驱动\n1 kubectl edit daemonsets.apps -n kube-system efs-csi-node 找到 aws-efs-csi-driver 驱动所在的位置\n然后将镜像修改为 918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/aws-efs-csi-driver:v1.3.3\n具体如下\n创建 Amazon EFS 文件系统 为 Amazon EKS 集群创建 Amazon EFS 文件系统 在控制台中搜索 efs 点击确认然后进入 EFS 控制台\n在控制台中点击 创建文件系统\n名称： 根据自己的情况填写\nvpc： 一定要创建在跟 eks 同一 VPC 下\n可用性和持久性: 根据提示说明创建自己所需要的\n如果有更多需求可以点击 自定义来设置更多 如：吞吐量、加密、备份等策略\n最后点击 创建\n创建入站规则 允许来自 EKS 集群 VPC 的 CIDR 的入站 NFS 流量\n在刚刚创建的 EFS 中选择 网络 \u0026ndash;\u0026gt; 安全组 然后复制安全组的ID sg-152XXX\n在 EC2 中找到 网络与安全 选择 安全组 然后在搜索框中搜索 sg-152XXX 选中安全组。并选择 入站规则\n在入站规则中允许来 EKS 集群来访问 NFS(2049)端口流量。\n部署示例应用程序 部署静态供给 内容部署动态供给 部署静态供给 部署使用你创建的持久性卷的示例应用程序\n此过程利用来自 Amazon EFS Container Storage Interface (CSI) 驱动程序 GitHub 存储库的多个 Pod 读写许多示例来使用静态预置的 Amazon EFS 持久性卷，并使用 ReadWriteMany 访问模式从多个 Pod 访问它。\n将 Amazon EFS Container Storage Interface (CSI) 驱动程序 GitHub 存储库克隆到你的本地系统。\ngit clone https://github.com/kubernetes-sigs/aws-efs-csi-driver.git 导航到 multiple_pods 示例目录。\ncd aws-efs-csi-driver/examples/kubernetes/multiple_pods/ 检索你的 Amazon EFS 文件系统 ID。你可以在 Amazon EFS 控制台中查找此信息，或者使用以下 AWS CLI 命令。\naws efs describe-file-systems --query \u0026#34;FileSystems[*].FileSystemId\u0026#34; --output text 输出：\nfs-\u0026lt;582a03f3\u0026gt; 编辑 specs/pv.yaml 文件并将 volumeHandle 值替换为你的 Amazon EFS 文件系统 ID。\napiVersion: v1 kind: PersistentVolume metadata: name: efs-pv spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: efs-sc csi: driver: efs.csi.aws.com volumeHandle: fs-\u0026lt;582a03f3\u0026gt; 注意\n由于 Amazon EFS 是弹性文件系统，因此它不会强制实施任何文件系统容量限制。在创建系统时，不使用持久性卷和持久性卷声明中的实际存储容量值。但是，由于存储容量是 Kubernetes 中的必需字段，你必须指定有效值，例如，在此示例中为 5Gi。此值不会限制 Amazon EFS 文件系统的大小。\n从 specs 目录部署 efs-sc 存储类、efs-claim 持久性卷声明以及 efs-pv 持久性卷。\nkubectl apply -f specs/pv.yaml kubectl apply -f specs/claim.yaml kubectl apply -f specs/storageclass.yaml 列出默认命名空间中的持久性卷。查找具有 default/efs-claim 声明的持久性卷。\nkubectl get pv -w 输出：\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE efs-pv 5Gi RWX Retain Bound default/efs-claim efs-sc 2m50s 在 STATUS 变为 Bound 之前，请勿继续执行下一步。\n从 specs 目录部署 app1 和 app2 示例应用程序。\nkubectl apply -f specs/pod1.yaml kubectl apply -f specs/pod2.yaml 查看默认命名空间中的 Pod 并等待 app1 和 app2 Pod 的 STATUS 变为 Running 状态。\nkubectl get pods --watch 注意\n可能需要几分钟 Pod 才能达到 Running 状态。\n描述持久性卷。\nkubectl describe pv efs-pv 输出：\nName: efs-pv Labels: none Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;PersistentVolume\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;efs-pv\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;accessModes\u0026#34;:[\u0026#34;ReadWriteMany\u0026#34;],\u0026#34;capaci... pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: efs-sc Status: Bound Claim: default/efs-claim Reclaim Policy: Retain Access Modes: RWX VolumeMode: Filesystem Capacity: 5Gi Node Affinity: none Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: efs.csi.aws.com VolumeHandle: fs-582a03f3 ReadOnly: false VolumeAttributes: none Events: none Amazon EFS 文件系统 ID 将作为 VolumeHandle 列出。\n验证 app1 Pod 是否成功将数据写入卷。\nkubectl exec -ti app1 -- tail /data/out1.txt 输出：\n... Mon Mar 22 18:18:22 UTC 2021 Mon Mar 22 18:18:27 UTC 2021 Mon Mar 22 18:18:32 UTC 2021 Mon Mar 22 18:18:37 UTC 2021 ... 验证 app2 Pod 在卷中显示的数据与 app1 写入卷的数据相同。\nkubectl exec -ti app2 -- tail /data/out1.txt 输出：\n... Mon Mar 22 18:18:22 UTC 2021 Mon Mar 22 18:18:27 UTC 2021 Mon Mar 22 18:18:32 UTC 2021 Mon Mar 22 18:18:37 UTC 2021 ... 完成试验时，请删除此示例应用程序的资源以进行清理。\nkubectl delete -f specs/ 你还可以手动删除你创建的文件系统和安全组。\n部署动态供给 Prerequisite\n您必须使用 1.2x 版或更高版本的 Amazon EFS CSI 驱动程序，该驱动程序需要 1.17 或更高版本的集群。要更新集群，请参阅 更新集群。\n部署使用控制器所创建的持久性卷的示例应用程序\n此过程利用来自 Amazon EFS Container Storage Interface (CSI) 驱动程序 GitHub 存储库的动态预置示例。它通过 Amazon EFS 接入点和 Pod 使用的持久性卷申领 (PVC) 动态创建一个持久性卷。\n为 EFS 创建存储类。有关所有参数和配置选项，请参阅 GitHub 上的 Amazon EFS CSI 驱动程序。\n下载 Amazon EFS 的 StorageClass 清单。\ncurl -o storageclass.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml 编辑相应文件，将 fileSystemId 的值替换为您的文件系统 ID。\n部署存储类。\nkubectl apply -f storageclass.yaml 通过部署利用 PersistentVolumeClaim 的 Pod 来测试自动预置：\n下载一个清单，该清单将部署一个 Pod 和一个 PersistentVolumeClaim。\ncurl -o pod.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/pod.yaml 使用示例应用程序和 Pod 使用的 PersistentVolumeClaim 来部署 Pod。\nkubectl apply -f pod.yaml 确定运行控制器的 Pod 的名称。\nkubectl get pods -n kube-system | grep efs-csi-controller 输出\nefs-csi-controller-74ccf9f566-q5989 3/3 Running 0 40m efs-csi-controller-74ccf9f566-wswg9 3/3 Running 0 40m 几秒钟后，您可以观察到控制器开始接受更改（已编辑，旨在提高可读性）。将 74ccf9f566-q5989 替换成来自上一个命令输出中的一个 Pod 的值。\nkubectl logs efs-csi-controller-74ccf9f566-q5989 \\ -n kube-system \\ -c csi-provisioner \\ --tail 10 输出\n... 1 controller.go:737] successfully created PV pvc-5983ffec-96cf-40c1-9cd6-e5686ca84eca for PVC efs-claim and csi volume name fs-95bcec92::fsap-02a88145b865d3a87 如果未看到上一个输出，请使用其他控制器 Pod 之一运行上一个命令。\n确认已创建状态为 Bound 至 PersistentVolumeClaim 的持久性卷：\nkubectl get pv 输出\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-5983ffec-96cf-40c1-9cd6-e5686ca84eca 20Gi RWX Delete Bound default/efs-claim efs-sc 7m57s 查看有关所创建的 PersistentVolumeClaim 的详细信息。\nkubectl get pvc 输出\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE efs-claim Bound pvc-5983ffec-96cf-40c1-9cd6-e5686ca84eca 20Gi RWX efs-sc 9m7s 查看示例应用程序 Pod 的状态。\nkubectl get pods -o wide 输出\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES efs-example 1/1 Running 0 10m 192.168.78.156 ip-192-168-73-191.us-west-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 确认数据已写入到卷。\nkubectl exec efs-app -- bash -c \u0026#34;cat data/out\u0026#34; 输出\n... Tue Mar 23 14:29:16 UTC 2021 Tue Mar 23 14:29:21 UTC 2021 Tue Mar 23 14:29:26 UTC 2021 Tue Mar 23 14:29:31 UTC 2021 ... （可选）终止运行 Pod 的 Amazon EKS 节点并等待重新安排运行 Pod。或者，您也可以删除 Pod 并重新部署它。再次完成步骤 7，确认输出包含先前的输出。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/220110850573/\n相关话题：https://www.cnsre.cn/tags/eks/\n","description":"如何在 Amazon Elastic Kubernetes Service (Amazon EKS) 中使用 EFS 实现持久性存储。","id":10,"section":"posts","tags":["eks","kubernetes","efs","aws"],"title":"Amazon EKS 中 EFS 持久性存储","uri":"https://www.cnsre.cn/posts/220110850573/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/220107906441/\n相关话题：https://www.cnsre.cn/tags/eks/\n介绍 最近一直在玩EKS（Elastic Kubernetes Service \u0026ndash; Amazon EKS） 和 kubesphere。 因为之前没有使用过EKS 和 kubesphere 所以这个过程也是一个试错的过程，在我使用 kubesphere 的时候发现有一个日志服务，在好奇心的驱使下，我创建了它。\n在我创建了日志服务（KubeSphere Logging System）以后，我发现我并不想使用它。（可能我只是想看看它到底是什么吧。）强迫症我的，就想把它给删除掉。于是我在我的 EKS 中对他进行了强制删除 kubectl delete ns kubesphere-logging-system --force --grace-period=0\n让人尴尬的是，这个 Namespace 并没有立马删除，我自我安慰到，可能 Namespace 下边有其他没有删除的资源在等待删除，我在等等。。。\n过了半个小时，我去查看的时候\n[root@ip-10-0-100-206 ~]# kubectl get ns kubesphere-logging-system NAME STATUS AGE kubesphere-logging-system Terminating 6d19h 它好像这地卡在了 Terminating 的状态。\n我试着寻找解决方法，http://github.com/kubernetes/kubernetes/issues/60807 但是这中方法要通过 API 才可以实现。EKS 是托管在 AWS 中的，我根本没有办法去操作eks的后台。\n我开始后悔我为什么要去安装它，我本可以不折腾的。但是回想如果我不去安装它的话，我肯定会像想在这样难过，因为我无法了解它（KubeSphere Logging System）到底是什么样的。\n终于我找到了 commented https://github.com/kubernetes/kubernetes/issues/60807#issuecomment-663853215\n如何彻底删除 namespace 获取namespace的详情信息并转为json 1 kubectl get namespace kubesphere-logging-system -o json \u0026gt; kubesphere-logging-system.json 打开 json 文件编辑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2021-12-31T05:03:58Z\u0026#34;, \u0026#34;deletionTimestamp\u0026#34;: \u0026#34;2022-01-05T08:05:40Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;kubesphere.io/namespace\u0026#34;: \u0026#34;kubesphere-logging-system\u0026#34;, \u0026#34;kubesphere.io/workspace\u0026#34;: \u0026#34;system-workspace\u0026#34; }, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:metadata\u0026#34;: { \u0026#34;f:labels\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:kubesphere.io/namespace\u0026#34;: {} }, \u0026#34;f:ownerReferences\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;k:{\\\u0026#34;uid\\\u0026#34;:\\\u0026#34;6d535470-2592-4f3c-a155-eabc362c339d\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:apiVersion\u0026#34;: {}, \u0026#34;f:blockOwnerDeletion\u0026#34;: {}, \u0026#34;f:controller\u0026#34;: {}, \u0026#34;f:kind\u0026#34;: {}, \u0026#34;f:name\u0026#34;: {}, \u0026#34;f:uid\u0026#34;: {} } } } }, \u0026#34;manager\u0026#34;: \u0026#34;controller-manager\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-12-31T05:04:01Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:metadata\u0026#34;: { \u0026#34;f:labels\u0026#34;: { \u0026#34;f:kubesphere.io/workspace\u0026#34;: {} } }, \u0026#34;f:status\u0026#34;: { \u0026#34;f:phase\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;kubectl\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-12-31T05:04:01Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:conditions\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceContentRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionContentFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionDiscoveryFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionGroupVersionParsingFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceFinalizersRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} } } } }, \u0026#34;manager\u0026#34;: \u0026#34;kube-controller-manager\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;kubesphere-logging-system\u0026#34;, \u0026#34;ownerReferences\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;tenant.kubesphere.io/v1alpha1\u0026#34;, \u0026#34;blockOwnerDeletion\u0026#34;: true, \u0026#34;controller\u0026#34;: true, \u0026#34;kind\u0026#34;: \u0026#34;Workspace\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;system-workspace\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;6d535470-2592-4f3c-a155-eabc362c339d\u0026#34; } ], \u0026#34;resourceVersion\u0026#34;: \u0026#34;7376520\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;2b76e9b1-75f2-4a2e-a819-73b36aea188e\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;finalizers\u0026#34;: [ \u0026#34;kubernetes\u0026#34; # 将此行删除 ] }, \u0026#34;status\u0026#34;: { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All resources successfully discovered\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ResourcesDiscovered\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionDiscoveryFailure\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All legacy kube types successfully parsed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ParsedGroupVersions\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionGroupVersionParsingFailure\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content successfully deleted, may be waiting on finalization\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentDeleted\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionContentFailure\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Some resources are remaining: fluentbits.logging.kubesphere.io has 1 resource instances\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;SomeResourcesRemain\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceContentRemaining\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Some content in the namespace has finalizers remaining: fluentbit.logging.kubesphere.io in 1 resource instances\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;SomeFinalizersRemain\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceFinalizersRemaining\u0026#34; } ], \u0026#34;phase\u0026#34;: \u0026#34;Terminating\u0026#34; } } 找到 spec 将 finalizers 下的 kubernetes 删除\n具体如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2021-12-31T05:03:58Z\u0026#34;, \u0026#34;deletionTimestamp\u0026#34;: \u0026#34;2022-01-05T08:05:40Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;kubesphere.io/namespace\u0026#34;: \u0026#34;kubesphere-logging-system\u0026#34;, \u0026#34;kubesphere.io/workspace\u0026#34;: \u0026#34;system-workspace\u0026#34; }, \u0026#34;managedFields\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:metadata\u0026#34;: { \u0026#34;f:labels\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:kubesphere.io/namespace\u0026#34;: {} }, \u0026#34;f:ownerReferences\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;k:{\\\u0026#34;uid\\\u0026#34;:\\\u0026#34;6d535470-2592-4f3c-a155-eabc362c339d\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:apiVersion\u0026#34;: {}, \u0026#34;f:blockOwnerDeletion\u0026#34;: {}, \u0026#34;f:controller\u0026#34;: {}, \u0026#34;f:kind\u0026#34;: {}, \u0026#34;f:name\u0026#34;: {}, \u0026#34;f:uid\u0026#34;: {} } } } }, \u0026#34;manager\u0026#34;: \u0026#34;controller-manager\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-12-31T05:04:01Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:metadata\u0026#34;: { \u0026#34;f:labels\u0026#34;: { \u0026#34;f:kubesphere.io/workspace\u0026#34;: {} } }, \u0026#34;f:status\u0026#34;: { \u0026#34;f:phase\u0026#34;: {} } }, \u0026#34;manager\u0026#34;: \u0026#34;kubectl\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2021-12-31T05:04:01Z\u0026#34; }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;fieldsType\u0026#34;: \u0026#34;FieldsV1\u0026#34;, \u0026#34;fieldsV1\u0026#34;: { \u0026#34;f:status\u0026#34;: { \u0026#34;f:conditions\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceContentRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionContentFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionDiscoveryFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceDeletionGroupVersionParsingFailure\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} }, \u0026#34;k:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;NamespaceFinalizersRemaining\\\u0026#34;}\u0026#34;: { \u0026#34;.\u0026#34;: {}, \u0026#34;f:lastTransitionTime\u0026#34;: {}, \u0026#34;f:message\u0026#34;: {}, \u0026#34;f:reason\u0026#34;: {}, \u0026#34;f:status\u0026#34;: {}, \u0026#34;f:type\u0026#34;: {} } } } }, \u0026#34;manager\u0026#34;: \u0026#34;kube-controller-manager\u0026#34;, \u0026#34;operation\u0026#34;: \u0026#34;Update\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;kubesphere-logging-system\u0026#34;, \u0026#34;ownerReferences\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;tenant.kubesphere.io/v1alpha1\u0026#34;, \u0026#34;blockOwnerDeletion\u0026#34;: true, \u0026#34;controller\u0026#34;: true, \u0026#34;kind\u0026#34;: \u0026#34;Workspace\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;system-workspace\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;6d535470-2592-4f3c-a155-eabc362c339d\u0026#34; } ], \u0026#34;resourceVersion\u0026#34;: \u0026#34;7376520\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;2b76e9b1-75f2-4a2e-a819-73b36aea188e\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;finalizers\u0026#34;: [ ] }, \u0026#34;status\u0026#34;: { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All resources successfully discovered\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ResourcesDiscovered\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionDiscoveryFailure\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All legacy kube types successfully parsed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ParsedGroupVersions\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionGroupVersionParsingFailure\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;All content successfully deleted, may be waiting on finalization\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;ContentDeleted\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceDeletionContentFailure\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Some resources are remaining: fluentbits.logging.kubesphere.io has 1 resource instances\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;SomeResourcesRemain\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceContentRemaining\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2022-01-05T08:05:47Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Some content in the namespace has finalizers remaining: fluentbit.logging.kubesphere.io in 1 resource instances\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;SomeFinalizersRemain\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;NamespaceFinalizersRemaining\u0026#34; } ], \u0026#34;phase\u0026#34;: \u0026#34;Terminating\u0026#34; } } 执行清理命令 现在我们只需要一条命令 就可以彻底删除这个 Namespace\n1 kubectl replace --raw \u0026#34;/api/v1/namespaces/kubesphere-logging-system/finalize\u0026#34; -f ./kubesphere-logging-system.json 执行完以后，你需要等待一会\n再次执行命令检查 namespaces\n1 kubectl replace --raw \u0026#34;/api/v1/namespaces/kubesphere-logging-system/finalize\u0026#34; -f ./kubesphere-logging-system.json 最后的检查 1 2 3 4 5 6 7 8 9 10 11 12 13 kubectl get ns kubesphere-logging-system Error from server (NotFound): namespaces \u0026#34;kubesphere-logging-system\u0026#34; not found [root@ip-10-0-100-206 ~]# kubectl get ns NAME STATUS AGE default Active 23d kubesphere-controls-system Active 9d kubesphere-devops-system Active 9d kubesphere-devops-worker Active 16h kubesphere-monitoring-federated Active 9d kubesphere-monitoring-system Active 9d kubesphere-sample-dev Active 8d kubesphere-system Active 9d 在此查看的时候，它已经不存在了。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/220107906441/\n相关话题：https://www.cnsre.cn/tags/eks/\n","description":"删除 Namespace 失败 Namespace 一直处于Terminating 的状态 如何彻底删除EKS中一直卡在Terminating的Namespace","id":11,"section":"posts","tags":["eks","故障集","kubernetes"],"title":"如何彻底删除EKS中一直卡在Terminating的Namespace","uri":"https://www.cnsre.cn/posts/220107906441/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211217431135/\n相关话题：https://www.cnsre.cn/tags/aws/\n最近在AWS 平台创建了EKS 用于测试环境项目，EKS 创建完以后我打算使用 Ingress 控制器 来暴露服务，ingress 前在添加一个ALB 负载均衡器，这样就可以实现完全的高可用了。但是在创建好 ingress 却发现无法调通服务，查看 aws 官方文档 Amazon EKS 上的应用程序负载均衡 发现需要使用 aws-load-balancer-controller.\n本文档的目标：\n创建 EKS ALB 所需要的角色。\n创建 EKS aws-load-balancer-controller\n创建 EKS pod 服务\n使用 ALB 将 pod 服务暴露出去\nAWS Load Balancer Controller 介绍 AWS Load Balancer Controller 的工作原理 AWS Load Balancer Controller 是帮助管理 Kubernetes 集群的弹性负载均衡器的控制器。\n它通过供应应用程序负载均衡器来满足 Kubernetes Ingress 资源。 它通过提供 网络负载均衡器来满足 Kubernetes Service 资源。 下图详细说明了此控制器创建的 AWS 组件。它还演示了从 ALB 到 Kubernetes 集群的路由入口流量。\n[1]：控制器监视来自 API 服务器的入口事件。当它找到满足其要求的入口资源时，它开始创建 AWS 资源。\n[2]：在 AWS 中为新的入口资源创建了一个 ALB (ELBv2)。此 ALB 可以面向 Internet 或内部。您还可以使用注释指定它在其中创建的子网。\n[3]：在 AWS 中为入口资源中描述的每个独特的 Kubernetes 服务创建目标组。\n[4]：为入口资源注释中详述的每个端口创建侦听器。如果未指定端口，则使用合理的默认值 (80或443)。证书也可以通过注释附加。\n[5]：为入口资源中指定的每个路径创建规则。这可确保将特定路径的流量路由到正确的 Kubernetes 服务。\n安装前的准备 EKS 已经创建完毕 准备 两个 Public 子网 能够创建 IAM 策略 的账户 关于子网的说明：\n根据EKS 最佳实践。EKS 的 worker node，它们只需要接收来自 alb ingress（通过内网转发）的流量，安全起见就需要把他们放在私有子网。但是它们又需要去公网上拉一些镜像，所以它们本身也需要放问公网的能力，这个时候它们的子网里配置个 nat，访问外网的时候由 NAT 做一个出向的转发，就可以实现了，但是因为 nat 是单向的，外界是无法通过NAT访问到eks的节点的，所以我就需要将ALB 放在 public 子网里。最后就是 ALB 放在 public 来接受流量，worker node 在私有子网处理业务。\n创建AWS Load Balancer Controller 的 IAM 策略 打开 策略 点击 创建策略 打开 IAM_Policy.json 复制内容粘贴到 json\n点击下一步:标签\n然后一直下一步 在下图中名称填写 AWSLoadBalancerControllerIAMPolicy 你也可以自定义名称。然后创建策略。\n如果以官方提供的 IAM_Policy.json 保存有错的话你可以使用一下策略（权限会大一些） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateServiceLinkedRole\u0026#34;, \u0026#34;ec2:DescribeAccountAttributes\u0026#34;, \u0026#34;ec2:DescribeAddresses\u0026#34;, \u0026#34;ec2:DescribeAvailabilityZones\u0026#34;, \u0026#34;ec2:DescribeInternetGateways\u0026#34;, \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;ec2:GetCoipPoolUsage\u0026#34;, \u0026#34;ec2:DescribeCoipPools\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancers\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026#34;, \u0026#34;elasticloadbalancing:DescribeListeners\u0026#34;, \u0026#34;elasticloadbalancing:DescribeListenerCertificates\u0026#34;, \u0026#34;elasticloadbalancing:DescribeSSLPolicies\u0026#34;, \u0026#34;elasticloadbalancing:DescribeRules\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTargetGroups\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTargetGroupAttributes\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTargetHealth\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:DescribeUserPoolClient\u0026#34;, \u0026#34;acm:ListCertificates\u0026#34;, \u0026#34;acm:DescribeCertificate\u0026#34;, \u0026#34;iam:ListServerCertificates\u0026#34;, \u0026#34;iam:GetServerCertificate\u0026#34;, \u0026#34;waf-regional:GetWebACL\u0026#34;, \u0026#34;waf-regional:GetWebACLForResource\u0026#34;, \u0026#34;waf-regional:AssociateWebACL\u0026#34;, \u0026#34;waf-regional:DisassociateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:GetWebACLForResource\u0026#34;, \u0026#34;wafv2:AssociateWebACL\u0026#34;, \u0026#34;wafv2:DisassociateWebACL\u0026#34;, \u0026#34;shield:GetSubscriptionState\u0026#34;, \u0026#34;shield:DescribeProtection\u0026#34;, \u0026#34;shield:CreateProtection\u0026#34;, \u0026#34;shield:DeleteProtection\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:CreateSecurityGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:CreateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws-cn:ec2:*:*:security-group/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ec2:CreateAction\u0026#34;: \u0026#34;CreateSecurityGroup\u0026#34; }, \u0026#34;Null\u0026#34;: { \u0026#34;aws:RequestTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;false\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws-cn:ec2:*:*:security-group/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;aws:RequestTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;false\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;false\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:CreateLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:CreateTargetGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;aws:RequestTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;false\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:CreateListener\u0026#34;, \u0026#34;elasticloadbalancing:DeleteListener\u0026#34;, \u0026#34;elasticloadbalancing:CreateRule\u0026#34;, \u0026#34;elasticloadbalancing:DeleteRule\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:AddTags\u0026#34;, \u0026#34;elasticloadbalancing:RemoveTags\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:targetgroup/*/*\u0026#34;, \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:loadbalancer/net/*/*\u0026#34;, \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:loadbalancer/app/*/*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;aws:RequestTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;false\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:AddTags\u0026#34;, \u0026#34;elasticloadbalancing:RemoveTags\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:listener/net/*/*/*\u0026#34;, \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:listener/app/*/*/*\u0026#34;, \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:listener-rule/net/*/*/*\u0026#34;, \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:listener-rule/app/*/*/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026#34;, \u0026#34;elasticloadbalancing:SetIpAddressType\u0026#34;, \u0026#34;elasticloadbalancing:SetSecurityGroups\u0026#34;, \u0026#34;elasticloadbalancing:SetSubnets\u0026#34;, \u0026#34;elasticloadbalancing:DeleteLoadBalancer\u0026#34;, \u0026#34;elasticloadbalancing:ModifyTargetGroup\u0026#34;, \u0026#34;elasticloadbalancing:ModifyTargetGroupAttributes\u0026#34;, \u0026#34;elasticloadbalancing:DeleteTargetGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026#34;: \u0026#34;false\u0026#34; } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:RegisterTargets\u0026#34;, \u0026#34;elasticloadbalancing:DeregisterTargets\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws-cn:elasticloadbalancing:*:*:targetgroup/*/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;elasticloadbalancing:SetWebAcl\u0026#34;, \u0026#34;elasticloadbalancing:ModifyListener\u0026#34;, \u0026#34;elasticloadbalancing:AddListenerCertificates\u0026#34;, \u0026#34;elasticloadbalancing:RemoveListenerCertificates\u0026#34;, \u0026#34;elasticloadbalancing:ModifyRule\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 赋予 EKS node 权限 在 角色 中搜索 nodegrou-NodeInstanceRole 找到你对应的 EKS 集群 如下图\n然后点击该角色\u0026ndash; 点击附加策略\n在搜索框内 输入刚才创建的策略名称 然后选中，点击最下边的附加策略。\n我的策略名称为：AWSLoadBalancerControllerIAMPolicy\n在 EKS 中安装 AWS Load Balancer Controller 安装证书管理器 1 kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.yaml 部署 YAML 下载负载平衡器控制器的规范。\n1 wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.3.1/v2_3_1_full.yaml 编辑保存的 yaml 文件，转到部署规范，并将控制器 \u0026ndash;cluster-name arg 值设置为您的 EKS 集群名称\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: apps/v1 kind: Deployment . . . name: aws-load-balancer-controller namespace: kube-system spec: . . . template: spec: containers: - args: - --cluster-name=\u0026lt;INSERT_CLUSTER_NAME\u0026gt; 如果您为服务账户使用 IAM 角色，我们建议您从 yaml 规范中删除 ServiceAccount。如果您从 yaml 规范中删除安装部分，这将保留 eksctl 创建的 iamserviceaccount。\n1 2 apiVersion: v1 kind: ServiceAccount 应用 yaml 文件\n1 kubectl apply -f v2_3_1_full.yaml 部署示例应用程序 将游戏 2048 部署为示例应用程序，以确认作为入口对象的结果，Amazon负载均衡器控制器是否会创建 Amazon ALB。\n1 kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/examples/2048/2048_full.yaml 几分钟后，验证是否已使用以下命令创建入口资源。\n1 kubectl get ingress/ingress-2048 -n game-2048 输出：\n1 2 NAME CLASS HOSTS ADDRESS PORTS AGE ingress-2048 \u0026lt;none\u0026gt; * k8s-game2048-ingress2-xxxxxxxxxx-yyyyyyyyyy.cn-north-1.elb.amazonaws.com.cn 80 3h42m 如果在几分钟后尚未创建入口，请运行以下命令以查看负载均衡器控制器日志。这些日志包含可帮助您诊断部署中任何问题的错误消息。 1 kubectl logs -n kube-system deployment.apps/aws-load-balancer-controller 打开浏览器并从上一命令输出导航到 ADDRESS URL 以查看示例应用程序。如果您没有看到任何内容，请等待几分钟，并刷新您的浏览器。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211217431135/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"eks 怎么配合alb 使用ingress? eks安装绑定alb ingress.","id":12,"section":"posts","tags":["kubernetes","aws","eks","ingress","alb"],"title":"AWS eks绑定alb 使用aws-load-balancer-controller(Ingress Controller)提供服务","uri":"https://www.cnsre.cn/posts/211217431135/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211213210004/\n相关话题：https://www.cnsre.cn/tags/log4j/\n近日的Log4j2，可是非常的火啊，我也是加班加点把补丁给打上了次安心。Apache Log4j2存在远程代码执行漏洞，经验证，该漏洞允许攻击者在目标服务器上执行任意代码，可导致服务器被黑客控制。由于Apache Log4j 2应用较为广泛，建议使用该组件的用户尽快采取安全措施。\n影响范围 漏洞影响版本：\n2.0 \u0026lt;= Apache Log4j 2 \u0026lt;= log4j-2.15.0-rc1\n漏洞描述 Apache Log4j 2是一个基于Java的日志记录工具，是对 Log4j 的升级。近日安恒信息应急响应中心监测到Apache Log4j 2存在远程代码执行漏洞，攻击者可通过构造恶意请求利用该漏洞实现在目标服务器上执行任意代码。\n漏洞修复 由于Log4j2 作为日志记录基础第三方库，被大量Java框架及应用使用，只要用到 Log4j2 进行日志输出且日志内容能被攻击者部分可控，即可能会受到漏洞攻击影响。因此，该漏洞也同时影响全球大量通用应用及组件，例如 ：\nApache Struts2、Apache Solr、Apache Druid、Apache Flink、Apache Flume、Apache Dubbo、Apache Kafka、Spring-boot-starter-log4j2、ElasticSearch、Redis、Logstash等\n建议及时检查并升级所有使用了 Log4j 组件的系统或应用。\n紧急： 目前漏洞POC已被公开，官方已发布安全版本，建议使用该组件的用户尽快采取安全措施。\n临时性缓解措施： 1、在 jvm 参数中添加 -Dlog4j2.formatMsgNoLookups=true\n2、系统环境变量中将LOG4J_FORMAT_MSG_NO_LOOKUPS 设置为 true\n3、创建 log4j2.component.properties 文件，文件中增加配置 log4j2.formatMsgNoLookups=true\n4、若相关用户暂时无法进行升级操作，也可通过禁止Log4j中SocketServer类所启用的socket端对公网开放来进行防护\n5、禁止安装log4j的服务器访问外网，并在边界对dnslog相关域名访问进行检测。部分公共dnslog平台如下\nceye.io dnslog.link dnslog.cn dnslog.io tu4.org awvsscan119.autoverify.cn burpcollaborator.net s0x.cn 彻底修复漏洞： 建议您在升级前做好数据备份工作，避免出现意外\n研发代码修复：升级到官方提供的 log4j-2.15.0-rc2 版本\nhttps://github.com/apache/logging-log4j2/releases/tag/log4j-2.15.1-rc1\n漏洞检测工具 检测工具下载地址 https://pan.cnsre.cn/d/Package/Linux/360log4j2.zip\n漏洞检测 浏览器被动式扫描检测方案 原理\n工程师可设置该代理通过浏览器被动扫描目标，查看 DNS Log 检测是否存在 log4j 漏洞。 使用方法\n1.浏览器或操作系统配置 HTTP/HTTPS 代理：219.141.219.69:18080 2.浏览器或操作系统将下列证书添加到信任名单：附件sqli-hunter.pem\n3.使用浏览器正常进行目标浏览，当结束扫描后，在http://219.141.219.69:18000/ 下检查是否存在以目标域名为名的 txt 文件，如 http://219.141.219.69/360.cn.txt\n4.若存在，则说明目标网站存在漏洞，细节如下：\n可看到完整 HTTP 请求细节，params参数为存在 log4j 注入漏洞的参数\n使用限制 主机外网 IP 无法访问 360 IP，请不要使用该代理扫描 360 目前只能检测 POST body 中的参数 不允许任何恶意攻击 本地扫描常规检测方案 下载本地检测工具\n扫描源码：./log4j-discoverer \u0026ndash;src\u0026quot;源码目录\u0026quot;\n扫描jar包：./log4j-discoverer\u0026ndash;jar \u0026ldquo;jar包文件\u0026rdquo;\n扫描系统进程：./log4j-discoverer –scan\nLog4j漏洞补丁方案 如果检测到相关漏洞的应用或组件，建议立即对该应用或组件进行打补丁修复， Log4j补丁方案如下：\n工具原理 Hook前受到log4j jndi注入攻击\n执行 java -jar PatchLog4j.jar\n打入补丁后 log4j不再处理JNDI逻辑直接将JNDI字符串输出\n工具来源【360政企安服高攻实验室】\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211213210004/\n相关话题：https://www.cnsre.cn/tags/log4j/\n","description":"Apache Log4j 2是一个基于Java的日志记录工具，是对 Log4j 的升级。近日安恒信息应急响应中心监测到Apache Log4j 2存在远程代码执行漏洞，攻击者可通过构造恶意请求利用该漏洞实现在目标服务器上执行任意代码。","id":13,"section":"posts","tags":["安全","log4j"],"title":"Log4j 漏洞修复检测 附检测工具","uri":"https://www.cnsre.cn/posts/211213210004/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211210952578/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\nkubecolor 是什么？ kubecolor 为您的kubectl命令输出着色，不执行任何其他操作。kubecolor 在内部调用kubectlcommand 并尝试对输出进行着色，以便你可以使用 kubecolor 作为 kubectl 的完整替代方案。\nkubecolor项目地址\n安装 通过 GitHub 发布下载 git clone https://github.com/dty1er/kubecolor.git 通过 go 命令手动构建 cd kubecolor/ go build -o kubecolor cmd/kubecolor/main.go 构建后，得到一个 kubecolor 的文件\n设置默认 kubectl 使用 kubecolor echo \u0026#34;alias kubectl=\u0026#39;/root/kubecolor/kubecolor\u0026#34; \u0026gt;\u0026gt; ~/.bashrc # 使配置生效 source .bashrc 效果展示 kubectl get pods\nkubectl describe\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211210952578/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"kubecolor 为您的kubectl命令输出着色，不执行任何其他操作。","id":14,"section":"posts","tags":["kubernetes","kubecolor"],"title":"kubectl 彩色输出 -- kubecolor","uri":"https://www.cnsre.cn/posts/211210952578/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211208038353/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n一、kubernetes 集群架构图 二、Openshift or Kubernetes 集群架构图 三、常见的 CI/CD 架构图 1、Gitlab Webhook + Jenkins SharedLibraries/Kubernetes + SonarScanner Maven Plugin 2、Gitlab CI/CD Workflow 3、Logging 4、Logging与Metrics 原文出处\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211208038353/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"kubernetes生态架构图，devops架构图","id":15,"section":"posts","tags":["kubernetes"],"title":"kubernetes架构图","uri":"https://www.cnsre.cn/posts/211208038353/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211206944174/\n相关话题：https://www.cnsre.cn/tags/aws/\n什么是 Amazon Elastic Container Registry（ECR）？ Amazon Elastic Container Registry (Amazon ECR) 是 Amazon 托管容器映像注册表服务，它安全、可扩展且可靠。Amazon ECR 支持私有存储库，其具有使用 Amazon IAM 的基于资源的权限。这样，指定用户或 Amazon EC2 实例可以访问您的容器存储库和映像。您可以使用首选 CLI 推送、提取和管理 Docker 映像、Open Container Itistry (OCI) 映像和 OCI 兼容构件。\nAmazon ECR 的功能 Amazon ECR 提供以下功能：\n生命周期策略有助于管理存储库中映像的生命周期。您可以定义导致清理未使用映像的规则。您可以在将规则应用到存储库之前对其进行测试。有关更多信息，请参阅 生命周期策略。 映像扫描有助于识别容器映像中的软件漏洞。每个存储库都可以配置为在推送时扫描。这可确保扫描推送到存储库的每个新映像。然后，您可以检索映像扫描的结果。有关更多信息，请参阅 映像扫描。 跨区域和跨账户复制使您可以更轻松地将映像放置在需要的位置。它配置为注册表设置，并基于每个区域。有关更多信息，请参阅 私有注册表设置。 推送 Docker 映像到 Amazon ECR 存储库 我们可以使用 docker push 命令将容器映像推送到 Amazon ECR 存储库。Amazon ECR 还支持创建和推送用于多架构映像的 Docker 清单列表。清单列表中引用的每个映像都必须已经被推送到我们的存储库。\n在推送映像之前，Amazon ECR 存储库必须存在。 向要向其推送映像的 Amazon ECR 注册表验证 Docker 客户端的身份。必须针对每个注册表获得授权令牌，令牌有效期为 12 小时。\n要对 Amazon ECR 注册表验证 Docker，请运行 aws ecr get-login-password 命令。将身份验证令牌传递给 docker login 命令时，将值 AWS 用作用户名，并指定要对其进行身份验证的 Amazon ECR 注册表 URI。如果对多个注册表进行身份验证，则必须针对每个注册表重复该命令。\n如果收到错误，请安装或更新到最新版本的 Amazon CLI。有关更多信息，请参阅 Amazon Command Line Interface 用户指南中的安装 Amazon Command Line Interface。 检索身份验证令牌并向注册表验证 Docker 客户端身份。 使用 亚马逊云科技 CLI:\naws ecr get-login-password --region cn-north-1 | docker login --username AWS --password-stdin 12345678.dkr.ecr.cn-north-1.amazonaws.com.cn 如果您在使用 亚马逊云科技 CLI 时遇到错误，请确保您已安装最新版本的 亚马逊云科技 CLI 和 Docker。 构建 Docker 映像 如果您已生成映像，则可跳过此步骤:\ndocker build -t cnsre-test . 标记映像 生成完成后，标记您的映像，以便将映像推送到此存储库:\ndocker tag cnsre-test:latest 12345678.dkr.ecr.cn-north-1.amazonaws.com.cn/cnsre-test:latest 推送镜像 运行以下命令将此映像推送到新创建的 亚马逊云科技 存储库:\ndocker push 12345678.dkr.ecr.cn-north-1.amazonaws.com.cn/cnsre-test:latest 出现 Head https://registry-1.docker.io/v2/library/node/manifests/14-alpine 的解决方法 vim /etc/docker/daemon.json # 添加国内的镜像 阿里云镜像 { \u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://6kx4zyno.mirror.aliyuncs.com\u0026#34;] } # 或者中科院镜像 \u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;] } 重启Docker\nsystemctl daemon-reload systemctl restart docker 拉取镜像 使用 docker pull 命令提取映像。映像名称格式应为 registry/repository[:tag] 以便按标签拉取，或为 registry/repository[@digest] 以便按摘要拉取。\ndocker pull 12345678.dkr.ecr.cn-north-1.amazonaws.com.cn/cnsre-test:latest 如果您收到 repository-url not found: does not exist or no pull access 错误，您可能需要向 Amazon ECR 验证您的 Docker 客户端。有关更多信息，请参阅 私有注册表身份验证。 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211206944174/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"本文学习目标是了解aws ECR仓库，并学会AWS私有仓库ECR的镜像推送和拉取。","id":16,"section":"posts","tags":["aws","ecr","docker"],"title":"AWS私有仓库ECR推送拉取镜像","uri":"https://www.cnsre.cn/posts/211206944174/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211203931498/\n相关话题：https://www.cnsre.cn/tags/aws/\n因为创建 Amazon EKS 集群时，IAM 用户或角色会自动在集群的 RBAC 配置中被授予 system:masters 权限。例如，IAM 用户或角色可以是创建集群的联合身份用户。如果使用不属于 aws-auth ConfigMap 的 IAM 用户或角色访问 Amazon EKS 控制台，则无法看到 Kubernetes 工作负载。也不会看到集群的概览详细信息。所以要向其他 AWS 用户或角色授予与集群交互的能力，您必须在 Kubernetes 中编辑 aws-auth ConfigMap。\n因为部门有不通的角色，所以想基于不通的角色分配不同的权限，下面是记录添加一个对 EKS 只有只读权限的 AIM 用户。\n如果您在运行 AWS 命令行界面 (AWS CLI) 命令时遇到错误，请确保您使用的是最新版的 AWS CLI。 为 IAM 用户或角色配置权限 要查找具有主要集群配置权限的集群创建者或管理员角色，请在 AWS CloudTrail 中搜索 CreateCluster API 调用。然后，检查此 API 调用的 UserIdentity 部分。 识别需要权限的 IAM 用户或角色。 确认已识别的 IAM 用户或角色有权在 AWS 管理控制台中查看所有集群的节点和工作负载。 使用 aws-auth ConfigMap 将 IAM 用户或角色映射到 RBAC 角色和组 在连接 Amazon EKS API 服务器之前，安装并配置最新版本的 AWS CLI。 获取 AWS CLI 用户或角色的配置：\naws sts get-caller-identity 输出将返回 IAM 用户或角色的 Amazon 资源名称 (ARN)。例如：\n{ \u0026#34;UserId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;XXXXXXXXXXXX\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::XXXXXXXXXXXX:user/testuser\u0026#34; } 确认 ARN 与具有主要集群配置访问权限的集群创建者或管理员相匹配。如果 ARN 与集群创建者或管理员不匹配，请联系集群创建者 aws-auth ConfigMap。\n添加对 EKS 集群具有只读访问权限的 IAM 用户 要允许超级用户访问权限以对任何资源执行任何操作，请添加 system:masters 而非 system:bootstrappers 和 system:nodes。有关更多信息，请参阅 Kubernetes 网站上的默认角色和角色绑定。 创建 rbac.yaml\n--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: reader rules: - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;configmaps\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;secrets\u0026#34;, \u0026#34;services\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: reader subjects: - kind: Group name: reader apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: reader apiGroup: rbac.authorization.k8s.io 要添加 IAM 用户或角色，请完成以下步骤之一。\n添加 IAM 用户到 mapUsers。\n... mapUsers: | - userarn: arn:aws:iam::424432388155:user/developer username: developer groups: - reader ... 创建 RBAC\nkubectl apply -f rbac.yaml 在AWS 中创建 AmazonEKSDeveloperPolicy 策略以让用户在 AWS 管理控制台中查看所有集群的节点和工作负载\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;eks:DescribeNodegroup\u0026#34;, \u0026#34;eks:ListNodegroups\u0026#34;, \u0026#34;eks:DescribeCluster\u0026#34;, \u0026#34;eks:ListClusters\u0026#34;, \u0026#34;eks:AccessKubernetesApi\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;eks:ListUpdates\u0026#34;, \u0026#34;eks:ListFargateProfiles\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 创建eks-developerIAM 组并附加AmazonEKSDeveloperPolicy策略 创建developer用户 添加developer个人资料aws configure --profile developer 添加到aws-authconfigmapdeveloper用户 ARN。 kubectl edit -n kube-system configmap/aws-auth ... mapUsers: | - userarn: arn:aws:iam::424432388155:user/developer username: developer groups: - reader ... 为developer用户配置 kubectl 上下文 1 aws eks --region us-east-1 update-kubeconfig --name eks --profile developer 检查 kubeconfig 1 kubectl config view --minify 检查权限 1 2 3 kubectl auth can-i get pods kubectl auth can-i create pods kubectl run nginx --image=nginx 创建具有管理员访问权限的 IAM 角色并由 IAM 用户代入此角色。 创建AmazonEKSAdminPolicy策略 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;eks:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:PassRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PassedToService\u0026#34;: \u0026#34;eks.amazonaws.com\u0026#34; } } } ] } 创建eks-admin角色并附加AmazonEKSAdminPolicy策略 描述eks-admin角色 1 aws iam get-role --profile terraform --role-name eks-admin 创建AmazonEKSAssumePolicy允许承担角色的策略 1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::424432388155:role/eks-admin\u0026#34; } ] } 创建manager用户以使用eks-admin角色 添加manager个人资料aws configure --profile manager 检查manager用户是否可以承担eks-admin角色 1 aws sts assume-role --role-arn arn:aws:iam::424432388155:role/eks-admin --role-session-name manager-session --profile manager 为创建 EKS 集群的用户更新 kubeconfig 1 aws eks --region us-east-1 update-kubeconfig --name eks --profile terraform 添加到aws-authconfigmapeks-admin角色 ARN。 1 2 3 4 5 6 7 kubectl edit -n kube-system configmap/aws-auth ... - rolearn: arn:aws:iam::424432388155:role/eks-admin username: eks-admin groups: - system:masters ... 创建eks-admin配置文件以承担角色vim ~/.aws/config [profile eks-admin] role_arn = arn:aws:iam::424432388155:role/eks-admin source_profile = manager 为manager用户配置 kubectl 上下文以自动承担eks-admin角色 1 aws eks --region us-east-1 update-kubeconfig --name eks --profile eks-admin 检查 kubeconfig 1 kubectl config view --minify 检查经理是否具有 EKS 集群的管理员访问权限 1 kubectl auth can-i \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211203931498/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"添加对 EKS 集群具有只读访问权限的 IAM 用户,添加具有根访问权限的 IAM 角色并由 IAM 用户代入此角色","id":17,"section":"posts","tags":["aws","eks","kubernetes"],"title":"AWS  EKS 添加IAM用户角色","uri":"https://www.cnsre.cn/posts/211203931498/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211201955418/\n相关话题：https://www.cnsre.cn/tags/split/\n1. 分割文件 文件分割可以使用split命令，该即支持文本文件分割，又支持二进制文件分割；而合并文件可以使用cat命令。\n1.1 文本文件分割 分割文本文件时，可以按文件大小分割，也可以按文本行数分割。\n按文件大小分割\n按文件大小分割文件时，需要以-C参数指定分割后的文件大小：\n$ split -C 100M large_file.txt stxt 如上所示，我们将大文件large_file.txt按100M大小进行分割，并指定了分割后文件前缀stxt；当不指定前缀时，split会自动对分割文件进行命名，一般会以x开头。\n按行分割\n文本文件还可以以行为单位进行分割，以行数进行分割时会忽略文件大小，并以-l参数指定分割后文件的行数：\n$ split -l 1000 large_file.txt stxt 1.2 二进制文件分割 二进制文件分割类似于按大小分割文本文件，不同的是以-b参数来指定分割后的文件大小：\n$ split -b 100M data.bak sdata 2. 文件合并 文件合并使用cat命令，上面几种方式分割的文件都可以使用cat命令合并。\ncat命令合并分割文件：\n$ cat stxt* \u0026gt; new_file.txt 3. 命令格式 3.1 split命令说明 split命令格式如下：\nsplit [选项]... [要切割的文件 [输出文件前缀]] 命令参数\n-a, --suffix-length=N 使用长度为 N 的后缀 (默认 2) -b, --bytes=SIZE 设置输出文件的大小。支持单位：m,k -C, --line-bytes=SIZE 设置输出文件的最大行数。与 -b 类似，但会尽量维持每行的完整性 -d, --numeric-suffixes 使用数字后缀代替字母 -l, --lines=NUMBER 设备输出文件的行数 --help 显示版本信息 --version 输出版本信息 3.2 cat命令说明 cat命令的常见使用场景有：\n显示文件内容：\n$ cat filename 创建一个空文件：\n$ cat \u0026gt; filename 文件合并：\n$ cat file1 file2 \u0026gt; file 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211201955418/\n相关话题：https://www.cnsre.cn/tags/split/\n","description":"Linux下使用 split 进行大文件的分割与合并","id":18,"section":"posts","tags":["split"],"title":"Linux 大文件分割合并","uri":"https://www.cnsre.cn/posts/211201955418/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211129946481/\n相关话题：https://www.cnsre.cn/kubernetes/\nTaint 和 Toleration（污点和容忍） 在 k8s 集群中，节点亲和性 NodeAffinity 是一种 Pod 上定义的属性，能够让 Pod 可以按找我们的需求调度到指定节点上，而 Taints (污点) 则于NodeAffinity (节点亲和性)是相反的，它是一种 Node 上定义的属性，可以让 Pod 不被调度到带污点的节点上，甚至会对带污点节点上已有的 Pod 进行驱逐。对应的 k8s 可以给 Pod 设置 Tolerations(容忍) 让 Pod 能够对有污点的节点设置容忍，将 Pod 调度到该节点。 Taints 一般是配合 Tolerations 使用的。\n为 node 设置污点和容忍 NoSchedule: 一定不能被调度 PreferNoSchedule: 尽量不要调度 NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod 为 node1 设置 taint：\n1 2 3 kubectl taint nodes k8s-node1 key1=value1:NoSchedule kubectl taint nodes k8s-node1 key1=value1:NoExecute kubectl taint nodes k8s-node1 key2=value2:NoSchedule 查看 node1 上的 taint：\n1 kubectl describe nodes k8s-node1 |grep Taint 删除上面的 taint：\n1 2 3 4 kubectl taint nodes k8s-node1 key1:NoSchedule- kubectl taint nodes k8s-node1 key1:NoExecute- kubectl taint nodes k8s-node1 key2:NoSchedule- kubectl taint nodes k8s-node1 key1- # 删除指定key所有的effect 为 pod 设置 toleration 只要在 pod 的 spec 中设置 tolerations 字段即可，可以有多个 key，如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 tolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; - key: \u0026#34;node.alpha.kubernetes.io/unreachable\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 4000 tolerations 和 containers 同级。 key 能容忍的污点 key。 operator Equal 等于表示 key=value ， Exists 不等于，表示当值不等于下面 value 正常 value 可以设置为 NoSchedule、PreferNoSchedule 或 NoExecute。 effect effect 策略 tolerationSeconds 是当 pod 需要被驱逐时，可以继续在 node 上运行的时间。 具体的使用方法请参考官方文档。\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211129946481/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"Kubernetes 污点 Taint 和容忍 Toleration。","id":19,"section":"posts","tags":["kubernetes","taint","toleration"],"title":"Kubernetes 的 Taint 和 Toleration（污点和容忍）","uri":"https://www.cnsre.cn/posts/211129946481/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211126003074/\n相关话题：https://www.cnsre.cn/tags/k3s/\n安装了 k3s 以后，发现 traefik 并没有 ingress 用的习惯，于是我就打算吧 traefik 卸载了安装上ingress。\n删除traefik 删除traefik舵图资源： 1 2 kubectl -n kube-system delete helmcharts.helm.cattle.io traefik kubectl -n kube-system delete helmcharts.helm.cattle.io traefik-crd 修改 k3s.service 配置文件 停止k3s服务： systemctl stop k3s 编辑服务文件vim /etc/systemd/system/k3s.service并将此行添加到ExecStart ：\n--disable traefik \\ 重新加载服务文件： systemctl daemon-reload\n从自动部署文件夹中删除清单文件： rm /var/lib/rancher/k3s/server/manifests/traefik.yaml\n启动k3s服务： systemctl start k3s\n安装ingress 安装 ingress 的话 参考我之前的文章 kubernetes 安装 ingress controller\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211126003074/\n相关话题：https://www.cnsre.cn/tags/k3s/\n","description":"k3s 安装以后想卸载 traefik","id":20,"section":"posts","tags":["k3s","traefik"],"title":"k3s 卸载 traefik","uri":"https://www.cnsre.cn/posts/211126003074/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211124953028/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n故障表现 server端执行zabbix_get提示报错:没有权限\n故障分析 zabbix自定义key监控日志\n脚本如下\n#!/bin/bash Path=/home/xxx/logs/AspectLog/aspect.log api1=xxxxxx/login if [ $# -ne 1 ];then echo \u0026#34;Follow the script name with an argument\u0026#34; fi case $1 in AccountLogin) cat $Path |grep $api1 |awk -F \u0026#39;,\u0026#39; \u0026#39;{print $23}\u0026#39;|awk -F\u0026#39;:\u0026#39; \u0026#39;END {print $2}\u0026#39; ;; *) echo -e \u0026#34;\\e[033mUsage: sh -bash [请输入API名称|如:AccountLogin]\\e[0m\u0026#34; esac agent的配置文件\n[root@ip-10-0-11-39 zabbix]# cat zabbix_agentd.conf |grep usedtime.sh UserParameter=used.time[*],/etc/zabbix/usedtime.sh $1 server端执行zabbix_get提示报错:没有权限\n[root@zabbix ~]# zabbix_get -s 10.0.10.243 -p 10050 -k usedtime.sh { [cat: /home/ec2-user/homeconnect/logs/AspectLog/aspect.log : Permission denied ] } 故障分析 分析结果：agent端的selinux和firewalld已经关闭\n通过zabbix_get 发现是因为 zabbix_agent 没有日志的权限。\n解决方法 需要更改agent的配置文件\n修改为 AllowRoot=1\n重启agent 即可\nsystemctl restart zabbix-agent 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211124953028/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix-agent执行脚本提示：Permission denied","id":21,"section":"posts","tags":["zabbix","故障集"],"title":"zabbix-agent执行脚本提示：Permission denied","uri":"https://www.cnsre.cn/posts/211124953028/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211122846494/\n相关话题：https://www.cnsre.cn/tags/shell/\n1、判断curl返回状态码 1 2 3 4 5 6 7 #!/bin/bash response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com) if [[ $response -ge 200 \u0026amp;\u0026amp; $response -le 299 ]] ;then echo \u0026#39;check point success\u0026#39; else echo \u0026#39;check point fail\u0026#39; fi 2、读取文件中的配置到变量中 1 2 3 4 5 6 #!/bin/bash # 配置文件中的配置项格式为key1=value1，一行一个配置项 while read line;do eval \u0026#34;$line\u0026#34; done \u0026lt; /etc/openvpn/server/smtp.conf echo $key1 3、根据console输入条件执行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/bin/bash echo \u0026#34;选择以下功能:\u0026#34; echo \u0026#34; 0) 功能0\u0026#34; echo \u0026#34; 1) 功能1\u0026#34; echo \u0026#34; 2) 功能2\u0026#34; echo \u0026#34; 3) 功能3\u0026#34; echo \u0026#34; 4) 功能4\u0026#34; read -p \u0026#34;功能选项[4]: \u0026#34; option until [[ -z \u0026#34;$option\u0026#34; || \u0026#34;$option\u0026#34; =~ ^[0-4]$ ]]; do read -p \u0026#34;$option为无效的选项，请重新输入功能选项: \u0026#34; option done case \u0026#34;$option\u0026#34; in 0) echo \u0026#34;功能0已执行!\u0026#34; ;; 1) echo \u0026#34;功能1已执行!\u0026#34; exit ;; 2) echo \u0026#34;功能2已执行!\u0026#34; exit ;; 3) echo \u0026#34;功能3已执行!\u0026#34; exit ;; # 默认选项 4|\u0026#34;\u0026#34;) echo \u0026#34;功能4已执行!\u0026#34; exit ;; esac 4、将指定输出内容写入文件 1 2 3 4 { echo \u0026#34;hahh\u0026#34; echo \u0026#34;lalal\u0026#34; } \u0026gt; /tmp/test 5、判断变量是否存在或为空 1 2 3 4 5 if [ -z ${var+x} ]; then echo \u0026#34;var is unset\u0026#34;; else echo \u0026#34;var is set to \u0026#39;$var\u0026#39;\u0026#34;; fi 6、换算秒为分钟、小时 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash a=60100 swap_seconds () { SEC=$1 (( SEC \u0026lt; 60 )) \u0026amp;\u0026amp; echo -e \u0026#34;持续时间: $SEC秒\\c\u0026#34; (( SEC \u0026gt;= 60 \u0026amp;\u0026amp; SEC \u0026lt; 3600 )) \u0026amp;\u0026amp; echo -e \u0026#34;持续时间: $(( SEC / 60 ))分钟$(( SEC % 60 ))秒\\c\u0026#34; (( SEC \u0026gt; 3600 )) \u0026amp;\u0026amp; echo -e \u0026#34;持续时间: $(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\\c\u0026#34; } b=`swap_seconds $a` echo $b 输出\n1 持续时间: 16小时41分钟40秒 7、脚本命令行参数的传递与判断 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash main() { if [[ $# == 1 ]]; then case $1 in \u0026#34;-h\u0026#34;) echo \u0026#34;脚本使用方法: \u0026#34; echo \u0026#34; ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)\u0026#34; exit ;; \u0026#34;--help\u0026#34;) echo \u0026#34;脚本使用方法: \u0026#34; echo \u0026#34; ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)\u0026#34; exit ;; *) echo \u0026#34;参数错误！\u0026#34; exit ;; esac fi } main $* 8、检测docker 容器的启动状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 第一步：判断镜像是否存在 if [ `docker images --format {{.Repository}}:{{.Tag}} |grep -Fx 192.168.1.7:32772/applications/$CI_PROJECT_NAME:${CI_COMMIT_SHORT_SHA};echo $?` -eq 0 ];then docker pull 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; fi; # 第二步：判断是否已经有重名的容器在运行或者处在其他状态。重名的，先删掉，在启动；不重名的直接启动 if [ `docker ps -a --format {{.Names}} |grep -Fx $CI_PROJECT_NAME \u0026gt; /dev/null ;echo $?` -eq 0 ] ;then docker rm -f $CI_PROJECT_NAME ; docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; else docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; fi; # 第三步：循环五次判断容器的监控检测是否处于什么状态。健康状态就直接退出循环，不健康显示健康检查日志，正在启动的直接显示。处于其他状态的直接显示状态 n=0; while true ;do container_state=`docker inspect --format=\u0026#39;{{json .State.Health.Status}}\u0026#39; $CI_PROJECT_NAME`; case $container_state in \u0026#39;\u0026#34;starting\u0026#34;\u0026#39; ) echo \u0026#34;应用容器正在启动！\u0026#34;; ;; \u0026#39;\u0026#34;healthy\u0026#34;\u0026#39; ) echo \u0026#34;应用容器已启动，状态健康！\u0026#34;; break; ;; \u0026#39;\u0026#34;unhealthy\u0026#34;\u0026#39; ) echo \u0026#34;应用容器健康检测失败！\u0026#34;; docker inspect --format=\u0026#39;{{json .State.Health.Log}}\u0026#39; $CI_PROJECT_NAME; ;; * ) echo \u0026#34;未知的状态:$container_state\u0026#34;; ;; esac; sleep 1s; n=$(($n+1)); if [ $n -eq 5 ]; then break ; fi ; done 9、检查常见系统命令是否安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 check_command() { if ! command -v ifconfig \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31mifconfig命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y net-tools \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y net-tools \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y net-tools \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi elif ! command -v ip \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31mip命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y iproute2 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y iproute2 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y iproute2 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi elif ! command -v curl \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31mcurl命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y curl \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y curl \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y curl \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi elif ! command -v wget \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31mawk命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y wget \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y wget \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y wget \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi elif ! command -v tail \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31mcoreutils命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y coreutils \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y coreutils \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y coreutils \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi elif ! command -v sed \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31msed命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y sed \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y sed \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y sed \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi elif ! command -v grep \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo -e \u0026#34;\\033[31mgrep命令不存在，正在下载安装！\\033[0m\u0026#34; if os=\u0026#34;ubuntu\u0026#34;; then apt install -y grep \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;centos\u0026#34;; then yum install -y grep \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 elif os=\u0026#34;fedora\u0026#34;; then dnf install -y grep \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 fi fi } 10、检查系统网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 check_network() { check_command ping -c 4 114.114.114.114 \u0026gt;/dev/null if [ ! $? -eq 0 ]; then echo -e \u0026#34;\\033[31mIP地址无法ping通，请检查网络连接！！！\\033[0m\u0026#34; exit fi ping -c 4 www.baidu.com \u0026gt;/dev/null if [ ! $? -eq 0 ]; then echo -e \u0026#34;\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m\u0026#34; exit fi curl -s --retry 2 --connect-timeout 2 www.baidu.com \u0026gt;/dev/null if [ ! $? -eq 0 ]; then echo -e \u0026#34;\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m\u0026#34; exit fi } 原文出处：https://gitbook.curiouser.top/origin/bash-scirpts.html#\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211122846494/\n相关话题：https://www.cnsre.cn/tags/shell/\n","description":"10个常用shell脚本，判断curl返回状态码，读取文件中的配置到变量中，根据console输入条件执行，将指定输出内容写入文件，判断变量是否存在或为空，脚本命令行参数的传递与判断 ...","id":22,"section":"posts","tags":["shell"],"title":"10个 shell 常用脚本","uri":"https://www.cnsre.cn/posts/211122846494/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211117937177/\n相关话题：https://www.cnsre.cn/tags/kvm/\nWebVirtMgr是近两年来发展较快，比较活跃，非常清新的一个KVM管理平台，提供对宿主机和虚机的统一管理，它有别于kvm自带的图形管理工具（virtual machine manager），让kvm管理变得更为可视化，对中小型kvm应用场景带来了更多方便。\nWebVirtMgr介绍 WebVirtMgr采用几乎纯Python开发，其前端是基于Python的Django，后端是基于Libvirt的Python接口，将日常kvm的管理操作变的更加的可视化。\nWebVirtMgr 特点 操作简单，易于使用\t、通过libvirt的API接口对kvm进行管理、提供对虚拟机生命周期管理\nWebVirtMgr 功能 宿主机管理支持以下功能、CPU利用率、内存利用率、网络资源池管理、存储资源池管理、虚拟机镜像、虚拟机克隆、快照管理、日志管理、虚机迁移、虚拟机管理支持以下功能、CPU利用率、内存利用率、光盘管理、关/开/暂停虚拟机、安装虚拟机、VNC console连接、创建快照\n官方文档 https://github.com/retspen/webvirtmgr/wiki/Install-WebVirtMgr\n安装前的部署 安装一些依赖包\n1 yum -y install git python-pip libvirt-python libxml2-python python-websockify supervisor nginx gcc python-devel wget vim net-tools lrzsz 安装pip 1 2 3 wget https://bootstrap.pypa.io/get-pip.py python get-pip.py pip -V　1 pip install numpy 安装python的需要包和配置Django环境 1 git clone git://github.com/retspen/webvirtmgr.git 安装nginx 1 2 rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm yum install nginx -y 安装supervisor 安装参考\nhttps://www.2cto.com/kf/201712/702837.html\n开机自启参考\nhttps://blog.csdn.net/binggoogle/article/details/53203991\n1 cat /etc/supervisord.conf ⚠️ 注意\n如果没有这个文件按照一下步骤安装\n有的话忽略此步骤\n1 2 3 pip install supervisor mkdir /etc/supervisord.d/ echo_supervisord_conf \u0026gt; /etc/supervisord.conf 新建文件夹\n1 vim /etc/supervisord.d/app.conf 配置文件 app.conf\n内容为\n1 2 3 4 5 6 7 8 [program:appname] command=/root/soft/push.api directory=/root/soft/push.api autostart=true autorestart=true user=root stdout_logfile = /var/log/supervisor/pushapi.log stderr_logfile = /var/log/supervisor/pushapi-error.log 修改 在配置文件最下方修改为\n1 2 3 vim /etc/supervisord.conf [include] files = /etc/supervisord.d/*.ini 1 2 3 supervisord -c /etc/supervisord.conf /usr/bin/supervisorctl start all /usr/bin/supervisorctl stop all 安装环境 1 2 cd webvirtmgr pip install -r requirements.txt 1 ./manage.py syncdb 创建用户 输入以下用户信息\n1 2 3 4 5 6 7 8 9 10 11 You just installed Django\u0026#39;s auth system, which means you don\u0026#39;t have any superusers defined. Would you like to create one now? (yes/no): yes Username (leave blank to use \u0026#39;root\u0026#39;): admin Email address: 275301281@qq.com Password: admin Password (again):admin Superuser created successfully. Installing custom SQL ... Installing indexes ... Installed 6 object(s) from 1 fixture(s) ./manage.py collectstatic 配置一个超级用户 1 ./manage.py createsuperuser 1 2 3 4 5 6 WARNING:root:No local_settings file found. Username (leave blank to use \u0026#39;root\u0026#39;): yes Email address: 275301281@qq.com Password: Lenovo@123 Password (again): Lenovo@123 Superuser created successfully. 设置nginx a、使用:8000端口\n移动这个 webvirtmgr 目录到 /var/www 下 1 2 cd .. mv webvirtmgr /var/www/ ⚠️ 注意：\nwebvirtmgr 目录下还有一个名称为webvirtmgr 的文件夹\n不要单独移动 webvirtmgr/webvirtmgr 文件 编辑配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 vim /etc/nginx/conf.d/webvirtmgr.conf server { listen 80 default_server; server_name $hostname; #access_log /var/log/nginx/webvirtmgr_access_log; location /static/ { root /var/www/webvirtmgr/webvirtmgr; # or /srv instead of /var expires max; } location / { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-for $proxy_add_x_forwarded_for; proxy_set_header Host $host:$server_port; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 600; proxy_read_timeout 600; proxy_send_timeout 600; client_max_body_size 1024M; # Set higher depending on your needs } } 启动nginx并设置开机自启动 (如果不设置开机自启动，重启服务器supervisor无法管理Django进程)，并开机自启动supervisord\n1 /etc/init.d/nginx start 或者\n1 2 systemctl restart nginx systemctl enable supervisord 分配权限\n1 chown nginx.nginx /var/www/webvirtmgr 设置supervisor 在/etc/supervisord.conf末尾加入下面的配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 vi /etc/supervisord.conf [program:webvirtmgr] command=/usr/bin/python /var/www/webvirtmgr/manage.py run_gunicorn -c /var/www/webvirtmgr/conf/gunicorn.conf.py directory=/var/www/webvirtmgr autostart=true autorestart=true logfile=/var/log/supervisor/webvirtmgr.log log_stderr=true user=nginx [program:webvirtmgr-console] command=/usr/bin/python /var/www/webvirtmgr/console/webvirtmgr-console directory=/var/www/webvirtmgr autostart=true autorestart=true #stdout_logfile=/var/log/supervisor/webvirtmgr-console.log redirect_stderr=true user=nginx\t⚠️ 注意\n进程无法启动或者报错 可以选择吧 log 注释取消\n重启supervisord 开机自启参考\nhttps://blog.csdn.net/binggoogle/article/details/53203991\n设置完之后重启即可\n1 2 3 systemctl restart supervisord.service systemctl enable supervisord.service systemctl status supervisord.service 更新 1 cd /var/www/webvirtmgr git pull 1 ./manage.py collectstatic 1 systemctl restart supervisord 如果有错误或不运行 1 2 3 4 ./manage.py runserver 0:8000 #或者后台运行脚本 nohup python /var/www/webvirtmgr/manage.py runserver 0:8000 \u0026gt;/dev/null \u0026amp; nohup python /var/www/console/webvirtmgr-console \u0026gt;/dev/null \u0026amp; 访问：http://x.x.x.x:8000(x.x.x.x - your server IP address )，输入创建的用户和密码，如果没有创建，请用python manager.py createsuperuser,命令创建。登录后如下图所示\n配置虚拟机所在宿主机\nwebvirtmgr客户端就这样搭建完了，接下来需要配置虚拟机所在宿主机的，参考git地址.\n配置宿主机 下载并执行脚本 如果虚拟机比较多，该脚本执行时间会比较长，因为会执行 service libvirt-guests restart，会将所有运行的虚拟机挂起然后再恢复，感觉这一步不是必须的，因为我有一台只设置ssh认证，也可以正常连接。\n1 curl http://retspen.github.io/libvirt-bootstrap.sh | sudo sh 如果没有curl就用wget\n1 wget -O - http://retspen.github.io/libvirt-bootstrap.sh | sudo sh 配置防火墙 1 iptables -I INPUT -m state --state NEW -m tcp -p tcp --dport 16509 -j ACCEPT 设置TCP授权 参考：https://github.com/retspen/webvirtmgr/wiki/Setup-TCP-authorization\nwebvirtmgr新建服务器连接时需要此账号\n用saslpasswd2命令给libvirt的用户cnsre设置密码 1 2 3 saslpasswd2 -a libvirt cnsre Password: cnsre Again (for verification): cnsre 生成一个密码库 1 2 3 sasldblistusers2 -f /etc/libvirt/passwd.db cnsre@webvirtmgr.cn: userPassword 设置ssh授权 1 ssh-keygen -t rsa # 产生公私钥 直接回车，回车，回车\n1 ssh-copy-id 192.168.1.120 ⚠️ 注意\n由于这里webvirtmgr和kvm服务部署在同一台机器，所以这里本地信任。\n如果kvm部署在其他机器，那么这个是其他它的ip 同时也要设置ssh key密钥\n提示输入密码的时候直接输入之前1.120的密码\n1 ssh 192.168.1.120 -L localhost:8000:localhost:8000 -L localhost:6080:localhost:6080 web 平台加入其他kvm宿主机 在部署web管理的主机上执行命令\n1 ssh-keygen -t rsa 然后在执行\n1 ssh-copy-id 192.168.1.165 添加新的kvm宿主机\n查看新加的kvm宿主机状态 看有无报错\n删除新加的账号 1 sudo saslpasswd2 -a libvirt -d cnsre 确认验证新加的账号配置 1 2 3 4 5 6 7 8 9 10 11 12 virsh -c qemu+tcp://IP_address/system nodeinfo (virsh -c qemu+tcp://192.168.1.50/system nodeinfo) Please enter your authentication name: cnsre Please enter your password: xxxxxx CPU model: x86_64 CPU(s): 2 CPU frequency: 2611 MHz CPU socket(s): 1 Core(s) per socket: 2 Thread(s) per core: 1 NUMA cell(s): 1 Memory size: 2019260 kB ⚠️ 注意\n账号全名带hostname，如 cnsre@webvirtmgr.cn\n测试的时候这一步测试没有成功 但是可以链接\n设置ssh认证 ssh和tcp设置一种即可，其实就是设置无密码登录，要注意的是从webvirtmgr的什么用户到宿主机的什么用户的无密码登录，比如我用root跑的django webvirtmgr，而宿主机也是root跑的virsh，所以需要设置root到root的无密码登录。而git官网推荐的是用nginx用户跑django webvirtmgr，webvirtmgr用户跑的virsh，所以设置的是nginx用户到宿主机webvirtmgr用户的无密码登录。 参考：https://github.com/retspen/webvirtmgr/wiki/Setup-SSH-Authorizatio\n使用tcp认证连接服务器 访问：http://192.168.1.120:8000，xxxx是webvirtmgr的ip地址，点击new connection\n填写kvm宿主机的一些信息 基础架构可以看到一些vm虚拟机\nKVM WEB管理常见报错 网页控制台 远程链接报错1006\n安装vnc即可\n1 yum install -y novnc 网页控制台 远程链接报错505\n1 2 cd /var/www/console/ ./webvirtmgr-console \u0026amp; 后台运行脚本\n1 2 nohup python /var/www/webvirtmgr/manage.py runserver 0:8000 \u0026gt;/dev/null \u0026amp; nohup python /var/www/console/webvirtmgr-console \u0026gt;/dev/null \u0026amp; 其他web管理 webvirtcloud webvirtcloud 的作者还是 webvirtmgr的作者，但是 webvirtmgr 已经有好久没有维护了。如果你想体验 webvirtcloud 你可以安装webvirtcloud。\nwebvirtcloud 特征 QEMU/KVM 管理程序管理 QEMU/KVM 实例管理 - 创建、删除、更新 虚拟机管理程序和实例基于 Web 的统计信息 管理多个 QEMU/KVM 管理程序 管理管理程序数据存储池 管理管理程序网络 使用浏览器访问实例控制台 基于 Libvirt API 的 Web 管理 UI 基于用户的授权和认证 用户可以在实例中将 SSH 公钥添加到 root（仅测试 Ubuntu） 用户可以在实例中更改 root 密码（仅测试 Ubuntu） 支持 cloud-init 数据源接口\nwebvirtcloud具体文档请查看 webvirtcloud 项目 webvirtcloud 的作者也贴心的提供了一键脚本\n1 2 3 wget https://raw.githubusercontent.com/retspen/webvirtcloud/master/install.sh chmod 744 install.sh #以 sudo 或 root 用户运行 ./install.sh 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211117937177/\n相关话题：https://www.cnsre.cn/tags/kvm/\n","description":"WebVirtMgr是近两年来发展较快，比较活跃，非常清新的一个KVM管理平台，提供对宿主机和虚机的统一管理，它有别于kvm自带的图形管理工具（virtual machine manager），让kvm管理变得更为可视化，对中小型 kvm 应用场景带来了更多方便。","id":23,"section":"posts","tags":["kvm"],"title":"快速搭建 kvm web 管理工具 WebVirtMgr","uri":"https://www.cnsre.cn/posts/211117937177/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211115903260/\n相关话题：https://www.cnsre.cn/tags/kvm/\nvirsh命令常用参数 参数 参数说明 基础操作 list 查看虚拟机列表，列出域 start 启动虚拟机，开始一个（以前定义的）非活跃的域 shutdown 关闭虚拟机，关闭一个域 destroy(危险) 强制关闭虚拟机，销毁（停止）域 vncdisplay 查询虚拟机vnc端口号 配置管理操作 dumpxml 导出主机配置信息 undefine 删除主机 define 导入主机配置 domrename 对虚拟机进行重命名 挂起与恢复 suspend 挂起虚拟机 resume 恢复虚拟机 自启动管理 autostart 虚拟机开机启动 autostart \u0026ndash;disable 取消虚拟机开机启动 以上参数通过 “virsh \u0026ndash;help” 获得。 删除虚拟机 1 2 3 4 5 6 7 8 9 10 11 12 13 virsh destroy njvm01 #强制关闭电源 virsh undefine njvm01 #删除虚拟机 [root@nkgtsv01 data]# virsh shutdown njvm01 域 njvm01 被关闭 [root@nkgtsv01 data]# virsh start njvm01 域 njvm02 已开始 [root@nkgtsv01 data]# virsh list --all 查看虚拟机状态 设置虚拟机自启动 1 2 3 virsh autostart njvm01 virsh autostart --disable njvm02 启动，关闭和重启一个虚拟机 1 2 3 4 5 virsh start njvm01 virsh shutdown njvm01 virsh reboot njvm01 宿主机链接到kvm虚拟机 1 virsh console njvm01 克隆虚拟机 1 virt-clone -o njvm01-n njvm02-f /data/kvm-img/njvm02.img 修改njvm05 配置 1 virsh edit njvm01 挂起及恢复虚拟机 1 2 3 4 5 6 7 挂起： virsh suspend njvm01 恢复： virsh resume njvm01 创建KVM linux 1 virt-install --name njvm01 --boot network,cdrom,menu=on --ram 8000 --vcpus=2 --os-variant=rhel6 --accelerate --cdrom=/home/iso/CentOS-7-x86-64-DVD-1708.iso --disk path=/data/kvm-i/njvm01.img,size=200,bus=virtio --bridge=br0,model=virtio --autostart --vnc --vncport=5930 --vnclisten=0.0.0.0 创建KVM Windows 1 2 3 4 virt-install --name njvmwin --boot network,cdrom,menu=on --ram 6411 --os-type=windows --vcpus=1 --os-variant=rhel6 --accelerate --cdrom=/data/BBackup/ --disk path=/data/kvm-images/njvmwin.img,size=200,bus=virtio --bridge=br0, --autostart --vnc --vncport=5910 --vnclisten=0.0.0.0 virt-install -n njvmwin --vcpus=1 --ram=6411--os-type=windows --os-variant=win2k8 -c /vm/iso/cn_windows_server_2012_r2_sp1_x64.iso --disk path=/usr/share/virtio-win/virtio-win-0.1.126_amd64.vfd,device=floppy --disk path=/vm/win2012.img,format=qcow2,bus=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole vm添加vnc端口 https://www.cnblogs.com/chenjiahe/p/5919742.html\n1 2 3 4 5 \u0026lt;graphics type=\u0026#39;vnc\u0026#39; port=\u0026#39;5900\u0026#39; autoport=\u0026#39;no\u0026#39; listen=\u0026#39;0.0.0.0\u0026#39;\u0026gt; \u0026lt;listen type=\u0026#39;address\u0026#39; address=\u0026#39;0.0.0.0\u0026#39;/\u0026gt; \u0026lt;/graphics\u0026gt; 注意\n要用 virsh edit vi命令不会生效\n--name njvm01 \\ #虚拟机名 --ram=1024 \\ #分配内存大小，MB --vcpus=1 \\ #配置虚拟机的vcpu 数目 --check-cpu \\ #检查确定vcpu是否超过物理 CPU数目，如果超过则发出警告。 --os-type=linux \\ #要安装的操作系统类型，例如：\u0026#39;linux\u0026#39;、\u0026#39;unix\u0026#39;、\u0026#39;windows\u0026#39; --os-variant=rhel6 \\ #操作系统版本，如：\u0026#39;Fedora6\u0026#39;, \u0026#39;rhel5\u0026#39;, \u0026#39;solaris10\u0026#39;, \u0026#39;win2k\u0026#39; --disk path=/virhost/node7.img,device=disk,bus=virtio,size=20,sparse=true \\ #虚拟 机所用磁盘或镜像文件，size大小G --bridge=br0 \\ #指定网络，采用透明网桥 --noautoconsole \\ #不自动开启控制台 --pxe #网络安装 virsh start njvm01 #开机 virsh destroy njvm01 #强制关闭电源 virsh shutdown njvm01 #关机 virsh list --all #查看虚拟机状态 virsh reboot njvm01 #重启 virt-viewer name #查看安装状态 xml文件详解 使用virt-install 工具安装虚拟机后，在目录 /etc/libvirt/qemu/ 下生成 xml 配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 \u0026lt;domain type=\u0026#39;kvm\u0026#39;\u0026gt; # 虚拟机类型 \u0026lt;name\u0026gt;centos\u0026lt;/name\u0026gt; 虚拟机名称 \u0026lt;uuid\u0026gt;78dx24ef-1d2d-810x-9213-2c02df529cx\u0026lt;/uuid\u0026gt; uuid唯一标示 \u0026lt;memory unit=\u0026#39;KiB\u0026#39;\u0026gt;2048576\u0026lt;/memory\u0026gt; 指定虚拟机内存大小，给出了单位 \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;2\u0026lt;/vcpu\u0026gt; 虚拟机占用虚拟cpu个数，这里指物理cpu的核心数量 \u0026lt;os\u0026gt; \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;rhel6.3.0\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; 指定虚拟系统架构 \u0026lt;boot dev=\u0026#39;hd\u0026#39;/\u0026gt; 启动类型，从硬盘启动 \u0026lt;/os\u0026gt; \u0026lt;devices\u0026gt; \u0026lt;emulator\u0026gt;/usr/libexec/qemu-kvm\u0026lt;/emulator\u0026gt; 驱动程序，同上，使用的是qemu-kvm \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; 指定磁盘类型 \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;raw\u0026#39; cache=\u0026#39;none\u0026#39;/\u0026gt; 指定磁盘格式，这里是raw，也支持qcow2. \u0026lt;source file=\u0026#39;/home/data/img/centos.img\u0026#39;/\u0026gt; img文件路径 \u0026lt;target dev=\u0026#39;hda\u0026#39; bus=\u0026#39;ide\u0026#39;/\u0026gt; 磁盘文件标示，驱动类型 \u0026lt;address type=\u0026#39;drive\u0026#39; controller=\u0026#39;0\u0026#39; bus=\u0026#39;0\u0026#39; target=\u0026#39;0\u0026#39; unit=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; \u0026lt;mac address=\u0026#39;34:72:00:15:65:e6\u0026#39;/\u0026gt; 自动生成，可以手动指定。 \u0026lt;source bridge=\u0026#39;br0\u0026#39;/\u0026gt; 桥接到哪一个接口 \u0026lt;/interface\u0026gt; \u0026lt;/devices\u0026gt; \u0026lt;/domain\u0026gt; 列出虚拟机的所有网口 virsh domiflist njvm01 结果如下： Interface Type Source Model MAC ------------------------------------------------------- vnet0 bridge br0 virtio 34:72:00:15:65:e6 vnet1 bridge br1 virtio 52:54:10:f5:c5:6c 新增一个网口 1 2 3 4 5 6 7 virsh attach-interface domain --type bridge --source br1 --model virtio --config # 下次启动生效 virsh attach-interface domain --type bridge --source br1 --model virtio --current # 立即生效 virsh detach-interface domain --type bridge --mac 34:72:00:15:65:e6 --config # 下次启动生效 virsh detach-interface domain --type bridge --mac 34:72:00:15:65:e6 --current # 立即生效 删除网卡命令 1 virsh detach-interface njvm01 --type network --mac 34:72:00:15:65:e6 永久添加网卡 1 virsh attach-interface domain --type network --source default --model virtio --config 临时添加网卡 virsh attach-interface njvm01 --type network --source default virsh attach-interface njvm01 --type network --source default --config 关闭或打开某个网口 1 2 3 virsh domif-setlink domain vnet0 down virsh domif-setlink domain vnet0 up 获取某个网口状态 1 virsh domif-getlink win2k8 vnet1 列出所有的块设备 1 virsh domblklist win2k8 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211115903260/\n相关话题：https://www.cnsre.cn/tags/kvm/\n","description":"kvm简单使用,kvm命令详解,kvm配置文件参数详解,kvm配置文件","id":24,"section":"posts","tags":["kvm"],"title":"kvm简单使用","uri":"https://www.cnsre.cn/posts/211115903260/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211112839379/\n相关话题：https://www.cnsre.cn/tags/shell/\n前言 最近有几台新上的设备，需要安装jdk和tomcat。为了避免以后这种重复的劳动力，我决定写一个脚本，直接执行脚本就可以自动选择你安装的jdk以及tomcat的脚本，来避免这种问题。\n脚本内容 此脚本只针对于centos7完成测试，其他版本未测试。使用时请注意。\n脚本内容如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 #!/bin/bash PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin export PATH LANG=en_US.UTF-8 RD=\u0026#34;\\033[31m\u0026#34; # 错误消息 GR=\u0026#34;\\033[32m\u0026#34; # 成功消息 YL=\u0026#34;\\033[33m\u0026#34; # 告警消息 BL=\u0026#34;\\033[36m\u0026#34; # 日志消息 PL=\u0026#39;\\033[0m\u0026#39; clear echo -e \u0026#34;${YL}##################################################${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL}${GR}脚本名称${PL}: 一键安装java、tomcat脚本 ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL}${GR}作 者${PL}: sre运维博客 ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL}${GR}网 址${PL}: www.cnsre.cn ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL}${GR}文章地址${PL}: https://cnsre.cn/posts/211112839379/ ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}##################################################${PL}\u0026#34; sleep 0.5 echo -e \u0026#34;${RD}本脚本将会自动安装1.8.0_221版本jdk以及8.5.23版本tomcat，请确认是否安装？${PL}\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then exit 1 else echo -e \u0026#34;${GR}正在下载java tomcat安装包,请稍后${PL}\u0026#34; fi wget -q -c https://pan.cnsre.cn/d/Package/Linux/jdk/tomcat-8.5.23_jdk1.8.0_221.tar.gz tar -zxf tomcat-8.5.23_jdk1.8.0_221.tar.gz sleep 0.5 echo -e \u0026#34;${RD}是否确定安装 1.8.0_221 版本 jdk? ${PL}\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then exit 1 else echo -e \u0026#34;${GR}你已经选择安装1.8.0_221版本 jdk${PL}\u0026#34; fi read -r -p \u0026#34;请选择安装jdk的绝对路径，请不要输入最后的\u0026#34;/\u0026#34;:\u0026#34; input if [ ! -d $input ]; then echo -e \u0026#34;${RD}你输入的路径不存在，请重新输入或者创建后再次执行脚本${PL}\u0026#34; exit 1 fi echo -e \u0026#34;${GR}正在安装 1.8.0_221 版本 jdk${PL}\u0026#34; mv jdk1.8.0_221 $input/jdk cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/profile ############## JAVA ############## export JAVA_HOME=$input/jdk export JAVA_BIN=$JAVA_HOME/bin export JAVA_LIB=$JAVA_HOME/lib export CLASSPATH=.:$JAVA_LIB/tools.jar:$JAVA_LIB/dt.jar export PATH=$JAVA_BIN:$PATH EOF echo -e \u0026#34;${GR}验JAVA home及版本${PL}\u0026#34; sleep 2 chmod +x $input/jdk/bin/java chmod +x $input/jdk/jre/bin/java source /etc/profile echo -e\u0026#34;${GR}你的java安装路径为：$JAVA_HOME ${PL}\u0026#34; java -version echo -e \u0026#34;${RD}是否确定安装 8.5.23 版本 tomcat ? ${PL}\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then exit 1 else echo -e \u0026#34;${GR}你已经选择安装 8.5.23 版本 tomcat ${PL}\u0026#34; fi read -r -p \u0026#34;请选择安装tomcat的绝对路径，请不要输入最后的\u0026#34;/\u0026#34;:\u0026#34; input if [ ! -d $input ]; then echo -e \u0026#34;${RD}你输入的路径不存在，请重新输入或者创建后再次执行脚本${PL}\u0026#34; exit 1 fi echo -e \u0026#34;${GR}正在安装 8.5.23 版本 tomcat${PL}\u0026#34; mv apache-tomcat-8.5.23 $input/tomcat chmod +x $input/tomcat/bin/*.sh echo -e \u0026#34;${GR}正在将tomcat加入系统服务${PL}\u0026#34; sleep 2 touch /usr/lib/systemd/system/tomcat.service cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /usr/lib/systemd/system/tomcat.service [Unit] Description=Tomcat After=syslog.target network.target remote-fs.target nss-lookup.target [Service] Type=forking ExecStart=$input/tomcat/bin/startup.sh ExecReload=$input/tomcat/bin/startup.sh ExecStop=$input/tomcat/bin/shutdown.sh [Install] WantedBy=multi-user.target EOF sed -i \u0026#39;2i\\export JAVA_HOME=\u0026#39;$JAVA_HOME\u0026#39;\u0026#39; $input/tomcat/bin/catalina.sh systemctl daemon-reload systemctl restart tomcat.service systemctl status tomcat.service echo -e \u0026#34;${RD}请确认是否选择开机启动tomcat ? ${PL}\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then systemctl disable tomcat.service echo -e \u0026#34;${GR}已为你关闭tomcat开机启动 ${PL}\u0026#34; else systemctl enable tomcat.service echo -e \u0026#34;${GR}已为你开启tomcat开机启动 ${PL}\u0026#34; fi echo -e \u0026#34;${YL}==================================================================${PL}\u0026#34; echo -e \u0026#34;${GR}tomcat部署完成并启动${PL}\u0026#34; echo -e \u0026#34;${YL}==================================================================${PL}\u0026#34; echo -e \u0026#34;${GR}外网tomcat地址: http://$(curl -sS --connect-timeout 10 -m 60 cip.cc |grep IP|awk -F \u0026#39;:[ ]\u0026#39; \u0026#39;{print $2}\u0026#39;):8080${PL}\u0026#34; echo -e \u0026#34;${GR}内网tomcat地址: http://$(ip addr | grep -E -o \u0026#39;[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\u0026#39; | grep -E -v \u0026#34;^127\\.|^255\\.|^0\\.\u0026#34; | head -n 1):8080${PL}\u0026#34; echo -e \u0026#34;${GR}你可以通过以下命令来管理tomcat服务${PL}\u0026#34; echo -e \u0026#34;${GR}启动tomcat服务：systemctl start tomcat.service${PL}\u0026#34; echo -e \u0026#34;${RD}停止tomcat服务：systemctl stop tomcat.service${PL}\u0026#34; echo -e \u0026#34;${YL}重启tomcat服务：systemctl restart tomcat.service${PL}\u0026#34; echo -e \u0026#34;${GR}若无法访问tomcat，请检查防火墙/安全组是否有放行tomcat 8080 端口${PL}\u0026#34; echo -e \u0026#34;${GR}==================================================================${PL}\u0026#34; 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211112839379/\n相关话题：https://www.cnsre.cn/tags/shell/\n","description":"一键安装jdk和tomcat。直接执行脚本就可以自动选择你安装的jdk以及tomcat的脚本，来避免避免以后这种重复的劳动力这种问题。","id":25,"section":"posts","tags":["shell"],"title":"一键安装java、tomcat脚本","uri":"https://www.cnsre.cn/posts/211112839379/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211109907029/\n相关话题：https://www.cnsre.cn/tags/k3s/\n双十一各大云厂商纷纷撒种子种韭菜（抢用户），良心云 也是一如既往的良心，新用户更是通过某宝 148 就可以买到三年 2C4G8M 的轻量应用服务器。于是我也出售薅了羊毛入手了一台。\n但是对于各种组件本身就对资源消耗比较大的 k8s 来说，跑起来还是有点费力的，于是我打算将这台实例部署一台轻量级的 Kubernetes: k3s\nk8s VS k3s k3s 是 Rancher 推出的轻量级 k8s。k3s 本身包含了 k8s 的源码，而二进制包却只有 60M 但是本质上和 k8s 没有区别。但为了降低资源占用，k3s 和 k8s 还是有一些区别的，主要是：\n使用了相比 Docker 更轻量的 containerd 作为容器运行时（Docker 并不是唯一的容器选择）。 去掉了 k8s 的 Legacy, alpha, non-default features。 用 sqlite3 作为默认的存储，而不是 etcd。 其他的一些优化，最终 k3s 只是一个 binary 文件，非常易于部署。、 所以 k3s 适用于边缘计算，IoT 等资源紧张的场景。同时 k3s 也是非常容易部署的，官网上提供了一键部署的脚本。\nk3s的优点 k3s将安装Kubernetes所需的一切打包进仅有60MB大小的二进制文件中，并且完全实现了Kubernetes API。为了减少运行Kubernetes所需的内存，Rancher删除了很多不必要的驱动程序，并用附加组件对其进行替换。 k3s是一款完全通过CNCF认证的Kubernetes发行版，这意味着你可以编写YAML来对完整版的Kubernetes进行操作，并且它们也将适用于k3s集群。 由于它只需要极低的资源就可以运行，因此它能够在任何512MB RAM以上的设备上运行集群，换言之，我们可以让pod在master和节点上运行。 k3s的缺点 首先，当前k3s的版本（k3s v0.8.1）仅能运行单个master，这意味着如果你的master宕机，那么你就无法管理你的集群，即便已有集群要继续运行。但是在k3s v0.10的版本中，多主模式已经是实验性功能，也许在下一个版本中能够GA。 其次，在单个master的k3s中，默认的数据存储是SQLite，这对于小型数据库十分友好，但是如果遭受重击，那么SQLite将成为主要痛点。但是，Kubernetes控制平面中发生的更改更多是与频繁更新部署、调度Pod等有关，因此对于小型开发/测试集群而言，数据库不会造成太大负载。 结论 K8s和k3s各有优劣，使用场景也有所区别，因此不能一概而论。如果你要进行大型的集群部署，那么我建议你选择使用K8s；\n如果你像我一样只是为了开发或者测试，那么选择k3s则是性价比更高的选择。\n安装 k3s 确保你是一台干净的 CentOS7 服务器。\n按照惯例先更新,更新前顺便把源换为国内的yum源。\n1 2 3 4 5 # 改国内yum源 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo # 更新 yum update -y 修改主机名\n1 hostnamectl set-hostname k3s-master 修改完毕后，断开重连一下。\n⚠️ 注意\nK3s 默认将使用 containerd 作为容器环境，请在下边选择 使用Docker安装 或者使用Containerd安装。\n使用docker安装 使用containerd安装 使用 docker 安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 安装 docker-ce yum remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io # 解决内核检查问题 重启生效 grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; systemctl enable docker systemctl start docker # 修改 docker 源 cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://3laho3y3.mirror.aliyuncs.com\u0026#34;] } EOF systemctl daemon-reload systemctl restart docker # 安装 docker 结束 # 关 firewalld 防火墙 systemctl stop firewalld systemctl disable firewalld # 安装 k3s curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --docker 使用 containerd 安装 1 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh - 安装完检查 安装完成后，可以执行以下命令进行主机检查。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 运行配置检查 k3s check-config # 查看节点状态以及 k3s 版本 [root@k3s-master ~]# kubectl get node NAME STATUS ROLES AGE VERSION vm-16-8-centos Ready control-plane,master 52m v1.21.5+k3s2 # 查看所有 pod 信息 [root@k3s-master ~]# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-5ff76fc89d-bbps4 1/1 Running 0 52m kube-system coredns-7448499f4d-42v9x 1/1 Running 0 52m kube-system metrics-server-86cbb8457f-xqlrg 1/1 Running 0 52m kube-system helm-install-traefik-crd-9wk9v 0/1 Completed 0 52m kube-system helm-install-traefik-d8llf 0/1 Completed 3 52m kube-system svclb-traefik-jqxvf 2/2 Running 0 49m kube-system traefik-97b44b794-wv6zv 1/1 Running 0 49m 截止到这里 k3s 已经安装完毕。\n安装 nfs 安装 nfs 服务 1 2 yum -y install nfs-utils systemctl start nfs \u0026amp;\u0026amp; systemctl enable nfs 创建nfs目录 1 mkdir -p /home/k8s/nfs 修改权限 1 chmod -R 755 /home/k8s/nfs 编辑export文件 1 2 3 cat \u0026gt;\u0026gt;/etc/exports \u0026lt;\u0026lt; EOF /home/k8s/nfs *(rw,no_root_squash,sync) EOF 配置生效 1 exportfs -r 启动rpcbind、nfs服务 1 2 systemctl restart rpcbind \u0026amp;\u0026amp; systemctl enable rpcbind systemctl restart nfs \u0026amp;\u0026amp; systemctl enable nfs 到这里 k3s 以及 nfs 已经安装完成，下面就可以去体验了。\n如果想以上都比较麻烦，你可以用下面的一键安装脚本来执行\n一键安装 k3s 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 #!/bin/bash PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin export PATH LANG=en_US.UTF-8 RD=\u0026#34;\\033[31m\u0026#34; # 错误消息 GR=\u0026#34;\\033[32m\u0026#34; # 成功消息 YL=\u0026#34;\\033[33m\u0026#34; # 告警消息 BL=\u0026#34;\\033[36m\u0026#34; # 日志消息 PL=\u0026#39;\\033[0m\u0026#39; clear echo -e \u0026#34;${YL}##################################################${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL} ${GR}脚本名称${PL}: 一键安装 k3s 脚本 ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL} ${GR}作 者${PL}: sre运维博客 ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL} ${GR}网 址${PL}: https:www.cnsre.cn ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}#${PL} ${GR}文章地址${PL}: https://cnsre.cn/posts/211109907029/ ${YL}#${PL}\u0026#34; echo -e \u0026#34;${YL}##################################################${PL}\u0026#34; sleep 0.5 set -e echo echo echo echo -e \u0026#34;${RD}是否确定安装 dockerb版本的 k3s? ${PL}\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then exit 1 else echo -e \u0026#34;$GR正在开始安装 dockerb版本的 k3s$PL\u0026#34; fi if [ `command -v docker` ];then echo -e \u0026#34;${YL}docker 已经安装,正在添加docker加速源${PL}\u0026#34; else echo -e \u0026#34;${GR}install docker${PL}\u0026#34; curl https://download.daocloud.io/docker/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo yum -y install https://download.daocloud.io/docker/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm curl -fsSL https://get.daocloud.io/docker | bash -s docker --mirror Aliyun fi sudo mkdir -p /etc/docker tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;] } EOF grubby --args=\u0026#34;user_namespace.enable=1\u0026#34; --update-kernel=\u0026#34;$(grubby --default-kernel)\u0026#34; sudo systemctl daemon-reload sudo systemctl restart docker sudo systemctl enable docker if [ `command -v k3s` ];then echo -e \u0026#34;${YL}k3s 已经安装${PL}\u0026#34; exit 1 else export K3S_NODE_NAME=${HOSTNAME//_/-} export INSTALL_K3S_EXEC=\u0026#34;--docker --kube-apiserver-arg service-node-port-range=1-65000 --no-deploy traefik --write-kubeconfig ~/.kube/config --write-kubeconfig-mode 666\u0026#34; curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh - fi echo -e \u0026#34;${GR}export K3S_TOKEN=$(cat /var/lib/rancher/k3s/server/node-token)${PL}\u0026#34; echo -e \u0026#34;${GR}export K3S_URL=https://$(ip addr | grep -E -o \u0026#39;[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\u0026#39; | grep -E -v \u0026#34;^127\\.|^255\\.|^0\\.\u0026#34; | head -n 1):6443${PL}\u0026#34; echo -e \u0026#34;${GR}安装结束，请重启服务器${PL}\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then reboot else exit 1 fi 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211109907029/\n相关话题：https://www.cnsre.cn/tags/k3s/\n","description":"k3s单机版安装部署，附一键安装脚本","id":26,"section":"posts","tags":["k3s","shell","kubernetes"],"title":"k3s单机版安装部署 附一键安装脚本","uri":"https://www.cnsre.cn/posts/211109907029/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211108848062/\n相关话题：https://www.cnsre.cn/tags/kvm/\n安装前准备 验证CPU是否支持KVM 如果结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的。\n1 egrep \u0026#39;(vmx|svm)\u0026#39; /proc/cpuinfo 关闭SELinux 将 /etc/sysconfig/selinux 中的 SELinux=enforcing 修改为 SELinux=disabled 安装一些最基本的服务 可选项，因为我是刚安装好的CentOS，所以为了下面方便点，先把一些必要的工具安装下\n1 yum install epel-release net-tools vim unzip zip wget ftp -y 安装KVM及其依赖项 1 yum install qemu-kvm libvirt virt-install bridge-utils -y 验证安装结果 下图说明已经成功安装了\n1 lsmod | grep kvm 开启kvm服务 并且设置其开机自动启动\n1 systemctl start libvirtd systemctl enable libvirtd 查看状态操作结果 如下图所示，说明运行情况良好\n1 systemctl status libvirtd 1 systemctl is-enabled libvirtd 配置网桥模式 先将 /etc/sysconfig/network-scripts/ 目录下的网卡配置文件备份一份\n创建 ifcfg-br0 文件 创建的 br0文件的IP地址要和物理网卡的IP地址一致，命令 ipconfig 查看物理网卡将不会显示IP\n内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@bogon ~]*# vim /etc/sysconfig/network-scripts/ifcfg-br0* DEVICE=br0 BOOTPROTO=none DEFROUTE=yes ONBOOT=yes TYPE=Bridge IPV4_FAILURE_FATAL=yes IPADDR=192.168.1.130 NETMASK=255.255.255.0 GATEWAY=192.168.1.254 DNS1=221.6.4.66 DELAY=0 USERCE=no 修改原网卡配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim /etc/sysconfig/network-scripts/ifcfg-eno1s TYPE=\u0026#34;Ethernet\u0026#34; PROXY_METHOD=\u0026#34;none\u0026#34; BROWSER_ONLY=\u0026#34;no\u0026#34; BOOTPROTO=\u0026#34;static\u0026#34; DEFROUTE=\u0026#34;yes\u0026#34; IPV4_FAILURE_FATAL=\u0026#34;YES\u0026#34; IPV6INIT=\u0026#34;yes\u0026#34; IPV6_AUTOCONF=\u0026#34;yes\u0026#34; IPV6_DEFROUTE=\u0026#34;yes\u0026#34; IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34; NAME=\u0026#34;eno1\u0026#34; UUID=\u0026#34;bb40d726-8d67-4187-90c3-eb61e1b42d61\u0026#34; DEVICE=\u0026#34;eno1\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; IPADDR=\u0026#34;192.168.1.130\u0026#34; NETAMSK=255.255.255.0 GATEWAY=\u0026#34;192.168.1.254\u0026#34; DNS1=\u0026#34;221.6.4.66\u0026#34; IPV6_PRIVACY=\u0026#34;no\u0026#34; BRIDGE=br0 重启网络服务 1 systemctl restart network 使用 ifconfig 验证操作结果,多了一块网卡 br0 ，现在访问宿主机 使用 192.168.1.130 就可以了。\n\u0026lt;script async src=\u0026ldquo;","description":"kvm安装windows虚拟机，kvm安装windows虚拟机找不到硬盘，kvm安装windows虚拟机找不到网卡，kvm安装windows虚拟机报错，kvm安装widnows2012","id":27,"section":"posts","tags":["kvm","vnc"],"title":"kvm 安装 windows 虚拟机","uri":"https://www.cnsre.cn/posts/211108848062/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211105852095/\n相关话题：https://www.cnsre.cn/tags/kvm/\n本文将介绍centos7 环境如何搭建一个kvm 环境并且创建windows，linux等虚拟机。\n安装前准备 验证CPU是否支持KVM 如果结果中有vmx（Intel）或svm(AMD)字样，就说明CPU的支持的。\n1 egrep \u0026#39;(vmx|svm)\u0026#39; /proc/cpuinfo 关闭SELinux 将 /etc/sysconfig/selinux 中的 SELinux=enforcing 修改为 SELinux=disabled 安装一些最基本的服务 可选项，因为我是刚安装好的CentOS，所以为了下面方便点，先把一些必要的工具安装下\n1 yum install epel-release net-tools vim unzip zip wget ftp -y 安装KVM及其依赖项 1 yum install qemu-kvm libvirt virt-install bridge-utils -y 验证安装结果 下图说明已经成功安装了\n1 lsmod | grep kvm 开启kvm服务 并且设置其开机自动启动\n1 systemctl start libvirtd systemctl enable libvirtd 查看状态操作结果 如下图所示，说明运行情况良好\n1 systemctl status libvirtd 1 systemctl is-enabled libvirtd 配置网桥模式 先将 /etc/sysconfig/network-scripts/ 目录下的网卡配置文件备份一份\n创建 ifcfg-br0 文件 创建的 br0文件的IP地址要和物理网卡的IP地址一致，命令 ipconfig 查看物理网卡将不会显示IP\n内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@bogon ~]# vim /etc/sysconfig/network-scripts/ifcfg-br0 DEVICE=br0 BOOTPROTO=none DEFROUTE=yes ONBOOT=yes TYPE=Bridge IPV4_FAILURE_FATAL=yes IPADDR=192.168.1.130 NETMASK=255.255.255.0 GATEWAY=192.168.1.254 DNS1=221.6.4.66 DELAY=0 USERCE=no 修改原网卡配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim /etc/sysconfig/network-scripts/ifcfg-eno1s TYPE=\u0026#34;Ethernet\u0026#34; PROXY_METHOD=\u0026#34;none\u0026#34; BROWSER_ONLY=\u0026#34;no\u0026#34; BOOTPROTO=\u0026#34;static\u0026#34; DEFROUTE=\u0026#34;yes\u0026#34; IPV4_FAILURE_FATAL=\u0026#34;YES\u0026#34; IPV6INIT=\u0026#34;yes\u0026#34; IPV6_AUTOCONF=\u0026#34;yes\u0026#34; IPV6_DEFROUTE=\u0026#34;yes\u0026#34; IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34; NAME=\u0026#34;eno1\u0026#34; UUID=\u0026#34;bb40d726-8d67-4187-90c3-eb61e1b42d61\u0026#34; DEVICE=\u0026#34;eno1\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; IPADDR=\u0026#34;192.168.1.130\u0026#34; NETAMSK=255.255.255.0 GATEWAY=\u0026#34;192.168.1.254\u0026#34; DNS1=\u0026#34;221.6.4.66\u0026#34; IPV6_PRIVACY=\u0026#34;no\u0026#34; BRIDGE=br0 重启网络服务 1 systemctl restart network 使用 ifconfig 验证操作结果,多了一块网卡 br0 ，现在访问宿主机 使用 192.168.1.130 就可以了。\n安装虚拟机 准备操作系统安装镜像文件 在本文中将使用和宿主环境一样的 CentOS7.2，把该文件放到 /home/iso 目录下\n挂载U盘\n1 2 3 4 5 6 7 8 9 10 [root@nkgtsv01 ~]# yum install fuse-ntfs-3g -y [root@nkgtsv01 ~]# ls /mnt/ udisk usb [root@nkgtsv01 ~]# ls /mnt/udisk/ CentOS-7.2-x86_64-DVD-1611.iso CentOS-7-x86_64-DVD-1708.iso maven_storey2.zip [root@bogon data]# mkdir -p /data/iso [root@bogon data]# ls iso kvm-bak network [root@nkgtsv01 ~]# mount -o loop /mnt/udisk/CentOS-7-x86_64-DVD-1708.iso /data/iso/ mount: /dev/loop0 写保护，将以只读方式挂载 创建虚拟机文件存放的目录 1 mkdir -p /data/kvm-images 使用 virt-install 创建虚拟机 1 virt-install --virt-type=kvm --name=njkvm07 --vcpus=4 --memory=6000 --location=/data/iso/CentOS-7-x86-64-DVD-1708.iso --disk path=/data/kvm-images/njkvm07.qcow2,size=200,format=qcow2 --network bridge=br0 --graphics none --extra-args=\u0026#39;console=ttyS0\u0026#39; --force 执行完这段命令\n感叹号为待选项\nc保存 q退出 b开始执行安装\n选择地区时间\n选则之后c保存\n自动返回主页面\n选择硬盘\n选择到硬盘 c保存\n全新安装 C保存\n选则添加IP地址和添加主机名称\n输入 回车\n添加IP地址\n选则ipv4\n添加IP地址回车\n添加netmask gateway c 保存\n添加密码\nB开始执行安装\n安装完成\n宿主机直接连接到虚拟机\n等待重启虚拟机，虚拟机起开之后直接远程就ok了 。\nvirt-clone克隆虚拟机 使用 virt-clone 克隆新的虚拟机 （虚拟机需要先关闭）\n1 virt-clone -o njvm02 -n njvm03 -f /data/kvm-img/njvm03.img 克隆完查看 所有的虚拟机以及状态\n1 virsh list --all 删除虚拟机 njvm01\n1 2 virsh undefine njvm01 virsh destroy njvm01 ⚠️ 注意\n取消定义 删除以后要找到虚拟机文件路径吧虚拟机文件也删除掉\n1 2 3 4 5 [root@nkgtsv01 data]# virsh shutdown njvm01 域 njvm01 被关闭 [root@nkgtsv01 data]# virsh start njvm02 域 njvm02 已开始 [root@nkgtsv01 data]# virsh list --all ⚠️ 注意\n克隆完以后因为IP地址还是njvm01的IP地址所以我们要修改IP地址\n开启我们克隆的虚拟机\n远程登陆上去\n1 2 3 4 5 6 7 8 9 10 11 12 [root@nkgtsv-vm01 ~]# cd /etc/sysconfig/network-scripts/ [root@nkgtsv-vm01 network-scripts]# ls ifcfg-eth0 ifdown-ppp ifup-eth ifup-sit ifcfg-lo ifdown-routes ifup-ippp ifup-Team ifdown ifdown-sit ifup-ipv6 ifup-TeamPort ifdown-bnep ifdown-Team ifup-isdn ifup-tunnel ifdown-eth ifdown-TeamPort ifup-plip ifup-wireless ifdown-ippp ifdown-tunnel ifup-plusb init.ipv6-global ifdown-ipv6 ifup ifup-post network-functions ifdown-isdn ifup-aliases ifup-ppp network-functions-ipv6 ifdown-post ifup-bnep ifup-routes [root@nkgtsv-vm01 network-scripts]# vim ifcfg-eth0 IPADDR=192.168.1.121 改为我们想要的IP地址\n保存退出\nService network restart\n重启网络\n重新链接\n参考文档：\nhttp://www.linuxidc.com/Linux/2017-01/140007.htm\nhttp://blog.csdn.net/u011414200/article/details/47310827\nhttps://www.cnblogs.com/5201351/p/4445199.html\nhttp://blog.51cto.com/7834466/2064277\nhttps://www.cnblogs.com/Yemilice/p/8080688.html\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211105852095/\n相关话题：https://www.cnsre.cn/tags/kvm/\n","description":"本文将介绍centos7 环境如何搭建一个kvm 环境并且创建虚拟机等。","id":28,"section":"posts","tags":["kvm"],"title":"Centos7搭建 KVM 命令行安装虚拟机","uri":"https://www.cnsre.cn/posts/211105852095/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211103006342/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n环境准备 zabbix-agent python2.7 pip 安装python 模块 pip2.7 install boto3 pip install awscli 部署py脚本 git地址 ：https://github.com/datorama/zabbix_rds_template\n# 克隆模板 git clone https://github.com/datorama/zabbix_rds_template.git # 进入模板目录 cd zabbix_rds_template # 可以看到一下内容 zabbix用的py为脚本 xml 为zabbix web端的模板 rds_stats.py rds_template.xml README.md # 把zabbix需要用的脚本放在zabbix 执行脚本的目录 cp rds_stats.py usr/local/share/zabbix/externalscripts cd usr/local/share/zabbix/externalscripts chmod +x rds_stats.py # 给执行权限 准备密钥ID 和访问密钥 Access key ID Secret access key AKxxxxxxxxxxxxxxxxx SKxxxxxxxxxxxxxxxxx # 执行命令 aws configure # 配置访问秘钥、私有访问秘钥、可用区域等，最后的格式为json格式 添加主机模板 先导入模板rds_template.xml\n添加主机 填入主机名称 dns 名称为RDS的 Endpoint\n添加模板\n在模板的宏部分{$ AWS_ACCESS_KEY} ，{$ AWS_SECRET_KEY}中输入aws的aksk或将其留空以使用IAM角色\n在模板的宏部分{$ REGION}中设置默认的AWS区域\n{$ AWS_ACCESS_KEY} = AKxxxxxxxxxxxxxxxxx {$ AWS_SECRET_KEY} = SKxxxxxxxxxxxxxxxxx {$ REGION} = cn-north-1 点击添加 等待数据\n⚠️ 注意\n需要注意的点是 要安装py的一些模块。\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211103006342/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"公司用了zabbix监控，主要监控也是zabbix，为了能在同一个平台上看到所有的服务信息，需要用zabbix 监控 aws rds 数据库。","id":29,"section":"posts","tags":["aws","zabbix"],"title":"zabbix 监控 aws rds 数据库","uri":"https://www.cnsre.cn/posts/211103006342/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211101950005/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n前言介绍 需要对页面 url 对页面的延迟，访问时间进行监控。如果页面不是正常的状态就发送告警。基于这个需求，使用zabbix 对url 的进行监控，使用的是zabbix 的web监控功能。\n选择主机添加应用集 选在相应主机，并添加Web监控 按照方式新建Web场景\n注意：\n名称统一规则：相应的域名\n应用集：选择之前创建的 或者 新建一个，名称为URL 告警\n更新间隔：默认为1m\n尝试次数：默认1次\n客户端：选择IE 11.0\n​ 添加\n添加步骤 ⚠️注意： 名称使用和场景名称一样即可：域名地址\nURL：复制nagios的URL地址，将域名替换为zabbix上的宏:端口{HOST.IP}:80\n如：http://http://download.tujia.com/monitor.html\n写为：http://http://{HOST.IP}/monitor.html\nPS：如果地址为 https 访问，此处直接写http即可\n头部：\n名称：host\n值：填写为监控页面的域名，如此例中的 download.tujia.com\n要求的字符串：填写之前访问测试页面包含的字符串，如ok、IsSuccess:true等，此例为{\u0026quot;status\u0026quot;:\u0026quot;failure\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;-1 参数不全\u0026quot;}\nURL地址\n添加完成，并且检测成功后，可观察如下\n主机中没有报错信息\n添加告警，触发报警 1）切回原主机界面，添加触发器 2）创建触发器： ⚠️注意： 触发器名称：规则为域名+无法响应，如此例中的download.tujia.com+无法响应\n严重性：选择一般严重\n表达式： 建立过程如下\n如下图中，当建立完web检测后，主机中会生成响应的监控项，其中一项web.test.fail[web_check_download.tujia.com],\n若web检测失败，此监控项的值为1，若正常，则为0\n用于检测此web检测是否成功，此例中的trigger含义为，当此web检测连续3min检测失败（选择周期T的最小值\u0026gt;N，T为3m），则触发，表达式的构建如下：\n{主机名称:监控项名称.条件}+比较符号+数值，如下为此例中的表达式\n{HZ 3A 150:web.test.fail[URLapi.php告警].min(3m)}\u0026lt;0\n{ 主机名 :监控项（此web检测失败） .条件（三分钟内）}\u0026gt;0\n表达式：如果最近连续4次取到的response_code 值不是200（网站响应代码），则触发报警\n{192.168.3.86:web.test.rspcode[3.86_http_status,3.86_http_status].last(0)}\u0026lt;\u0026gt;200 and {192.168.3.86:web.test.rspcode[3.86_http_status,3.86_http_status].last(1)}\u0026lt;\u0026gt;200 and {192.168.3.86:web.test.rspcode[3.86_http_status,3.86_http_status].last(2)}\u0026lt;\u0026gt;200 and {192.168.3.86:web.test.rspcode[3.86_http_status,3.86_http_status].last(3)}\u0026lt;\u0026gt;200\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211101950005/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"需要对页面 url 对页面的延迟，访问时间进行监控。如果页面不是正常的状态就发送告警。","id":30,"section":"posts","tags":["zabbix"],"title":"zabbix 使用 web 监控功能监控 url","uri":"https://www.cnsre.cn/posts/211101950005/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211029955133/\n相关话题：https://www.cnsre.cn/tags/故障集/\n前言 Linux 磁盘空间总是报警，查到到大文件，删除之后，df看到磁盘空间并没有释放。\n用 du -sh ./* | sort -nr （查看当前目录下文件的大小）通过查找了下发现文件被mysql 的zabbix库占用了 zabbix 已经迁移可以删除\nrm 删除之后 df 查看 发现磁盘空间并没有得到释放\n执行 lsof | grep deleted 发现有大量刚刚删除文件的进程存在\nkill掉进程（或者重启进程） OK\n问题分析 一般说来不会出现删除文件后空间不释放的情况，但是也存在例外，比如文件被进程锁定，或者有进程一直在向这个文件写数据等等，要理解这个问题，就需要知道Linux下文件的存储机制和存储结构。\n一个文件在文件系统中的存放分为两个部分：数据部分和指针部分，指针位于文件系统的 meta-data 中，数据被删除后，这个指针就从 meta-data 中清除了，而数据部分存储在磁盘中，数据对应的指针从 meta-data 中清除后，文件数据部分占用的空间就可以被覆盖并写入新的内容，之所以出现删除 access_log 文件后，空间还没释放，就是因为 httpd 进程还在一直向这个文件写入内容，导致虽然删除了 access_log 文件，但文件对应的指针部分由于进程锁定，并未从 meta-data 中清除，而由于指针并未被删除，那么系统内核就认为文件并未被删除，因此通过 df 命令查询空间并未释放也就不足为奇了。\n处理问题 既然有了解决问题的思路，那么接下来看看是否有进程一直在向文件中写，这里需要用到 Linux 下的 lsof 命令，通过这个命令可以获取一个已经被删除但仍然被应用程序占用的文件列表，命令执行如下图所示：\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/211029955133/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"linux服务器删除文件后 df 查看磁盘空间并没有释放","id":31,"section":"posts","tags":["故障集"],"title":"删除文件后 df 查看磁盘空间并没有释放","uri":"https://www.cnsre.cn/posts/211029955133/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211027957506/\n相关话题：https://www.cnsre.cn/tags/故障集/\n最近接到一个case 涉及到Linux下面大硬盘(大于2T)的使用,以后大家遇到大硬盘的可能性会越来越多,特把处理过程分享出来,以供大家参考\n问题背景 Dell r85 机架式服务器 配置6个1T硬盘, 做完底层raid后,操作系统可以认到3T\n问题 在实际使用中,系统只用到2T的空间,最后1T始终无法分区,报错如下:\n原因分析 安装Linux时,使用了普通的MBR模式 ,对主流主板BIOS而言，MBR分区定义每个扇区512字节，磁盘寻址32位地址，所能访问的磁盘容量最大是2.19TB（512 * 232）,因此系统能使用的最大空间也为2T\n解决方案 通过咨询Dell 原厂,提供了两个解决方案\n1.安装Linux 系统时重新更改 BIOS的启动模式为UEFI\n2.重新划分底层 raid 划分为一个小于 2T 的盘做为安装系统的盘,另一个大于2T的盘做为数据盘使用.\n实操过程 在遇到大硬盘后,最好电询原厂,看是否支持UEFI\n另外: 在划分区时,如果选择自定义,在redhat 6.X 之后要必须要新建 /boot/efi 分区.大小不能小于 100M 我在实际操作时设置了200M\n原厂的解释是:\n/boot/efi/ partition (100 MB minimum) — the partition mounted on /boot/efi/ contains all the installed kernels, the initrd images, and ELILO configuration files.\n修改BIOS中设置,下面以Dell R815 为例说明修改\n1.重启主机,按F2 进入BIOS 设置 (或按各厂商提示),\n出现该界面后,按 F2 ,\n生效后如下图\n在引导到一定阶段后会自动进入BIOS 设置,按方向键上,下选择 [Boot Settings] 后,回车\n在小窗口用方向键选择 [Boot Mode] 使用 -/+ 选择启动方式为 [UEFI]\n修改后ESC退出时保存\nnote: uEFI does not read from legacy mbr labels\n2.更改磁盘的 disk lable 更改为 GPT\n因为On hardware where uEFI is enabled, the boot disk must have a GPT partition label\n在安装任意介面,输入 Ctrl+Alt+F2 切换到命令行介面,使用 parted 更改 disk lable为 gpt\nnote: 如果是已经安装过系统的主机,这一步时必须要先将旧分区全部删除,才能更改成功\n因为是重装,所以要先把旧分区删除,红框标示出来的就是传统的 mbr 模式,只能支持最大2T 的硬盘\n删除旧分区\n更改 disk label 为 gpt\n更改完成后,使用 Ctrl+Alt+F6 回切至安装界面,按正常步骤安装即可\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211027957506/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"Linux下面大硬盘(大于2T)的使用,以后大家遇到大硬盘的可能性会越来越多,特把处理过程分享出来,以供大家参考","id":32,"section":"posts","tags":["故障集"],"title":"大于2T硬盘安装 Redhat 系统","uri":"https://www.cnsre.cn/posts/211027957506/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211025912047/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n环境介绍 操作系统：centos 7.4\nzabbix版本：zabbix server 3.4.7\n客户端：zabbix-agent 3.4.7\n监控进程：mysqld\n监控端口：3306 tcp\n进程监控 确认客户端已经安装且运行agent\n查看进程\n查看属于那个用户的 几个进程\nmysql 的进程为root用户 两个进程\n添加监控项\n名称随便写\n类型zabbix客户端\n键值选则进程数返回数\n应用集选则prosesses 进程\nproc.num[\u0026lt;name\u0026gt;,\u0026lt;user\u0026gt;,\u0026lt;state\u0026gt;,\u0026lt;cmdline\u0026gt;]\n以下是对mysql进程的监控配置，key中的参数说明，\n\u0026lt;name\u0026gt;第一个参数是进程名字，没必要填写，填了反而会使监控不太准确（仅个人测试）\n\u0026lt;user\u0026gt;第二个参数是运行进程的用户名\n\u0026lt;state\u0026gt;第三个为进程的状态 ，一般选则all 包括：all (default), run, sleep, zomb\n\u0026lt;cmdline\u0026gt;第四个参数用来指定进程名中包含的字符，对进程进行过滤。\n确认更新\n创建触发器\n选择刚才创建的监控项\n插入\n修改{hgh3a01:proc.num[,root,all,mysqld].last()}=0\n为\n{hgh3a01:proc.num[,root,all,mysqld].max(#2)}=2\nhgh3a01：主机名称\nproc.num[,root,all,mysqld]：监控项\nmax(#2)}=2 ：表示最后两次的接收到的值都是2个进程（ps -ef 看到 mysqld 的进程为两个所以=2 ），说明mysqld进程在运行，则出发报警。\n因为我们要测试是否能出发告警，所以要选则 =2 正常的\u0026lt;1 就是没有运行。\n稍等几分钟观察看看能不能触发触发器\n收到邮箱警告\n吧测试的进程改为正常的（因为mysql 运行的进程为两个我这变设置的触发器为进程小于两个进程就发出告警）\n保存更新观察是否回复正常\n监控端口 添加监控项\n修改端口 保存更新\n同监控进程一样（先测试）\n收到触发警告\n吧之前的值调整为0\n测试回复正常\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211025912047/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix 通过 zabbix_agent 对应用的进程、端口进行自定义监控告警。","id":33,"section":"posts","tags":["zabbix"],"title":"zabbix 通过 agent 监控进程、端口","uri":"https://www.cnsre.cn/posts/211025912047/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211021950307/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n系统环境 Zabbix 版本：3.4\n操作系统版本：centos7.4\n监控分区 / 、/boot 、/home\n先创建监控项的模板 点击创建监控项 名称随意填写，键值的话因为我们监控车的是磁盘剩余的百分比所有选则次键值，应用集则选则filsystems\n选则键值 选则添加我们的监控项已经创建。\n查看监控项看下创建测监控项是否异常。 选则需要监控的主机 接下来我们来创建监控项。\n监控项名称随意填写。\n然后吧严重性选则为一般告警（因为我们短信邮箱告警等级是一般告警以上的告警等级推送）\n选则我们刚才创建的监控项\n修改表达式 写出监控的分区我们这以home分区为例。\n完成以后添加即可。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211021950307/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"","id":34,"section":"posts","tags":["zabbix"],"title":"zabbix自定义监控磁盘分区","uri":"https://www.cnsre.cn/posts/211021950307/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211012054181/\n相关话题：https://www.cnsre.cn/tags/aws/\n文章链接\n在 /etc/resolv.conf 中配置了 DNS 只能临时生效，重启就丢失了，这种情况下如何永久修改保留 NDS 呢？\n简短描述 默认情况下，与 Amazon Virtual Private Cloud (Amazon VPC) 关联的 Amazon EC2 实例在启动时使用动态主机配置协议 (DHCP) 请求 DNS 服务器地址。DHCP 响应返回写入到本地 /etc/resolv.conf 文件的 DNS 服务器地址。重新启动实例时，对具有自定义 DNS 服务器地址的 resolv.conf 文件的手动修改将丢失。您用于解决此问题的方法取决于您的 Linux 发行版。有关 VPC 和 DNS 服务器的更多信息，请参阅 Amazon DNS 服务器。\n解决方法 ⚠️ 重要提示：\n更改 Amazon EC2 实例之前，使用 Amazon 系统映像 (AMI) 或 Amazon Elastic Block Store (Amazon EBS) 快照创建备份。更改实例的联网配置可能会导致实例无法访问。 Amazon Linux、Amazon Linux 2 使用以下选项之一来配置 Amazon EC2 实例。如果您同时应用这两个选项，则在 ifcfg-eth0 文件中指定的 DNS 服务器会优先（选项 2）。\n要使任一选项正常运行，必须将 ifcfg-eth0 文件中的 PEERDNS 参数值设置为 yes。将 PEERDNS 参数设置为 no 意味着，ifcfg-* 文件中指定的或 DHCP 提供的 DNS 服务器会被忽略。\n选项1 选项2 选项 1：\n编辑或创建 /etc/dhcp/dhclient.conf 文件。 ⚠️ 注意： 您必须拥有根用户权限才能编辑此文件。您可以使用 sudo -i 成为根用户，或者使用 sudo 执行所有命令。 2. 将 **supersede** 命令添加到文件以覆盖 domain-name-servers。在以下示例中，将 **xxx.xxx.xxx.xxx** 替换为您希望实例使用的 DNS 服务器的 IP 地址： 1 supersede domain-name-servers xxx.xxx.xxx.xxx, xxx.xxx.xxx.xxx; 在上述修改之后，resolv.conf 文件将在实例重启时更新，以仅包含您在 dhclient 文件中指定的 DNS 服务器。有关 supersede 命令的更多信息，请参阅 Linux 手册页上的 dhclient.conf(5)。\n在每个接口的配置文件 (/etc/sysconfig/network-scripts/ifcfg-*) 中将 PEERDNS 参数设置为 yes。\n重启 EC2 实例。\n选项 2：\n要覆盖 /etc/dhcp/dhclient.conf 文件中的 DNS 服务器值，请在每个接口的配置文件 (/etc/sysconfig/network-scripts/ifcfg-*) 中指定自定义 DNS 服务器。 例如，下例中显示来自 Amazon Linux 实例的 /etc/sysconfig/network-scripts/ifcfg-eth0 文件修改为包含两个自定义 DNS 服务器（DNS1 和 DNS2）：\nDEVICE=eth0 BOOTPROTO=dhcp ONBOOT=yes TYPE=Ethernet USERCTL=yes PEERDNS=yes IPV6INIT=no PERSISTENT_DHCLIENT=yes RES_OPTIONS=\u0026#34;timeout:2 attempts:5\u0026#34; DHCP_ARP_CHECK=no MTU=\u0026#34;9001\u0026#34; DNS1=8.8.8.8 DNS2=8.8.4.4 在每个接口的配置文件 (/etc/sysconfig/network-scripts/ifcfg-*) 中将 PEERDNS 参数设置为 yes。 Ubuntu 16.04 编辑或创建 /etc/dhcp/dhclient.conf 文件。 ⚠️ 注意： 您必须拥有根用户权限才能编辑此文件。您可以使用 sudo -i 成为根用户，或者使用 sudo 执行所有命令。\n2. 将 supersede 命令添加到文件以覆盖 domain-name-servers。在以下示例中，将 xxx.xxx.xxx.xxx 替换为您希望实例使用的 DNS 服务器的 IP 地址：\nsupersede domain-name-servers xxx.xxx.xxx.xxx, xxx.xxx.xxx.xxx; 在此修改之后，resolv.conf 文件将在实例重启时更新，以仅包含您在 dhclient 文件中指定的 DNS 服务器。有关 supersede 命令的更多信息，请参阅 Linux 手册页上的 dhclient.conf(5)。\n重启实例。 Ubuntu 18.04 默认情况下，在 Ubuntu 18.04 上，由 netplan.io 软件包处理网络接口配置，且由启用系统解析的服务使用存根解析程序处理 DNS 查询。存根解析程序 IP 位于 /etc/resolv.conf。\n反过来，/etc/resolv.conf 文件是 /run/systemd/resolve/stub-resolv.conf 文件的符号链接。如果以下任一项对于 /etc/resolv.conf 文件为真，/etc/dhcp/dhclient.conf 中的 supersede 语句可能无法按预期起作用：\n文件不是您的实例上的符号链接。 文件是一个指向不同文件的符号链接，如 /run/systemd/resolve/resolv.conf。 上述任一条件都表示自定义了默认的 Ubuntu 18.04 配置。\n运行以下步骤以覆盖 DNS 服务器值：\nNetplan 通常将配置文件存储在 /etc/netplan 目录中。创建名为 /etc/netplan/99-custom-dns.yaml 的文件，然后通过以下行填充此文件。请务必将占位符 DNS 服务器 IP 地址替换为首选地址： network: version: 2 ethernets: eth0: nameservers: addresses: [1.2.3.4, 5.6.7.8] dhcp4-overrides: use-dns: false 在进行这些更改后，您将仍然可以在 /etc/resolv.conf 中看到存根解析程序 IP。这是正常的。存根解析程序 IP 对于您的操作系统来说是本地的，在后台中，存根解析程序将使用您在前述 99-custom-dns.yaml 文件中指定的 DNS 服务器。\n重启实例。\n运行 systemd-resolve 命令以确认系统正确地提取预期的 DNS 服务器 IP 地址：\n1 systemd-resolve --status RHEL 7.5 默认情况下，由 NetworkManager 服务管理 resolv.conf 文件。然后，该服务会通过 DHCP 提供的 DNS 服务器填充此文件。阻止 NetworkManager 管理 resolv.conf 文件，以使 resolv.conf 文件忽略 DHCP 提供的 DNS 服务器。\n选项1 选项2 选项 1：\n编辑或创建 /etc/dhcp/dhclient.conf 文件。 ⚠️ 注意： 您必须拥有根用户权限才能编辑此文件。您可以使用 sudo -i 成为根用户，或者使用 sudo 执行所有命令。 2. 将 **supersede** 命令添加到文件以覆盖 domain-name-servers。在以下示例中，将 **xxx.xxx.xxx.xxx** 替换为您希望实例使用的 DNS 服务器的 IP 地址： 1 supersede domain-name-servers xxx.xxx.xxx.xxx, xxx.xxx.xxx.xxx; 在此修改之后，resolv.conf 文件将在实例重启时更新，以仅包含您在 dhclient 文件中指定的 DNS 服务器。有关 supersede 命令的更多信息，请参阅 Linux 手册页上的 dhclient.conf(5)。\n在每个接口的配置文件 (/etc/sysconfig/network-scripts/ifcfg-*) 中将 PEERDNS 参数设置为 yes。\n重启实例。\n选项 2：\n使用以下内容创建 /etc/NetworkManager/conf.d/disable-resolve.conf-managing.conf 文件： [main] dns=none 重启实例，然后手动填充 /etc/resolv.conf 文件。 相关视频\nPooja 演示了如何将重启期间保留的自定义 DNS 服务器分配给 Amazon EC2 实例\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211012054181/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"","id":35,"section":"posts","tags":["aws"],"title":"Amazon EC2 永久修改静态 DNS","uri":"https://www.cnsre.cn/posts/211012054181/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211009115571/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n简介 ​ Orabbix 是设计用来为 zabbix 监控 Oracle 数据库的插件，它提供多层次的监控，包括可用性和服务器性能指标。\n​ 它提供了从众多 oracle 实例采集数据的有效机制，进而提供此信息的监控和性能指标。然后，您可以利用的 zabbix 的报告功能为收集的所有数据，并提供分析。目前的发行版中包含了一组预先定义的模板，包括从初始部署警报和图形功能。然而，这些可以进行微调，以满足您的需求和数据/监控要求\n环境介绍 系统环境：\nlinux Centos 7.4\n3.10.0-693.21.1.el7.x86_64\nzabbix版本\nzabbix 3.4.7\nOrabbix 监控什么？ 数据库版本\n归档日志与生产趋势分析\n触发器，表/过程等命中率\n逻辑 I/O 性能\n物理 I/O 性能\nPGA\nSGA\n共享池\nSessions\n数据库大小\n表空间\n安装配置 zabbix server 端操作\nOrabbix插件的下载 http://www.smartmarmot.com/product/orabbix/download/\n安装 orabbix 创建目录\n1 2 3 mkdir -p /opt/orabbix mv orabbix-1.2.3.zip /opt/orabbix cd /opt/orabbix yum安装unzip\n1 2 3 yum install unzip -y # 解压oeabbix unzip orabbix-1.2.3.zip 创建orabbix备份\ncp conf/config.props.sample conf/config.props 启动程序拷贝至/etx/init.d/\ncp init.d/orabbix /etc/init.d/ 分配权限\nchmod +x /etc/init.d/orabbix chmod +x /opt/orabbix/run.sh 安装jdk\nyum install java -y 创建数据库账号 ## oracle 服务器端操作 ##\n登录 oracle 命令行\n1 2 3 4 5 6 7 8 9 10 11 su - oracle # 切换到 oracle 用户 sqlplus /nolog # 不连接任何数据库 conn /as sysdba # 用sysdba 登陆 # 或者 conn 用户名/密码 select instance_name from v$instance; # 查看实例 首先我们需要在被监控的Oracle上面创建一个账号，用于zabbix的数据获取，在oracle的sqlplus里面执行\n1 2 3 4 5 6 7 8 9 10 11 CREATE USER ZABBIX IDENTIFIED BY \u0026#34;zabbix\u0026#34; DEFAULT TABLESPACE SYSTEM TEMPORARY TABLESPACE TEMP PROFILE DEFAULT ACCOUNT UNLOCK； 赋予角色权限\n1 2 3 4 5 GRANT CONNECT TO ZABBIX; GRANT RESOURCE TO ZABBIX; ALTER USER ZABBIX DEFAULT ROLE ALL; 赋予系统权限\n1 2 3 4 5 6 7 8 9 GRANT SELECT ANY TABLE TO ZABBIX; GRANT CREATE SESSION TO ZABBIX; GRANT SELECT ANY DICTIONARY TO ZABBIX; GRANT UNLIMITED TABLESPACE TO ZABBIX; GRANT SELECT ANY DICTIONARY TO ZABBIX; 如果我们的数据库是Oracle 11g，我们还需要执行下面的语句\n注释：官方文档是需要执行这个语句的，测试没有执行也一样可以用，目前没有发现问题（可参考）\nexec dbms_network_acl_admin.create_acl(acl =\u0026gt; \u0026#39;resolve.xml\u0026#39;,description =\u0026gt; \u0026#39;resolve acl\u0026#39;, principal =\u0026gt;\u0026#39;ZABBIX\u0026#39;, is_grant =\u0026gt; true, privilege =\u0026gt; \u0026#39;resolve\u0026#39;); exec dbms_network_acl_admin.assign_acl(acl =\u0026gt; \u0026#39;resolve.xml\u0026#39;, host =\u0026gt;\u0026#39;*\u0026#39;); commit; 参考官网文档\nhttp://www.smartmarmot.com/wiki/index.php/Orabbix\n编辑刚刚生成的config.props文件 zabbix server 端操作\nvi /opt/orabbix/conf/config.props ** 修改后内容如下 ** #comma separed list of Zabbix servers ZabbixServerList=ZabbixServer ZabbixServer.Address=192.168.2.145 **#zabbix server IP地址** ZabbixServer.Port=10051 **#端口** ZabbixServer2.Address=IP_ADDRESS_OF_ZABBIX_SERVER ZabbixServer2.Port=PORT_OF_ZABBIX_SERVER #pidFile OrabbixDaemon.PidFile=./logs/orabbix.pid #frequency of item\u0026#39;s refresh OrabbixDaemon.Sleep=300 #MaxThreadNumber should be \u0026gt;= than the number of your databases OrabbixDaemon.MaxThreadNumber=100 #put here your databases in a comma separated list DatabaseList=192.168.2.142 **# 名称与该机在 zabbix 中监控的主机名称保持一致** #Configuration of Connection pool #if not specified Orabbis is going to use default values (hardcoded) #Maximum number of active connection inside pool DatabaseList.MaxActive=10 #The maximum number of milliseconds that the pool will wait #(when there are no available connections) for a connection to be returned #before throwing an exception, or \u0026lt;= 0 to wait indefinitely. DatabaseList.MaxWait=100 DatabaseList.MaxIdle=1 #define here your connection string for each database 192.168.2.142.Url=jdbc:oracle:thin:@192.168.2.142:1521:orcl **# 需要 jdk 环境，因为这里是通过 JDBC 连接的，** ​ **#orcl 为数据库实例名称** 192.168.2.142.User=ZABBIX **# 用来监控 oracle 数据库的用户名和密码，需要在 oracle 中创建并赋予一定的权限** 192.168.2.142.Password=ZABBIX #Those values are optionals if not specified Orabbix is going to use the general values 192.168.2.142.MaxActive=10 192.168.2.142.MaxWait=100 192.168.2.142.MaxIdle=1 192.168.2.142.QueryListFile=./conf/query.props #DB2.Url=jdbc:oracle:thin:@server2.domain.example.com:\u0026lt;LISTENER_PORT\u0026gt;:DB2 #DB2.User=zabbix #DB2.Password=zabbix_password #DB2.QueryListFile=./conf/query.props #DB3.Url=jdbc:oracle:thin:@server3.domain.example.com:\u0026lt;LISTENER_PORT\u0026gt;:DB3 #DB3.User=zabbix #DB3.Password=zabbix_password #DB3.QueryListFile=./conf/query.props ——————分—————————割—————————线————————\n**注：**以上端口号为与Zabbix Server通讯的端口，我这里是将Orabbix与Zabbix server 装在同一台机器上的，如果不在同一台机器，那装Orabbix的机器需要先装Zabbix Agent，否则数据将无法传送到Zabbix Server。\n注：\nZabbixServerList：可以设置多个，用\u0026quot;,\u0026ldquo;进行分割；\nDatabaseList：可以设置多个被监控的Oracle数据库服务器，用\u0026rdquo;,\u0026ldquo;进行分割，该名称要和zabbix server界面中的Host name保持一致，该配置文件中后续所引用的设定都以该名称为准。\n关于 JDBC 可参考：\nOracle = jdbc:oracle:thin:@\u0026lt;host\u0026gt;:\u0026lt;LISTENER_PORT\u0026gt;:\u0026lt;instance\u0026gt; PostgreSQL = jdbc:postgresql://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt; MS Sql Server = jdbc:jtds:sqlserver://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;instancename\u0026gt; [MySQL](http://lib.csdn.net/base/mysql) Server = jdbc:[mysql](http://lib.csdn.net/base/mysql)://[host:port],[host:port].../[database] DB2 = jdbc:db2://\u0026lt;servername\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;installation\u0026gt; 启动服务 /etc/init.d/orabbix start # 或 systemctl start orabbix 配置zabbix添加监控 zabbix web 端操作 导入模板 模板在/opt/orabbix/template/ 目录下面，全部导入zabbix web 即可\n添加主机 主机名称必须要和配至文件中的databaseLst 中的名称一致\n添加oracle 模板 验证 监控库的大小 配置 query.props 默认按照上面的步骤就差不多了，但是你导入模板之后就会发现监控项目不全，如dbsize及dbfilesize这些类目，orabbix默认情况下未开启数据库大小，需要配置query.props\ncp /opt/orabbix/conf/query.props /opt/orabbix/conf/query.props.bak vi /opt/orabbix/conf/query.props 在QueryList=类目下增加dbfilesize,dbsize，如图所示\n然后在该文件的末尾添加 dbfilesize.Query=select to_char(sum(bytes/1024/1024/10), \u0026#39;FM99999999999999990\u0026#39;) retvalue from dba_data_files dbsize.Query=SELECT to_char(sum( NVL(a.bytes/1024/1024/10 - NVL(f.bytes/1024/1024/10, 0), 0)), \u0026#39;FM99999999999999990\u0026#39;) retvalue \\ FROM sys.dba_tablespaces d, \\ (select tablespace_name, sum(bytes) bytes from dba_data_files group by tablespace_name) a, \\ (select tablespace_name, sum(bytes) bytes from dba_free_space group by tablespace_name) f \\ WHERE d.tablespace_name = a.tablespace_name(+) AND d.tablespace_name = f.tablespace_name(+) \\ AND NOT (d.extent_management like \u0026#39;LOCAL\u0026#39; AND d.contents like \u0026#39;TEMPORARY\u0026#39;) 用以定义查询dbfilesize,dbsize的SQL语句\n添加内容如图所示\n注释：语句过长的话要用 \\ 来分隔\n这个时候就可以启动orabbix服务了\nsystemctl restart orabbix 检查服务ps aux |grep orabbix|wc -l 如果等于2就说明启动那个成功了\n查看日志看是否有报错\n/opt/orabbix/logs/orabbix.log 表空间监控的优化 自定义SQL检查 Orabbix提供了表空间的监控，监控项对应的SQL:\nvi /opt/orabbix/conf/query.props tbl_space.Query=SELECT * FROM ( \\ select \u0026#39;- Tablespace -\u0026gt;\u0026#39;,t.tablespace_name ktablespace, \\ \u0026#39;- Type-\u0026gt;\u0026#39;,substr(t.contents, 1, 1) tipo, \\ \u0026#39;- Used(MB)-\u0026gt;\u0026#39;,trunc((d.tbs_size-nvl(s.free_space, 0))/1024/1024) ktbs_em_uso, \\ \u0026#39;- ActualSize(MB)-\u0026gt;\u0026#39;,trunc(d.tbs_size/1024/1024) ktbs_size, \\ \u0026#39;- MaxSize(MB)-\u0026gt;\u0026#39;,trunc(d.tbs_maxsize/1024/1024) ktbs_maxsize, \\ \u0026#39;- FreeSpace(MB)-\u0026gt;\u0026#39;,trunc(nvl(s.free_space, 0)/1024/1024) kfree_space, \\ \u0026#39;- Space-\u0026gt;\u0026#39;,trunc((d.tbs_maxsize - d.tbs_size + nvl(s.free_space, 0))/1024/1024) kspace, \\ \u0026#39;- Perc-\u0026gt;\u0026#39;,decode(d.tbs_maxsize, 0, 0, trunc((d.tbs_size-nvl(s.free_space, 0))*100/d.tbs_maxsize)) kperc \\ from \\ ( select SUM(bytes) tbs_size, \\ SUM(decode(sign(maxbytes - bytes), -1, bytes, maxbytes)) tbs_maxsize, tablespace_name tablespace \\ from ( select nvl(bytes, 0) bytes, nvl(maxbytes, 0) maxbytes, tablespace_name \\ from dba_data_files \\ union all \\ select nvl(bytes, 0) bytes, nvl(maxbytes, 0) maxbytes, tablespace_name \\ from dba_temp_files \\ ) \\ group by tablespace_name \\ ) d, \\ ( select SUM(bytes) free_space, \\ tablespace_name tablespace \\ from dba_free_space \\ group by tablespace_name \\ ) s, \\ dba_tablespaces t \\ where t.tablespace_name = d.tablespace(+) and \\ t.tablespace_name = s.tablespace(+) \\ order by 8) \\ where kperc \u0026gt; 93 \\ and tipo \u0026lt;\u0026gt;\u0026#39;T\u0026#39; \\ and tipo \u0026lt;\u0026gt;\u0026#39;U\u0026#39; tbl_space.NoDataFound=none 这个SQL会返回93%满的表空间信息，而对应这个监控项，orabbix也定义了触发器，因为监控项的返回值是文本，而没有满足条件的记录时返回字符串“none“，所以监控项对应的触发器会检查返回值开头是不是none，如果不是，就报警，这样，用户除了收到预警信息，还能从返回值的具体值中看到具体时哪个表空间快满了。\n当然，大部分时间监控项会返回none，所以我们无法画出正常未满的表空间的空间占用时间曲线。只有超过93%慢时，我们才知道具体的占用情况。\n测试 把值调为5的触发效果\n参考文档\nhttps://www.cnblogs.com/dujiaxiaoK/p/7719049.html\nhttps://blog.csdn.net/frank0521/article/details/7469457\nhttp://www.zhimengzhe.com/shujuku/other/182171.html\n官方文档\nhttp://www.smartmarmot.com/wiki/index.php/Orabbix\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/211009115571/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"","id":36,"section":"posts","tags":["zabbix","oracle"],"title":"zabbix 监控 oracle 数据库","uri":"https://www.cnsre.cn/posts/211009115571/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210930010174/\n相关话题：https://www.cnsre.cn/tags/nginx/\n为了将客户端和服务器之间的连接从 HTTP/1.1 转换为 WebSocket，使用了 HTTP/1.1 中可用的协议切换机制。\n但是由于 Upgrade 是一个 hop-by-hop 标头，它不会从客户端传递到代理服务器。通过正向代理，客户端可以使用该CONNECT 方法来规避此问题。然而，这不适用于反向代理，因为客户端不知道任何代理服务器，并且需要在代理服务器上进行特殊处理。\n从 1.3.13 版本开始，nginx 实现了特殊的操作模式，如果代理服务器返回代码为 101（切换协议）的响应，并且客户端通过请求中的 Upgrade 标头。\n如上所述，包括 Upgrade 和 Connection 在内的逐跳标头不会从客户端传递到代理服务器，因此为了让代理服务器了解客户端将协议切换到 WebSocket 的意图，这些标头必须明确传递：\n1 2 3 4 5 6 location /chat/ { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; } 一个更复杂的示例，其中对代理服务器的请求中“Connection”标头字段的值取决于客户端请求标头中“Upgrade”字段的存在：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 http { map $http_upgrade $connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } server { ... location /chat/ { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } } 默认情况下，如果代理服务器在 60 秒内没有传输任何数据，连接将被关闭。可以使用proxy_read_timeout指令增加此超时 。或者，可以将代理服务器配置为定期发送 WebSocket ping 帧以重置超时并检查连接是否仍然有效。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210930010174/\n相关话题：https://www.cnsre.cn/tags/nginx/\n","description":"","id":37,"section":"posts","tags":["websocket","nginx"],"title":"Nginx WebSocket 代理","uri":"https://www.cnsre.cn/posts/210930010174/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210928957081/\n相关话题：https://www.cnsre.cn/tags/故障集/\n前言 线上服务器磁盘告警\n登录服务器检查磁盘发现一个叫做 journal 的文件夹占用了大量空间。\n问题分析 Journald是什么 journal 是 centos7 中 systemd 的一个组件，由 journald处理。捕获系统日志信息、内核日志信息，以及来自原始RAM磁盘的信息，早期启动信息以及所有服务中写入 STDOUT 和 STDERR 数据流的信息。可以说是为 Linux 服务器打造的一种新系统日志方式，这些日志信息写入到二进制文件，使用 journalctl 阅读，默认存放在 /run/log/ 下。 Journald系统主要由三个主要的系统日记服务组件组成：\n守护程序：systemd 日志服务由 systemd-journald 守护程序处理。 配置文件：日志服务的配置在 /etc/systemd/journald.conf 里面设置。 日志搜索程序：用于搜索日记日志文件的程序是 journalctl。 journalctl使用 journalctl 常用命令\n1 2 3 4 5 6 7 8 9 10 11 journalctl #查看所有日志 journalctl -n 5 #查看最后5条日志 journalctl -p err #查看err类型的日志 journalctl -f #不断输出最后10条日志 journalctl --since today #查看今天的日志 journalctl --since \u0026#34;2021-9-28 08:00:00\u0026#34; --until \u0026#34;2021-9-28 09:00:00\u0026#34; journalctl -o verbose #查看日志详细信息 journalctl --disk-usage #检查当前journal使用磁盘量 journalctl --vacuum-time=2d #只保存2天的日志 journalctl --vacuum-size=500M #最大500M journalctl --verify #检查journal是否运行正常以及日志文件是否完整无损坏 持久保存日志\n由于 journald 默认是保存在内存中，一旦服务器重启，就会丢失，作为生成环境，管理员必须保证系统任何日志不能丢失，通过修改配置文件做持久保存。\n同时，systemd-journald.service 的配置文件主要参考 /etc/systemd/journald.conf 的内容，详细的参数可以参考如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 [Journal] #日志存储到磁盘 Storage=persistent #压缩日志 Compress=yes #为日志添加序列号 Seal=yes #每个用户分别记录日志 SplitMode=uid #日志同步到磁盘的间隔，高级别的日志，如：CRIT、ALERT、EMERG 三种总是实时同步 SyncIntervalSec=1m #即制日志的最大流量，此处指 30s 内最多记录 100000 条日志，超出的将被丢弃 RateLimitInterval=30s #与 RateLimitInterval 配合使用 RateLimitBurst=100000 #限制全部日志文件加在一起最多可以占用多少空间，默认值是10%空间与4G空间两者中的较小者 SystemMaxUse=64G #默认值是15%空间与4G空间两者中的较大者 SystemKeepFree=1G #单个日志文件的大小限制，超过此限制将触发滚动保存 SystemMaxFileSize=128M #日志滚动的最大时间间隔，若不设置则完全以大小限制为准 MaxFileSec=1day #日志最大保留时间，超过时限的旧日志将被删除 MaxRetentionSec=100year #是否转发符合条件的日志记录到本机的其它日志管理系统，如：rsyslog ForwardToSyslog=yes ForwardToKMsg=no #是否转发符合条件的日志到所有登陆用户的终端 ForwardToWall=yes MaxLevelStore=debug MaxLevelSyslog=err MaxLevelWall=emerg ForwardToConsole=no #TTYPath=/dev/console #MaxLevelConsole=info #MaxLevelKMsg=notice 处理过程 知道了这些，就可以轻松的处理这些日志了。\n检查当前journal使用磁盘量\n1 journalctl --disk-usage 清理方法可以采用按照日期清理，或者按照允许保留的容量清理，只保存2天的日志，最大500M\n1 2 journalctl --vacuum-time=2d journalctl --vacuum-size=500M 要启用日志限制持久化配置，可以修改\n1 2 3 4 5 6 vim /etc/systemd/journald.conf SystemMaxUse=16M ForwardToSyslog=no # 重启 systemctl restart systemd-journald.service 检查journal是否运行正常以及日志文件是否完整无损坏\n1 journalctl --verify 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210928957081/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"","id":38,"section":"posts","tags":["journal","故障集"],"title":"journal log日志的问题","uri":"https://www.cnsre.cn/posts/210928957081/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210924925541/\n相关话题：https://www.cnsre.cn/tags/docker/\n文章链接\n在学习K8S 的过程中总，部分镜像需要从 k8s.grc.io 仓库中拉拉取。但是因为网络的问题导致无法拉取狗歌的镜像，也就导致了创建 pod 拉取镜像失败。\n今天就跟大家分享下我从国外拉到国内的镜像。\n替换规则 同理，其他镜像地址也可以用该 dockerhub 的地址。\n具体替换规则：\nk8s.gcr.io 替换为 cnsre\ningress-nginx/controller 替换为 ingress-nginx-controller\n1 2 3 k8s.gcr.io/ingress-nginx/controller:v1.0.0 # 等同于 cnsre/ingress-nginx-controller:v1.0.0 举例 比如 ingress 中的镜像地址为 k8s.gcr.io/ingress-nginx/controller:v1.0.0\n需要将镜像地址更改为：cnsre/ingress-nginx-controller:v1.0.0\ndocker pull k8s.gcr.io/ingress-nginx/controller:v1.0.0 # 等同于 docker pull cnsre/ingress-nginx-controller:v1.0.0 dockerhub 中没有的镜像 如果 cnsre 仓库中没有的一些国外镜像需要下载的话，你可以在地址留言(最好在留言中补充邮箱信息，这样你就可以收到通知。)\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210924925541/\n相关话题：https://www.cnsre.cn/tags/docker/\n","description":"","id":39,"section":"posts","tags":["docker","kubernetes"],"title":"国外镜像拉取到docker hub","uri":"https://www.cnsre.cn/posts/210924925541/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210922013357/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\nK8S 使用 kubectl top 看K8S监控\nkubectl top 是基础命令，但是需要部署配套的组件才能获取到监控值\n部署 metric-server\nKubernetes Metrics Server 是 Cluster 的核心监控数据的聚合器，kubeadm 默认是不部署的。\nMetrics Server 供 Dashboard 等其他组件使用，是一个扩展的 APIServer，依赖于 API Aggregator。所以，在安装 Metrics Server 之前需要先在 kube-apiserver 中开启 API Aggregator。\nMetrics API 只可以查询当前的度量数据，并不保存历史数据。\nMetrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 下维护。\n必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 kubelet Summary API 获取数据.\n不指定pod 名称，则显示命名空间下所有 pod，\u0026ndash;containers可以显示 pod 内所有的container\n指标含义：\n和k8s中的request、limit一致，CPU单位100m=0.1 内存单位1Mi=1024Ki\npod的内存值是其实际使用量，也是做limit限制时判断oom的依据。pod的使用量等于其所有业务容器的总和，不包括 pause 容器，值等于cadvisr中的container_memory_working_set_bytes指标\nnode的值并不等于该node 上所有 pod 值的总和，也不等于直接在机器上运行 top 或 free 看到的值\n要求 注意：使用 Metrics Server 有必备两个条件：\nAPI Server 启用 Aggregator Routing 支持 API Server 能访问 Metrics Server Pod IP 启用API Aggregator 安装 metric-server 下载yaml文件 可以通过运行以下命令下载最新的 Metrics Server 版本：\n1 wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 兼容性：\n指标服务器 指标 API 组/版本 支持的 Kubernetes 版本 0.6.x metrics.k8s.io/v1beta1 *1.19+ 0.5.x metrics.k8s.io/v1beta1 *1.8+ 0.4.x metrics.k8s.io/v1beta1 *1.8+ 0.3.x metrics.k8s.io/v1beta1 1.8-1.21 对于 \u0026lt;1.16 需要--authorization-always-allow-paths=/livez,/readyz 修改镜像地址 国内无法下载 k8s.gcr.io/metrics-server/metrics-server:v0.4.1镜像\n需要修改为一下镜像地址\n1 cnsre/metrics-server-metrics-server:v0.4.1 添加 \u0026ndash;kubelet-insecure-tls参数 1 2 3 4 5 6 7 8 spec: containers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --kubelet-insecure-tls # 新增内容 部署metrics-server服务 1 kubectl apply -f components.yaml 查看状态 1 2 3 [root@master ~]# kubectl -n kube-system get deploy metrics-server NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1/1 1 1 9m39s 测试 1 2 3 4 5 6 7 8 9 10 11 12 [root@master ~]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% master 294m 8% 5971Mi 41% node1 198m 5% 3800Mi 26% [root@master ~]# kubectl top pods NAME CPU(cores) MEMORY(bytes) cnsre-deployment-cf6fddb9f-bvfdk 1m 1Mi cnsre-deployment-cf6fddb9f-d8spc 1m 1Mi cnsre-deployment-cf6fddb9f-nrrhz 1m 1Mi nfs-client-provisioner-fd74f99b4-gk8l4 4m 8Mi tomcat-deployment-66dc86bb8f-6xj28 2m 974Mi tomcat-deployment-66dc86bb8f-db66l 2m 884Mi 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210922013357/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"","id":40,"section":"posts","tags":["kubernetes","metric-server"],"title":"kubernetes 部署 metric-server","uri":"https://www.cnsre.cn/posts/210922013357/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210917128049/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n下载deployment 我这里保存成kube-event.yaml\n# cat kube-event.yaml --- apiVersion: apps/v1 kind: Deployment metadata: labels: name: kube-eventer name: kube-eventer namespace: kube-system spec: replicas: 1 selector: matchLabels: app: kube-eventer template: metadata: labels: app: kube-eventer annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026#39;\u0026#39; spec: dnsPolicy: ClusterFirstWithHostNet serviceAccount: kube-eventer containers: - image: registry.aliyuncs.com/acs/kube-eventer-amd64:v1.2.0-484d9cd-aliyun name: kube-eventer command: - \u0026#34;/kube-eventer\u0026#34; - \u0026#34;--source=kubernetes:https://kubernetes.default\u0026#34; ## .e.g,dingtalk sink demo #- --sink=dingtalk:[your_webhook_url]\u0026amp;label=[your_cluster_id]\u0026amp;level=[Normal or Warning(default)] - --sink=dingtalk:https://oapi.dingtalk.com/robot/send?access_token=355cf0156xxxxxxxxxxxxxxxxxx\u0026amp;level=Warning env: # If TZ is assigned, set the TZ value as the time zone - name: TZ value: \u0026#34;Asia/Shanghai\u0026#34; volumeMounts: - name: localtime mountPath: /etc/localtime readOnly: true - name: zoneinfo mountPath: /usr/share/zoneinfo readOnly: true resources: requests: cpu: 100m memory: 100Mi limits: cpu: 500m memory: 250Mi volumes: - name: localtime hostPath: path: /etc/localtime - name: zoneinfo hostPath: path: /usr/share/zoneinfo --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: kube-eventer rules: - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - events verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kube-eventer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-eventer subjects: - kind: ServiceAccount name: kube-eventer namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-eventer namespace: kube-system 钉钉群里创建自定义webhook 设置\u0026ndash;智能群助手\u0026ndash;添加机器人\u0026ndash;选择WeebHook。定义机器人名称和安全设置\n安全设置这里我定义了关键字，Waring。创建后复制webhook地址。然后更改上面deployment中的sink处。\n我把上面的label删掉了，只留下了level=Waring，刚好对应了我关键字的Waring。只有带有关键字的才会触发告警。\n测试告警 然后创建一个测试的Tomcat的deployment，故意把image镜像的tag写错，让他无法拉取镜像\n[root@master allenjol]# kubectl apply -f deploy-tomcat-test.yaml deployment.apps/tomcat-deployment-allenjol created service/tomcat-service-allenjol created [root@master allenjol]# kubectl get po NAME READY STATUS RESTARTS AGE tomcat-deployment-allenjol-b6687f99-l5vj9 0/1 ImagePullBackOff 0 45s 部署kube-event.yaml并查看日志。可以看到隔30s去检测一次\n]# kubectl apply -f kube-event.yaml ]# kubectl get po -n kube-system | grep kube-event [root@master allenjol]# kubectl logs -f kube-eventer-648f64c985-zfkkg -n kube-system I0708 09:26:36.409034 1 eventer.go:67] /kube-eventer --source=kubernetes:https://kubernetes.default --sink=dingtalk:https://oapi.dingtalk.com/robot/send?access_token=355cf01569aef206dc6c05681aaf3ed0ea19ed3597db4c26c565dbeb69ce1303\u0026amp;level=Warning I0708 09:26:36.409191 1 eventer.go:68] kube-eventer version: v1.2.0 commit: 484d9cd I0708 09:26:36.411557 1 eventer.go:94] Starting with DingTalkSink sink I0708 09:26:36.411596 1 eventer.go:108] Starting eventer I0708 09:26:36.411678 1 eventer.go:116] Starting eventer http service I0708 09:27:00.000163 1 manager.go:102] Exporting 5 events I0708 09:27:30.000130 1 manager.go:102] Exporting 9 events I0708 09:28:00.000147 1 manager.go:102] Exporting 1 events I0708 09:28:30.000150 1 manager.go:102] Exporting 4 events I0708 09:29:00.000138 1 manager.go:102] Exporting 1 events ... 可以看到这里已经看到了钉钉的webhook地址了，并且还收集到了events。\n查看钉钉群，就会看到已经出现了告警了。\n其实这个告警当前还存在点问题。个人认为不应该这么频繁发送，应该像prometheus一样可以配置抑制和静默。然后监控时间可以更改。当然熟悉go语言可以自己改源码然后构建成镜像。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210917128049/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"kube-eventer事件监控通过钉钉告警","id":41,"section":"posts","tags":["kubernetes","kube-eventer"],"title":"kube-eventer事件监控","uri":"https://www.cnsre.cn/posts/210917128049/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210915154434/\n相关话题：https://www.cnsre.cn/tags/故障集/\n情况介绍 负责的项目下有一批 ubuntu 18.04 的服务器在 AWS 上，因为安全的问题，需要把内核从 5.3.0 升级到 5.4.0。\n首次升级为测试环境测两台都是ubuntu 18.04 的版本 内核也都为5.3.0。第一台升级进展很顺利。软件更新，然后内核进行单独升级。等到需要重启的时候出现了问题。\n处理问题及解决思路 问题1 无法挂载磁盘\n首先遇到的第一个问题\n解决思路：\n升级内核导致 boot 空间越来越小，然后导致无法引导进入系统。因为之前遇到过boot空间占满的情况。但是那是在 kvm 的 vm 中，可以通过 VNC 进行链接修复。这在 aws 上怎么办？\n解决方法：\n一开始我选择了将改服务器的根磁盘取消挂载。然后挂载到同一可用区的其他服务器上，进行修复。因为磁盘格式的问题，始终挂载不上，为了避免浪费时间，只能以快照恢复的方式将根磁盘进行扩容。\n以快照的方式恢复了回复，在快照恢复的过程中将根磁盘扩容的方法果然将服务器运行起来了。\n后面就接着尝试进行内核升级\u0026hellip;.\n问题2 内核升级数据库依赖报错？\n具体内容如下：\n解决思路：\n这个问题，真的是没有思路。处理了很久，都没有解决这个问题。还希望有思路的能到指导下。\n解决方法:\n为了快速解决内核升级的问题，我将 mysql 以及相关的依赖都卸载掉了。\n问题3 升级完重启失败？\n这个问题也是最大的问题，最明显的表现就是。升级没有报错，但是升级完需要重启，服务器进行重启的时候无法进入操作系统。\n此时已经是凌晨4点多钟了，已经很迷糊了。然后就把服务器恢复到升级内核前的样子。打算明天启动快照进行复现。\n解决思路：\n又是挂载失败？怎么又会遇到挂载失败呢？最后发现重启自动挂载磁盘的配置并没有按照官方的指示去做使用UUID的配置开启挂载盘符。从而系统会检测磁盘的过程中会检测到该错误。无法正常进如系统。\n解决方法：\n如果是物理机，或者是可以通过其他方式进行控制引导的话还可以修复。但是云主机怎么修复呢？只能去修复磁盘了\n在云主机上有两种访问磁盘卷的方法\n方法 1：使用 EC2 控制台\n（摘自 AWS 文档）\n如果您为 Linux 启用了 EC2 串行控制台，则可以使用它来排查受支持的基于 Nitro 的实例类型问题。串行控制台可帮助您排查启动问题、网络配置和 SSH 配置问题。串行控制台无需网络连接即可连接到您的实例。您可以使用 Amazon EC2 控制台或 AWS 命令行界面 (AWS CLI) 访问串行控制台。\n在使用串行控制台之前，请在账户层面授予对串行控制台的访问权限。然后，创建 AWS Identity and Access Management (IAM) 策略，授予对 IAM 用户的访问权限。此外，每个使用串行控制台的实例都必须至少包含一个基于密码的用户。如果您的实例无法访问，并且尚未配置对串行控制台的访问权限，请按照方法 2 中的说明进行操作。有关为 Linux 配置 EC2 串行控制台的信息，请参阅配置对 EC2 串行控制台的访问权限。\n注意：如果在运行 AWS CLI 命令时遇到错误，请确保您使用的是最新版本的 AWS CLI。\n方法 2：挂载到其他实例上\n创建一个临时救援实例，然后将您的 Amazon Elastic Block Store (Amazon EBS) 卷重新挂载到该救援实例上。从该救援实例中，您可以将 GRUB 配置为使用以前的内核进行启动。\n**重要提示：**请勿在实例存储支持的实例上执行此操作。由于此恢复方法需要首先停止然后再重启实例，该实例上的任何数据都将丢失。有关更多信息，请参阅确定实例的根设备类型。\n为根卷创建 EBS 快照。有关更多信息，请参阅创建 Amazon EBS 快照。 打开 Amazon EC2 控制台。 注意： 请确保您位于正确的区域。 从导航窗格中选择 实例，然后选择受损的实例。 选择 Instance State（实例状态）、Stop Instance（停止实例），然后选择 Stop（停止）。 在 **Storage（存储）**选项卡的 **Block devices（块储存设备）**下，为 /dev/sda1 或 /dev/xvda 选择 Volume ID（卷 ID）。 依次选择 操作 、 断开卷 ，然后选择 是，请分离 。记下可用区。 在同一可用区中启动一个救援 EC2 实例。 启动救援实例后，从导航窗格中选择 卷，然后选择受损实例已分离的根卷。 依次选择 操作、附加卷 。 选择救援实例 ID (id-xxxxx)，然后设置一个未使用的设备。在本示例中为 /dev/sdf。 使用 SSH 连接到救援实例。 运行 lsblk 命令以查看可用的磁盘设备： lsblk # 输出如下： xvda 202:0 0 20G 0 disk └─xvda1 202:1 0 20G 0 part / xvdb 202:16 0 100G 0 disk xvdf 202:80 0 15G 0 disk └─xvdf1 202:81 0 15G 0 part # 该磁盘为故障集服务器根磁盘 查看磁盘格式\nlsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT xvda └─xvda1 ext4 cloudimg-rootfs d32458a7-7f4c-415f-9a66-b579f14fb82d / xvdb ext4 eb0e325a-471c-4a99-a9be-a3ee296c2405 xvdf └─xvdf1 ext4 cloudimg-rootfs d32458a7-7f4c-415f-9a66-b579f14fb82d 挂载磁盘\nsudo -i mount /dev/xvdf1 /mnt 然后查看挂载目录，发现根磁盘已经挂载到了mnt下\n查看配置文件\nubuntu@ip-10-0-20-27:~$ cat /etc/fstab LABEL=cloudimg-rootfs / ext4 defaults,discard 0 0 /dev/nvme0n1 /data ext4 defaults 0 0 查看官网挂载文档如下：\n重启后自动挂载附加的卷 (摘自AWS 官方文档)\n要在每次系统重启时附加附加的 EBS 卷，可在 /etc/fstab 文件中为该设备添加一个条目。\n您可以在 /dev/xvdf 中使用设备名称（如 /etc/fstab），但建议改为使用设备的 128 位通用唯一标识符 (UUID)。设备名称可以更改，但 UUID 会在整个分区的使用寿命期间保留。通过使用 UUID，您可以减少系统在硬件重新配置后无法启动的机会。有关更多信息，请参阅识别 EBS 设备。\n重启后自动附加附加卷\n（可选）创建 /etc/fstab 文件的备份，以便在编辑时误损坏或删除此文件时使用。\n[ec2-user ~]$ sudo cp /etc/fstab /etc/fstab.orig 使用 blkid 命令查找设备的 UUID。记下要在重新启动后挂载的设备的 UUID。在下一步中您将需要用到它。\n例如，以下命令显示有两个设备挂载到实例上，并显示了两个设备的 UUID。\n[ec2-user ~]$ sudo blkid /dev/xvda1: LABEL=\u0026#34;/\u0026#34; UUID=\u0026#34;ca774df7-756d-4261-a3f1-76038323e572\u0026#34; TYPE=\u0026#34;xfs\u0026#34; PARTLABEL=\u0026#34;Linux\u0026#34; PARTUUID=\u0026#34;02dcd367-e87c-4f2e-9a72-a3cf8f299c10\u0026#34; /dev/xvdf: UUID=\u0026#34;aebf131c-6957-451e-8d34-ec978d9581ae\u0026#34; TYPE=\u0026#34;xfs\u0026#34; 对于 Ubuntu 18.04，请使用 lsblk 命令。\n[ec2-user ~]$ sudo lsblk -o +UUID 使用任何文本编辑器（如 /etc/fstab 和 nano）打开 vim 文件。\n[ec2-user ~]$ sudo vim /etc/fstab 将以下条目添加到 /etc/fstab 以在指定的挂载点挂载设备。这些字段是 blkid（或用于 Ubuntu 18.04 的 lsblk）返回的 UUID 值、挂载点、文件系统以及建议的文件系统挂载选项。有关必填字段的更多信息，请运行 man fstab 以打开 fstab 手册。\n在以下示例中，我们将 UUID 为 aebf131c-6957-451e-8d34-ec978d9581ae 的设备挂载到挂载点 /data，然后我们使用 xfs 文件系统。我们还使用 defaults 和 nofail 标志。我们指定 0 以防止文件系统被转储，并且我们指定 2 以指示它是非根设备。\nUUID=aebf131c-6957-451e-8d34-ec978d9581ae /data xfs defaults,nofail 0 2 注意\n如果您要在未附加此卷的情况下启动实例（例如，将卷移动到另一个实例之后），nofail 附加选项允许该实例即使在卷附加过程中出现错误时也可启动。Debian 衍生物 (包括早于 16.04 的 Ubuntu 版本) 还必须添加 nobootwait 挂载选项。\n要检查条目是否有效，请在 /etc/fstab 中运行以下命令以卸载设备，然后挂载所有文件系统。如果未产生错误，则说明 /etc/fstab 文件正常，您的文件系统会在重启后自动挂载。\n[ec2-user ~]$ sudo umount /data [ec2-user ~]$ sudo mount -a 如果收到错误消息，请解决文件中的错误。\n警告\n/etc/fstab 文件中的错误可能显示系统无法启动。请勿关闭 /etc/fstab 文件中有错误的系统。\n如果您无法确定如何更正 /etc/fstab 中的错误并且您在此过程的第一步中创建了一个备份文件，则可以使用以下命令从您的备份文件还原。\n[ec2-user ~]$ sudo mv /etc/fstab.orig /etc/fstab 查看修改日期核对修改时间\n问题都解决了。接下来继续升级内核吧。\nsudo apt-get install linux-image-5.4.0-1055-aws 等待重启查看\n终于成功了。。。\n问题总结 问题1\n更新内核导致引导分区存储占满。 优化\n在ubuntu 进行内核补丁软件更新时需要注意boot、root分区的容量。以避免重启后无法正常引导进入系统。 问题2\n更新下载软件，提示 were encountered while processing 优化\n后测试发现更新下载任何软件都会出现这种情况，暂未解决。 问题3\n磁盘开机自动挂载配置问题。 优化\n以后需要严格按照 AWS 官方文档来进行操作部署，以免再次遇到类似的事情发生。 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210915154434/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"AWS Ubuntu 18.04  升级内核5.4.0 的故障处理思路以及解决方法","id":42,"section":"posts","tags":["ubuntu","故障集","aws"],"title":"记一次 Ubuntu 内核升级故障处理","uri":"https://www.cnsre.cn/posts/210915154434/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210913932516/\n相关话题：https://www.cnsre.cn/tags/故障集/\n为了避免数据库服务器等内网应用资源暴露到公网中，打算利用VPN 技术实现链接到内网。\n本文主要介绍CentOS 7 服务器上安装与配置OpenVPN服务器,以及如何编写客户端连接到新建立的OpenVPN服务器上所需的配置文件\nOpenVPN的介绍 OpenVPN是一个开源的应用程序，它允许您通过公共互联网创建一个安全的专用网络。OpenVPN实现一个虚拟专用网（VPN）来创建一个安全连接。OpenVPN使用OpenSSL库提供加密，它提供了几种身份验证机制，如基于证书的、预共享密钥和用户名/密码身份验证。\nopenvpn 有两种模式 数据包（TUN模式）或数据帧（TAP模式）\nTUN模式：TUN模拟了网络层设备，第三层数据包如IP封包，底层数据隧道数据 TAP模式等同于一个设备，第二操作层数据包如扩展数据帧，创建一个相对桥接接，复杂T 接口接口的好处可见，客户端优化VPN服务子网的IP（忽然忽隐忽现）物理上的区别，可以完全将客户端看做完全与VPN服务器相关的时间，而TUN接口下所有的客户端则出现一个独立的子网内，与VPN服务器相关的子网没有关系，这种使用比较好，和公司的网络区分开，完全是一个虚拟的网络\n脚本内容 openvpn 一键安装脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 #!/bin/bash # # https://github.com/Nyr/openvpn-install # # Copyright (c) 2013 Nyr. Released under the MIT License. # Detect Debian users running the script with \u0026#34;sh\u0026#34; instead of bash if readlink /proc/$$/exe | grep -q \u0026#34;dash\u0026#34;; then echo \u0026#39;This installer needs to be run with \u0026#34;bash\u0026#34;, not \u0026#34;sh\u0026#34;.\u0026#39; exit fi # Discard stdin. Needed when running from an one-liner which includes a newline read -N 999999 -t 0.001 # Detect OpenVZ 6 if [[ $(uname -r | cut -d \u0026#34;.\u0026#34; -f 1) -eq 2 ]]; then echo \u0026#34;The system is running an old kernel, which is incompatible with this installer.\u0026#34; exit fi # Detect OS # $os_version variables aren\u0026#39;t always in use, but are kept here for convenience if grep -qs \u0026#34;ubuntu\u0026#34; /etc/os-release; then os=\u0026#34;ubuntu\u0026#34; os_version=$(grep \u0026#39;VERSION_ID\u0026#39; /etc/os-release | cut -d \u0026#39;\u0026#34;\u0026#39; -f 2 | tr -d \u0026#39;.\u0026#39;) group_name=\u0026#34;nogroup\u0026#34; elif [[ -e /etc/debian_version ]]; then os=\u0026#34;debian\u0026#34; os_version=$(grep -oE \u0026#39;[0-9]+\u0026#39; /etc/debian_version | head -1) group_name=\u0026#34;nogroup\u0026#34; elif [[ -e /etc/centos-release ]]; then os=\u0026#34;centos\u0026#34; os_version=$(grep -oE \u0026#39;[0-9]+\u0026#39; /etc/centos-release | head -1) group_name=\u0026#34;nobody\u0026#34; elif [[ -e /etc/fedora-release ]]; then os=\u0026#34;fedora\u0026#34; os_version=$(grep -oE \u0026#39;[0-9]+\u0026#39; /etc/fedora-release | head -1) group_name=\u0026#34;nobody\u0026#34; else echo \u0026#34;This installer seems to be running on an unsupported distribution. Supported distributions are Ubuntu, Debian, CentOS, and Fedora.\u0026#34; exit fi if [[ \u0026#34;$os\u0026#34; == \u0026#34;ubuntu\u0026#34; \u0026amp;\u0026amp; \u0026#34;$os_version\u0026#34; -lt 1804 ]]; then echo \u0026#34;Ubuntu 18.04 or higher is required to use this installer. This version of Ubuntu is too old and unsupported.\u0026#34; exit fi if [[ \u0026#34;$os\u0026#34; == \u0026#34;debian\u0026#34; \u0026amp;\u0026amp; \u0026#34;$os_version\u0026#34; -lt 9 ]]; then echo \u0026#34;Debian 9 or higher is required to use this installer. This version of Debian is too old and unsupported.\u0026#34; exit fi if [[ \u0026#34;$os\u0026#34; == \u0026#34;centos\u0026#34; \u0026amp;\u0026amp; \u0026#34;$os_version\u0026#34; -lt 7 ]]; then echo \u0026#34;CentOS 7 or higher is required to use this installer. This version of CentOS is too old and unsupported.\u0026#34; exit fi # Detect environments where $PATH does not include the sbin directories if ! grep -q sbin \u0026lt;\u0026lt;\u0026lt; \u0026#34;$PATH\u0026#34;; then echo \u0026#39;$PATH does not include sbin. Try using \u0026#34;su -\u0026#34; instead of \u0026#34;su\u0026#34;.\u0026#39; exit fi if [[ \u0026#34;$EUID\u0026#34; -ne 0 ]]; then echo \u0026#34;This installer needs to be run with superuser privileges.\u0026#34; exit fi if [[ ! -e /dev/net/tun ]] || ! ( exec 7\u0026lt;\u0026gt;/dev/net/tun ) 2\u0026gt;/dev/null; then echo \u0026#34;The system does not have the TUN device available. TUN needs to be enabled before running this installer.\u0026#34; exit fi new_client () { # Generates the custom client.ovpn { cat /etc/openvpn/server/client-common.txt echo \u0026#34;\u0026lt;ca\u0026gt;\u0026#34; cat /etc/openvpn/server/easy-rsa/pki/ca.crt echo \u0026#34;\u0026lt;/ca\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;cert\u0026gt;\u0026#34; sed -ne \u0026#39;/BEGIN CERTIFICATE/,$ p\u0026#39; /etc/openvpn/server/easy-rsa/pki/issued/\u0026#34;$client\u0026#34;.crt echo \u0026#34;\u0026lt;/cert\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;key\u0026gt;\u0026#34; cat /etc/openvpn/server/easy-rsa/pki/private/\u0026#34;$client\u0026#34;.key echo \u0026#34;\u0026lt;/key\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;tls-crypt\u0026gt;\u0026#34; sed -ne \u0026#39;/BEGIN OpenVPN Static key/,$ p\u0026#39; /etc/openvpn/server/tc.key echo \u0026#34;\u0026lt;/tls-crypt\u0026gt;\u0026#34; } \u0026gt; ~/\u0026#34;$client\u0026#34;.ovpn } if [[ ! -e /etc/openvpn/server/server.conf ]]; then clear echo \u0026#39;Welcome to this OpenVPN road warrior installer!\u0026#39; # If system has a single IPv4, it is selected automatically. Else, ask the user if [[ $(ip -4 addr | grep inet | grep -vEc \u0026#39;127(\\.[0-9]{1,3}){3}\u0026#39;) -eq 1 ]]; then ip=$(ip -4 addr | grep inet | grep -vE \u0026#39;127(\\.[0-9]{1,3}){3}\u0026#39; | cut -d \u0026#39;/\u0026#39; -f 1 | grep -oE \u0026#39;[0-9]{1,3}(\\.[0-9]{1,3}){3}\u0026#39;) else number_of_ip=$(ip -4 addr | grep inet | grep -vEc \u0026#39;127(\\.[0-9]{1,3}){3}\u0026#39;) echo echo \u0026#34;Which IPv4 address should be used?\u0026#34; ip -4 addr | grep inet | grep -vE \u0026#39;127(\\.[0-9]{1,3}){3}\u0026#39; | cut -d \u0026#39;/\u0026#39; -f 1 | grep -oE \u0026#39;[0-9]{1,3}(\\.[0-9]{1,3}){3}\u0026#39; | nl -s \u0026#39;) \u0026#39; read -p \u0026#34;IPv4 address [1]: \u0026#34; ip_number until [[ -z \u0026#34;$ip_number\u0026#34; || \u0026#34;$ip_number\u0026#34; =~ ^[0-9]+$ \u0026amp;\u0026amp; \u0026#34;$ip_number\u0026#34; -le \u0026#34;$number_of_ip\u0026#34; ]]; do echo \u0026#34;$ip_number: invalid selection.\u0026#34; read -p \u0026#34;IPv4 address [1]: \u0026#34; ip_number done [[ -z \u0026#34;$ip_number\u0026#34; ]] \u0026amp;\u0026amp; ip_number=\u0026#34;1\u0026#34; ip=$(ip -4 addr | grep inet | grep -vE \u0026#39;127(\\.[0-9]{1,3}){3}\u0026#39; | cut -d \u0026#39;/\u0026#39; -f 1 | grep -oE \u0026#39;[0-9]{1,3}(\\.[0-9]{1,3}){3}\u0026#39; | sed -n \u0026#34;$ip_number\u0026#34;p) fi # If $ip is a private IP address, the server must be behind NAT if echo \u0026#34;$ip\u0026#34; | grep -qE \u0026#39;^(10\\.|172\\.1[6789]\\.|172\\.2[0-9]\\.|172\\.3[01]\\.|192\\.168)\u0026#39;; then echo echo \u0026#34;This server is behind NAT. What is the public IPv4 address or hostname?\u0026#34; # Get public IP and sanitize with grep get_public_ip=$(grep -m 1 -oE \u0026#39;^[0-9]{1,3}(\\.[0-9]{1,3}){3}$\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$(wget -T 10 -t 1 -4qO- \u0026#34;http://ip1.dynupdate.no-ip.com/\u0026#34; || curl -m 10 -4Ls \u0026#34;http://ip1.dynupdate.no-ip.com/\u0026#34;)\u0026#34;) read -p \u0026#34;Public IPv4 address / hostname [$get_public_ip]: \u0026#34; public_ip # If the checkip service is unavailable and user didn\u0026#39;t provide input, ask again until [[ -n \u0026#34;$get_public_ip\u0026#34; || -n \u0026#34;$public_ip\u0026#34; ]]; do echo \u0026#34;Invalid input.\u0026#34; read -p \u0026#34;Public IPv4 address / hostname: \u0026#34; public_ip done [[ -z \u0026#34;$public_ip\u0026#34; ]] \u0026amp;\u0026amp; public_ip=\u0026#34;$get_public_ip\u0026#34; fi # If system has a single IPv6, it is selected automatically if [[ $(ip -6 addr | grep -c \u0026#39;inet6 [23]\u0026#39;) -eq 1 ]]; then ip6=$(ip -6 addr | grep \u0026#39;inet6 [23]\u0026#39; | cut -d \u0026#39;/\u0026#39; -f 1 | grep -oE \u0026#39;([0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}\u0026#39;) fi # If system has multiple IPv6, ask the user to select one if [[ $(ip -6 addr | grep -c \u0026#39;inet6 [23]\u0026#39;) -gt 1 ]]; then number_of_ip6=$(ip -6 addr | grep -c \u0026#39;inet6 [23]\u0026#39;) echo echo \u0026#34;Which IPv6 address should be used?\u0026#34; ip -6 addr | grep \u0026#39;inet6 [23]\u0026#39; | cut -d \u0026#39;/\u0026#39; -f 1 | grep -oE \u0026#39;([0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}\u0026#39; | nl -s \u0026#39;) \u0026#39; read -p \u0026#34;IPv6 address [1]: \u0026#34; ip6_number until [[ -z \u0026#34;$ip6_number\u0026#34; || \u0026#34;$ip6_number\u0026#34; =~ ^[0-9]+$ \u0026amp;\u0026amp; \u0026#34;$ip6_number\u0026#34; -le \u0026#34;$number_of_ip6\u0026#34; ]]; do echo \u0026#34;$ip6_number: invalid selection.\u0026#34; read -p \u0026#34;IPv6 address [1]: \u0026#34; ip6_number done [[ -z \u0026#34;$ip6_number\u0026#34; ]] \u0026amp;\u0026amp; ip6_number=\u0026#34;1\u0026#34; ip6=$(ip -6 addr | grep \u0026#39;inet6 [23]\u0026#39; | cut -d \u0026#39;/\u0026#39; -f 1 | grep -oE \u0026#39;([0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}\u0026#39; | sed -n \u0026#34;$ip6_number\u0026#34;p) fi echo echo \u0026#34;Which protocol should OpenVPN use?\u0026#34; echo \u0026#34; 1) UDP (recommended)\u0026#34; echo \u0026#34; 2) TCP\u0026#34; read -p \u0026#34;Protocol [1]: \u0026#34; protocol until [[ -z \u0026#34;$protocol\u0026#34; || \u0026#34;$protocol\u0026#34; =~ ^[12]$ ]]; do echo \u0026#34;$protocol: invalid selection.\u0026#34; read -p \u0026#34;Protocol [1]: \u0026#34; protocol done case \u0026#34;$protocol\u0026#34; in 1|\u0026#34;\u0026#34;) protocol=udp ;; 2) protocol=tcp ;; esac echo echo \u0026#34;What port should OpenVPN listen to?\u0026#34; read -p \u0026#34;Port [1194]: \u0026#34; port until [[ -z \u0026#34;$port\u0026#34; || \u0026#34;$port\u0026#34; =~ ^[0-9]+$ \u0026amp;\u0026amp; \u0026#34;$port\u0026#34; -le 65535 ]]; do echo \u0026#34;$port: invalid port.\u0026#34; read -p \u0026#34;Port [1194]: \u0026#34; port done [[ -z \u0026#34;$port\u0026#34; ]] \u0026amp;\u0026amp; port=\u0026#34;1194\u0026#34; echo echo \u0026#34;Select a DNS server for the clients:\u0026#34; echo \u0026#34; 1) Current system resolvers\u0026#34; echo \u0026#34; 2) Google\u0026#34; echo \u0026#34; 3) 1.1.1.1\u0026#34; echo \u0026#34; 4) OpenDNS\u0026#34; echo \u0026#34; 5) Quad9\u0026#34; echo \u0026#34; 6) AdGuard\u0026#34; read -p \u0026#34;DNS server [1]: \u0026#34; dns until [[ -z \u0026#34;$dns\u0026#34; || \u0026#34;$dns\u0026#34; =~ ^[1-6]$ ]]; do echo \u0026#34;$dns: invalid selection.\u0026#34; read -p \u0026#34;DNS server [1]: \u0026#34; dns done echo echo \u0026#34;Enter a name for the first client:\u0026#34; read -p \u0026#34;Name [client]: \u0026#34; unsanitized_client # Allow a limited set of characters to avoid conflicts client=$(sed \u0026#39;s/[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-]/_/g\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$unsanitized_client\u0026#34;) [[ -z \u0026#34;$client\u0026#34; ]] \u0026amp;\u0026amp; client=\u0026#34;client\u0026#34; echo echo \u0026#34;OpenVPN installation is ready to begin.\u0026#34; # Install a firewall in the rare case where one is not already available if ! systemctl is-active --quiet firewalld.service \u0026amp;\u0026amp; ! hash iptables 2\u0026gt;/dev/null; then if [[ \u0026#34;$os\u0026#34; == \u0026#34;centos\u0026#34; || \u0026#34;$os\u0026#34; == \u0026#34;fedora\u0026#34; ]]; then firewall=\u0026#34;firewalld\u0026#34; # We don\u0026#39;t want to silently enable firewalld, so we give a subtle warning # If the user continues, firewalld will be installed and enabled during setup echo \u0026#34;firewalld, which is required to manage routing tables, will also be installed.\u0026#34; elif [[ \u0026#34;$os\u0026#34; == \u0026#34;debian\u0026#34; || \u0026#34;$os\u0026#34; == \u0026#34;ubuntu\u0026#34; ]]; then # iptables is way less invasive than firewalld so no warning is given firewall=\u0026#34;iptables\u0026#34; fi fi read -n1 -r -p \u0026#34;Press any key to continue...\u0026#34; # If running inside a container, disable LimitNPROC to prevent conflicts if systemd-detect-virt -cq; then mkdir /etc/systemd/system/openvpn-server@server.service.d/ 2\u0026gt;/dev/null echo \u0026#34;[Service] LimitNPROC=infinity\u0026#34; \u0026gt; /etc/systemd/system/openvpn-server@server.service.d/disable-limitnproc.conf fi if [[ \u0026#34;$os\u0026#34; = \u0026#34;debian\u0026#34; || \u0026#34;$os\u0026#34; = \u0026#34;ubuntu\u0026#34; ]]; then apt-get update apt-get install -y openvpn openssl ca-certificates $firewall elif [[ \u0026#34;$os\u0026#34; = \u0026#34;centos\u0026#34; ]]; then yum install -y epel-release yum install -y openvpn openssl ca-certificates tar $firewall else # Else, OS must be Fedora dnf install -y openvpn openssl ca-certificates tar $firewall fi # If firewalld was just installed, enable it if [[ \u0026#34;$firewall\u0026#34; == \u0026#34;firewalld\u0026#34; ]]; then systemctl enable --now firewalld.service fi # Get easy-rsa easy_rsa_url=\u0026#39;https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.8/EasyRSA-3.0.8.tgz\u0026#39; mkdir -p /etc/openvpn/server/easy-rsa/ { wget -qO- \u0026#34;$easy_rsa_url\u0026#34; 2\u0026gt;/dev/null || curl -sL \u0026#34;$easy_rsa_url\u0026#34; ; } | tar xz -C /etc/openvpn/server/easy-rsa/ --strip-components 1 chown -R root:root /etc/openvpn/server/easy-rsa/ cd /etc/openvpn/server/easy-rsa/ # Create the PKI, set up the CA and the server and client certificates ./easyrsa init-pki ./easyrsa --batch build-ca nopass EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-server-full server nopass EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-client-full \u0026#34;$client\u0026#34; nopass EASYRSA_CRL_DAYS=3650 ./easyrsa gen-crl # Move the stuff we need cp pki/ca.crt pki/private/ca.key pki/issued/server.crt pki/private/server.key pki/crl.pem /etc/openvpn/server # CRL is read with each client connection, while OpenVPN is dropped to nobody chown nobody:\u0026#34;$group_name\u0026#34; /etc/openvpn/server/crl.pem # Without +x in the directory, OpenVPN can\u0026#39;t run a stat() on the CRL file chmod o+x /etc/openvpn/server/ # Generate key for tls-crypt openvpn --genkey --secret /etc/openvpn/server/tc.key # Create the DH parameters file using the predefined ffdhe2048 group echo \u0026#39;-----BEGIN DH PARAMETERS----- MIIBCAKCAQEA//////////+t+FRYortKmq/cViAnPTzx2LnFg84tNpWp4TZBFGQz +8yTnc4kmz75fS/jY2MMddj2gbICrsRhetPfHtXV/WVhJDP1H18GbtCFY2VVPe0a 87VXE15/V8k1mE8McODmi3fipona8+/och3xWKE2rec1MKzKT0g6eXq8CrGCsyT7 YdEIqUuyyOP7uWrat2DX9GgdT0Kj3jlN9K5W7edjcrsZCwenyO4KbXCeAvzhzffi 7MA0BM0oNC9hkXL+nOmFg/+OTxIy7vKBg8P+OxtMb61zO7X8vC7CIAXFjvGDfRaD ssbzSibBsu/6iGtCOGEoXJf//////////wIBAg== -----END DH PARAMETERS-----\u0026#39; \u0026gt; /etc/openvpn/server/dh.pem # Generate server.conf echo \u0026#34;local $ip port $port proto $protocol dev tun ca ca.crt cert server.crt key server.key dh dh.pem auth SHA512 tls-crypt tc.key topology subnet server 10.8.0.0 255.255.255.0\u0026#34; \u0026gt; /etc/openvpn/server/server.conf # IPv6 if [[ -z \u0026#34;$ip6\u0026#34; ]]; then echo \u0026#39;push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf else echo \u0026#39;server-ipv6 fddd:1194:1194:1194::/64\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf echo \u0026#39;push \u0026#34;redirect-gateway def1 ipv6 bypass-dhcp\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf fi echo \u0026#39;ifconfig-pool-persist ipp.txt\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf # DNS case \u0026#34;$dns\u0026#34; in 1|\u0026#34;\u0026#34;) # Locate the proper resolv.conf # Needed for systems running systemd-resolved if grep -q \u0026#39;^nameserver 127.0.0.53\u0026#39; \u0026#34;/etc/resolv.conf\u0026#34;; then resolv_conf=\u0026#34;/run/systemd/resolve/resolv.conf\u0026#34; else resolv_conf=\u0026#34;/etc/resolv.conf\u0026#34; fi # Obtain the resolvers from resolv.conf and use them for OpenVPN grep -v \u0026#39;^#\\|^;\u0026#39; \u0026#34;$resolv_conf\u0026#34; | grep \u0026#39;^nameserver\u0026#39; | grep -oE \u0026#39;[0-9]{1,3}(\\.[0-9]{1,3}){3}\u0026#39; | while read line; do echo \u0026#34;push \\\u0026#34;dhcp-option DNS $line\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf done ;; 2) echo \u0026#39;push \u0026#34;dhcp-option DNS 8.8.8.8\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf echo \u0026#39;push \u0026#34;dhcp-option DNS 8.8.4.4\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf ;; 3) echo \u0026#39;push \u0026#34;dhcp-option DNS 1.1.1.1\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf echo \u0026#39;push \u0026#34;dhcp-option DNS 1.0.0.1\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf ;; 4) echo \u0026#39;push \u0026#34;dhcp-option DNS 208.67.222.222\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf echo \u0026#39;push \u0026#34;dhcp-option DNS 208.67.220.220\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf ;; 5) echo \u0026#39;push \u0026#34;dhcp-option DNS 9.9.9.9\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf echo \u0026#39;push \u0026#34;dhcp-option DNS 149.112.112.112\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf ;; 6) echo \u0026#39;push \u0026#34;dhcp-option DNS 94.140.14.14\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf echo \u0026#39;push \u0026#34;dhcp-option DNS 94.140.15.15\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf ;; esac echo \u0026#34;keepalive 10 120 cipher AES-256-CBC user nobody group $group_name persist-key persist-tun verb 3 crl-verify crl.pem\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf if [[ \u0026#34;$protocol\u0026#34; = \u0026#34;udp\u0026#34; ]]; then echo \u0026#34;explicit-exit-notify\u0026#34; \u0026gt;\u0026gt; /etc/openvpn/server/server.conf fi # Enable net.ipv4.ip_forward for the system echo \u0026#39;net.ipv4.ip_forward=1\u0026#39; \u0026gt; /etc/sysctl.d/99-openvpn-forward.conf # Enable without waiting for a reboot or service restart echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward if [[ -n \u0026#34;$ip6\u0026#34; ]]; then # Enable net.ipv6.conf.all.forwarding for the system echo \u0026#34;net.ipv6.conf.all.forwarding=1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.d/99-openvpn-forward.conf # Enable without waiting for a reboot or service restart echo 1 \u0026gt; /proc/sys/net/ipv6/conf/all/forwarding fi if systemctl is-active --quiet firewalld.service; then # Using both permanent and not permanent rules to avoid a firewalld # reload. # We don\u0026#39;t use --add-service=openvpn because that would only work with # the default port and protocol. firewall-cmd --add-port=\u0026#34;$port\u0026#34;/\u0026#34;$protocol\u0026#34; firewall-cmd --zone=trusted --add-source=10.8.0.0/24 firewall-cmd --permanent --add-port=\u0026#34;$port\u0026#34;/\u0026#34;$protocol\u0026#34; firewall-cmd --permanent --zone=trusted --add-source=10.8.0.0/24 # Set NAT for the VPN subnet firewall-cmd --direct --add-rule ipv4 nat POSTROUTING 0 -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to \u0026#34;$ip\u0026#34; firewall-cmd --permanent --direct --add-rule ipv4 nat POSTROUTING 0 -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to \u0026#34;$ip\u0026#34; if [[ -n \u0026#34;$ip6\u0026#34; ]]; then firewall-cmd --zone=trusted --add-source=fddd:1194:1194:1194::/64 firewall-cmd --permanent --zone=trusted --add-source=fddd:1194:1194:1194::/64 firewall-cmd --direct --add-rule ipv6 nat POSTROUTING 0 -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to \u0026#34;$ip6\u0026#34; firewall-cmd --permanent --direct --add-rule ipv6 nat POSTROUTING 0 -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to \u0026#34;$ip6\u0026#34; fi else # Create a service to set up persistent iptables rules iptables_path=$(command -v iptables) ip6tables_path=$(command -v ip6tables) # nf_tables is not available as standard in OVZ kernels. So use iptables-legacy # if we are in OVZ, with a nf_tables backend and iptables-legacy is available. if [[ $(systemd-detect-virt) == \u0026#34;openvz\u0026#34; ]] \u0026amp;\u0026amp; readlink -f \u0026#34;$(command -v iptables)\u0026#34; | grep -q \u0026#34;nft\u0026#34; \u0026amp;\u0026amp; hash iptables-legacy 2\u0026gt;/dev/null; then iptables_path=$(command -v iptables-legacy) ip6tables_path=$(command -v ip6tables-legacy) fi echo \u0026#34;[Unit] Before=network.target [Service] Type=oneshot ExecStart=$iptables_path -t nat -A POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to $ip ExecStart=$iptables_path -I INPUT -p $protocol --dport $port -j ACCEPT ExecStart=$iptables_path -I FORWARD -s 10.8.0.0/24 -j ACCEPT ExecStart=$iptables_path -I FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT ExecStop=$iptables_path -t nat -D POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to $ip ExecStop=$iptables_path -D INPUT -p $protocol --dport $port -j ACCEPT ExecStop=$iptables_path -D FORWARD -s 10.8.0.0/24 -j ACCEPT ExecStop=$iptables_path -D FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\u0026#34; \u0026gt; /etc/systemd/system/openvpn-iptables.service if [[ -n \u0026#34;$ip6\u0026#34; ]]; then echo \u0026#34;ExecStart=$ip6tables_path -t nat -A POSTROUTING -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to $ip6 ExecStart=$ip6tables_path -I FORWARD -s fddd:1194:1194:1194::/64 -j ACCEPT ExecStart=$ip6tables_path -I FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT ExecStop=$ip6tables_path -t nat -D POSTROUTING -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to $ip6 ExecStop=$ip6tables_path -D FORWARD -s fddd:1194:1194:1194::/64 -j ACCEPT ExecStop=$ip6tables_path -D FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT\u0026#34; \u0026gt;\u0026gt; /etc/systemd/system/openvpn-iptables.service fi echo \u0026#34;RemainAfterExit=yes [Install] WantedBy=multi-user.target\u0026#34; \u0026gt;\u0026gt; /etc/systemd/system/openvpn-iptables.service systemctl enable --now openvpn-iptables.service fi # If SELinux is enabled and a custom port was selected, we need this if sestatus 2\u0026gt;/dev/null | grep \u0026#34;Current mode\u0026#34; | grep -q \u0026#34;enforcing\u0026#34; \u0026amp;\u0026amp; [[ \u0026#34;$port\u0026#34; != 1194 ]]; then # Install semanage if not already present if ! hash semanage 2\u0026gt;/dev/null; then if [[ \u0026#34;$os_version\u0026#34; -eq 7 ]]; then # Centos 7 yum install -y policycoreutils-python else # CentOS 8 or Fedora dnf install -y policycoreutils-python-utils fi fi semanage port -a -t openvpn_port_t -p \u0026#34;$protocol\u0026#34; \u0026#34;$port\u0026#34; fi # If the server is behind NAT, use the correct IP address [[ -n \u0026#34;$public_ip\u0026#34; ]] \u0026amp;\u0026amp; ip=\u0026#34;$public_ip\u0026#34; # client-common.txt is created so we have a template to add further users later echo \u0026#34;client dev tun proto $protocol remote $ip $port resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC ignore-unknown-option block-outside-dns block-outside-dns verb 3\u0026#34; \u0026gt; /etc/openvpn/server/client-common.txt # Enable and start the OpenVPN service systemctl enable --now openvpn-server@server.service # Generates the custom client.ovpn new_client echo echo \u0026#34;Finished!\u0026#34; echo echo \u0026#34;The client configuration is available in:\u0026#34; ~/\u0026#34;$client.ovpn\u0026#34; echo \u0026#34;New clients can be added by running this script again.\u0026#34; else clear echo \u0026#34;OpenVPN is already installed.\u0026#34; echo echo \u0026#34;Select an option:\u0026#34; echo \u0026#34; 1) Add a new client\u0026#34; echo \u0026#34; 2) Revoke an existing client\u0026#34; echo \u0026#34; 3) Remove OpenVPN\u0026#34; echo \u0026#34; 4) Exit\u0026#34; read -p \u0026#34;Option: \u0026#34; option until [[ \u0026#34;$option\u0026#34; =~ ^[1-4]$ ]]; do echo \u0026#34;$option: invalid selection.\u0026#34; read -p \u0026#34;Option: \u0026#34; option done case \u0026#34;$option\u0026#34; in 1) echo echo \u0026#34;Provide a name for the client:\u0026#34; read -p \u0026#34;Name: \u0026#34; unsanitized_client client=$(sed \u0026#39;s/[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-]/_/g\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$unsanitized_client\u0026#34;) while [[ -z \u0026#34;$client\u0026#34; || -e /etc/openvpn/server/easy-rsa/pki/issued/\u0026#34;$client\u0026#34;.crt ]]; do echo \u0026#34;$client: invalid name.\u0026#34; read -p \u0026#34;Name: \u0026#34; unsanitized_client client=$(sed \u0026#39;s/[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-]/_/g\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$unsanitized_client\u0026#34;) done cd /etc/openvpn/server/easy-rsa/ EASYRSA_CERT_EXPIRE=3650 ./easyrsa build-client-full \u0026#34;$client\u0026#34; nopass # Generates the custom client.ovpn new_client echo echo \u0026#34;$client added. Configuration available in:\u0026#34; ~/\u0026#34;$client.ovpn\u0026#34; exit ;; 2) # This option could be documented a bit better and maybe even be simplified # ...but what can I say, I want some sleep too number_of_clients=$(tail -n +2 /etc/openvpn/server/easy-rsa/pki/index.txt | grep -c \u0026#34;^V\u0026#34;) if [[ \u0026#34;$number_of_clients\u0026#34; = 0 ]]; then echo echo \u0026#34;There are no existing clients!\u0026#34; exit fi echo echo \u0026#34;Select the client to revoke:\u0026#34; tail -n +2 /etc/openvpn/server/easy-rsa/pki/index.txt | grep \u0026#34;^V\u0026#34; | cut -d \u0026#39;=\u0026#39; -f 2 | nl -s \u0026#39;) \u0026#39; read -p \u0026#34;Client: \u0026#34; client_number until [[ \u0026#34;$client_number\u0026#34; =~ ^[0-9]+$ \u0026amp;\u0026amp; \u0026#34;$client_number\u0026#34; -le \u0026#34;$number_of_clients\u0026#34; ]]; do echo \u0026#34;$client_number: invalid selection.\u0026#34; read -p \u0026#34;Client: \u0026#34; client_number done client=$(tail -n +2 /etc/openvpn/server/easy-rsa/pki/index.txt | grep \u0026#34;^V\u0026#34; | cut -d \u0026#39;=\u0026#39; -f 2 | sed -n \u0026#34;$client_number\u0026#34;p) echo read -p \u0026#34;Confirm $client revocation? [y/N]: \u0026#34; revoke until [[ \u0026#34;$revoke\u0026#34; =~ ^[yYnN]*$ ]]; do echo \u0026#34;$revoke: invalid selection.\u0026#34; read -p \u0026#34;Confirm $client revocation? [y/N]: \u0026#34; revoke done if [[ \u0026#34;$revoke\u0026#34; =~ ^[yY]$ ]]; then cd /etc/openvpn/server/easy-rsa/ ./easyrsa --batch revoke \u0026#34;$client\u0026#34; EASYRSA_CRL_DAYS=3650 ./easyrsa gen-crl rm -f /etc/openvpn/server/crl.pem cp /etc/openvpn/server/easy-rsa/pki/crl.pem /etc/openvpn/server/crl.pem # CRL is read with each client connection, when OpenVPN is dropped to nobody chown nobody:\u0026#34;$group_name\u0026#34; /etc/openvpn/server/crl.pem echo echo \u0026#34;$client revoked!\u0026#34; else echo echo \u0026#34;$client revocation aborted!\u0026#34; fi exit ;; 3) echo read -p \u0026#34;Confirm OpenVPN removal? [y/N]: \u0026#34; remove until [[ \u0026#34;$remove\u0026#34; =~ ^[yYnN]*$ ]]; do echo \u0026#34;$remove: invalid selection.\u0026#34; read -p \u0026#34;Confirm OpenVPN removal? [y/N]: \u0026#34; remove done if [[ \u0026#34;$remove\u0026#34; =~ ^[yY]$ ]]; then port=$(grep \u0026#39;^port \u0026#39; /etc/openvpn/server/server.conf | cut -d \u0026#34; \u0026#34; -f 2) protocol=$(grep \u0026#39;^proto \u0026#39; /etc/openvpn/server/server.conf | cut -d \u0026#34; \u0026#34; -f 2) if systemctl is-active --quiet firewalld.service; then ip=$(firewall-cmd --direct --get-rules ipv4 nat POSTROUTING | grep \u0026#39;\\-s 10.8.0.0/24 \u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;!\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39; -d 10.8.0.0/24\u0026#39; | grep -oE \u0026#39;[^ ]+$\u0026#39;) # Using both permanent and not permanent rules to avoid a firewalld reload. firewall-cmd --remove-port=\u0026#34;$port\u0026#34;/\u0026#34;$protocol\u0026#34; firewall-cmd --zone=trusted --remove-source=10.8.0.0/24 firewall-cmd --permanent --remove-port=\u0026#34;$port\u0026#34;/\u0026#34;$protocol\u0026#34; firewall-cmd --permanent --zone=trusted --remove-source=10.8.0.0/24 firewall-cmd --direct --remove-rule ipv4 nat POSTROUTING 0 -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to \u0026#34;$ip\u0026#34; firewall-cmd --permanent --direct --remove-rule ipv4 nat POSTROUTING 0 -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to \u0026#34;$ip\u0026#34; if grep -qs \u0026#34;server-ipv6\u0026#34; /etc/openvpn/server/server.conf; then ip6=$(firewall-cmd --direct --get-rules ipv6 nat POSTROUTING | grep \u0026#39;\\-s fddd:1194:1194:1194::/64 \u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;!\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39; -d fddd:1194:1194:1194::/64\u0026#39; | grep -oE \u0026#39;[^ ]+$\u0026#39;) firewall-cmd --zone=trusted --remove-source=fddd:1194:1194:1194::/64 firewall-cmd --permanent --zone=trusted --remove-source=fddd:1194:1194:1194::/64 firewall-cmd --direct --remove-rule ipv6 nat POSTROUTING 0 -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to \u0026#34;$ip6\u0026#34; firewall-cmd --permanent --direct --remove-rule ipv6 nat POSTROUTING 0 -s fddd:1194:1194:1194::/64 ! -d fddd:1194:1194:1194::/64 -j SNAT --to \u0026#34;$ip6\u0026#34; fi else systemctl disable --now openvpn-iptables.service rm -f /etc/systemd/system/openvpn-iptables.service fi if sestatus 2\u0026gt;/dev/null | grep \u0026#34;Current mode\u0026#34; | grep -q \u0026#34;enforcing\u0026#34; \u0026amp;\u0026amp; [[ \u0026#34;$port\u0026#34; != 1194 ]]; then semanage port -d -t openvpn_port_t -p \u0026#34;$protocol\u0026#34; \u0026#34;$port\u0026#34; fi systemctl disable --now openvpn-server@server.service rm -rf /etc/openvpn/server rm -f /etc/systemd/system/openvpn-server@server.service.d/disable-limitnproc.conf rm -f /etc/sysctl.d/99-openvpn-forward.conf if [[ \u0026#34;$os\u0026#34; = \u0026#34;debian\u0026#34; || \u0026#34;$os\u0026#34; = \u0026#34;ubuntu\u0026#34; ]]; then apt-get remove --purge -y openvpn else # Else, OS must be CentOS or Fedora yum remove -y openvpn fi echo echo \u0026#34;OpenVPN removed!\u0026#34; else echo echo \u0026#34;OpenVPN removal aborted!\u0026#34; fi exit ;; 4) exit ;; esac fi 安装使用 1 wget https://git.io/vpn -O openvpn-install.sh \u0026amp;\u0026amp; bash openvpn-install.sh 运行结束以后，就可以添加、删除用户了。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210913932516/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"OpenVPN 一键安装脚本，添加用户，删除用户，卸载服务。","id":43,"section":"posts","tags":["openvpn"],"title":"CentOS 7 OpenVPN 脚本","uri":"https://www.cnsre.cn/posts/210913932516/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210910030083/\n相关话题：https://www.cnsre.cn/tags/故障集/\n问题描述 基于docker使用jenkins 构建cicd，在执行docker build 的时候出现了权限的问题。具体报错如下\n1 2 3 4 5 6 + REPOSITORY=10.0.0.100/library/wenlong:master + cat + docker build -t 10.0.0.100/library/wenlong:master . Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker. sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargt=10.0.0.100%version=1: dial unix /var/run/docker.sock: connect: permission denied 原因 docker 进程使用 Unix Socket 而不是 TCP 端口。而默认情况下，Unix socket 属于 root 用户，需要 root 权限才能访问。\n这样的话 我们就需要用 root 去运行 docker 而在我们安装的时候就已经是 root 运行了，所以问题出现在 jenkins 身上。\n解决方法 修改jenkins 用户权限\n我是rpm安装的jenkins，所以你要找到你的jenkins配置文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@docker-jenkins ]# find / -name \u0026#34;jenkins\u0026#34; /run/lock/subsys/jenkins /etc/sysconfig/jenkins /etc/rc.d/init.d/jenkins /etc/logrotate.d/jenkins /var/lib/jenkins /var/log/jenkins /var/cache/jenkins /usr/lib/jenkins [root@docker-jenkins wenlong]# vim /etc/sysconfig/jenkins #修改jenkins用户为root ... JENKINS_USER=\u0026#34;root\u0026#34; ... 运行jenkins build 验证问题，已经解决。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210910030083/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"docker使用jenkins 构建cicd，在执行docker build 的时候出现了权限的问题","id":44,"section":"posts","tags":["jenkins","docker"],"title":"Jenkins 使用 Docker 构建报错","uri":"https://www.cnsre.cn/posts/210910030083/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210908023010/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\nPV 和 PVC 模式要先创建好 PV，然后再定义好 PVC 进行一对一的绑定。那么如果遇到大集群，也一一的创建吗？这样来说维护成本很高，工作量大。这个时候就有了 Kubernetes 提供一种自动创建 PV 的机制，叫 StorageClass ，它的作用就是创建 PV 的模板。\n** StorageClass 会定义两部分：**\nPV的属性：\n比如存储的大小、类型等 PV需要使用到的存储插件\n比如Ceph等； 有了这两部分信息，Kubernetes 就能够根据用户提交的 PVC ，找到对应的 StorageClass ，然后 Kubernetes 就会调用 StorageClass 声明的存储插件，自动创建 PV 。\n不过要使用 NFS ，我们就需要一个 nfs-client 的插件。这个插件会使 NFS 服务自动帮我们创建 PV 。\n自动创建的 PV 会以 ${namespace}-${pvcName}-${pvName} 的格式存储\n如果 PV 被回收，则会以 archieved-${namespace}-${pvcName}-${pvName} 的格式存储\n详细可以参考 Github\nPV、PVC、NFS不再介绍,没有完成的请查看 kubernetes使用PV和PVC管理数据存储\n创建ServiceAccount 创建 ServiceAccount 的目的是为了给 nfs-client 授权。\n1 2 # 下载 rbac.yaml wget https://github.com/kubernetes-retired/external-storage/blob/201f40d78a9d3fd57d8a441cfc326988d88f35ec/nfs-client/deploy/rbac.yaml 部署 rbac.yaml\n1 2 3 4 5 6 7 kubectl apply -f rbac.yaml # 输出如下 serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created 创建 nfs-client 使用 Deployment 来创建 nfs-client\n1 2 # 下载 deployment.yaml wget https://github.com/kubernetes-retired/external-storage/blob/201f40d78a9d3fd57d8a441cfc326988d88f35ec/nfs-client/deploy/deployment.yaml 修改 yaml 如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs # 这里的供应者名称必须和class.yaml中的provisioner的名称一致，否则部署不成功 - name: NFS_SERVER value: 10.0.10.51 # 这里写NFS服务器的IP地址或者能解析到的主机名 - name: NFS_PATH value: /home/bsh/nfs # 这里写NFS服务器中的共享挂载目录（强调：这里的路径必须是目录中最后一层的文件夹，否则部署的应用将无权限创建目录导致Pending） volumes: - name: nfs-client-root nfs: server: 10.0.10.51 # NFS服务器的IP或可解析到的主机名 path: /home/bsh/nfs # NFS服务器中的共享挂载目录（强调：这里的路径必须是目录中最后一层的文件夹，否则部署的应用将无权限创建目录导致Pending） ⚠️ 注意\nvalue: fuseim.pri/ifs # 这里的供应者名称必须和 class.yaml 中的 provisioner 的名称一致，否则部署不成功\n创建检查\n1 2 3 4 # 部署 nfs-client kubectl apply -f deployment.yaml # 输出如下 deployment.apps/nfs-client-provisioner created 查看pod\n1 2 3 4 kubectl get pod # 输出如下 NAME READY STATUS RESTARTS AGE nfs-client-provisioner-fd74f99b4-wr58j 1/1 Running 1 30s 创建 StorageClass class.yaml 内容比较少，可以不用下载,具体内容如下\nclass.yaml 下载地址\n1 2 3 4 5 6 7 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: fuseim.pri/ifs # or choose another name, must match deployment\u0026#39;s env PROVISIONER_NAME\u0026#39; parameters: archiveOnDelete: \u0026#34;false\u0026#34; ⚠️ 注意\nprovisioner 必须和上面得 Deployment 的 YAML 文件中 PROVISIONER_NAME 的值保持一致。 创建 storageclass\n1 2 3 4 # 创建 kubectl apply -f class.yaml # 输出如下 storageclass.storage.k8s.io/managed-nfs-storage created 查看状态\n1 2 3 4 kubectl get storageclass # 输出如下 NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-nfs-storage fuseim.pri/ifs Delete Immediate false 53s 创建 PVC 创建 tomcat-storageclass-pvc.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: tomcat annotations: volume.beta.kubernetes.io/storage-class: \u0026#34;managed-nfs-storage\u0026#34; spec: accessModes: - ReadWriteMany resources: requests: storage: 500Mi 部署 yaml\n1 2 3 kubectl apply -f tomcat-storageclass-pvc.yaml # 输出如下 persistentvolumeclaim/tomcat created 查看状态\n1 2 3 4 kubectl get pvc # 输出如下 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE tomcat Bound pvc-d35c82e3-29f3-4f6d-b25d-3ccdd365d1ec 500Mi RWX managed-nfs-storage 48s pod 使用添加 pvc 还拿之前的 tomcat 做实验，我们把 tomcat 目录下的 logs 拿到本地 nfs 中。\n⚠️ 注意\n如果遇到使用PVC 创建 pod 的时候发现无法创建成功。出现一下报错的时候请参考 kubernetes 使用 PCV 创建 pod 报错 persistentvolume-controller waiting for a volume to be created\n具体 yaml 如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment labels: app: tomcat spec: replicas: 3 selector: matchLabels: app: tomcat minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: wenlongxue/tomcat:tomcat-demo-62-123xw2 imagePullPolicy: Always ports: - containerPort: 8080 resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; limits: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 180 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 volumeMounts: - mountPath: \u0026#34;/usr/local/tomcat/logs\u0026#34; name: tomcat # pvc 部分 volumes: - name: tomcat persistentVolumeClaim: claimName: tomcat --- # Service 服务部分 apiVersion: v1 kind: Service metadata: name: tomcat-service labels: app: tomcat spec: selector: app: tomcat ports: - name: tomcat-port protocol: TCP port: 8080 targetPort: 8080 type: ClusterIP --- # ingress 服务部分 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tomcat annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: tls: - hosts: - tomcat.cnsre.cn secretName: tls-secret rules: - host: tomcat.cnsre.cn http: paths: - path: \u0026#34;/\u0026#34; pathType: Prefix backend: service: name: tomcat-service port: number: 8080 部署 pod 服务\n1 2 3 kubectl apply -f tomcatc.yaml # 输出如下 deployment.apps/tomcat-deployment created 查看状态\n1 2 3 4 5 6 7 kubectl get pod # 输出如下 NAME READY STATUS RESTARTS AGE nfs-client-provisioner-fd74f99b4-wr58j 1/1 Running 0 76m tomcat-deployment-7588b5c8fd-cnwvt 1/1 Running 0 59m tomcat-deployment-7588b5c8fd-kl8fj 1/1 Running 0 59m tomcat-deployment-7588b5c8fd-ksbg9 1/1 Running 0 59m 查看 PV PVC 1 2 3 4 5 6 [root@master tomccat]# kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-d35c82e3-29f3-4f6d-b25d-3ccdd365d1ec 500Mi RWX Delete Bound default/tomcat managed-nfs-storage 65m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/tomcat Bound pvc-d35c82e3-29f3-4f6d-b25d-3ccdd365d1ec 500Mi RWX managed-nfs-storage 65m 查看 nfs server 目录中信息 1 2 3 4 5 6 7 [root@node1 ~]# ll /home/bsh/nfs/default-tomcat-pvc-d35c82e3-29f3-4f6d-b25d-3ccdd365d1ec/ 总用量 220 -rw-r-----. 1 root root 22217 9月 3 14:49 catalina.2021-09-03.log -rw-r-----. 1 root root 0 9月 3 14:41 host-manager.2021-09-03.log -rw-r-----. 1 root root 2791 9月 3 14:49 localhost.2021-09-03.log -rw-r-----. 1 root root 118428 9月 3 15:31 localhost_access_log.2021-09-03.txt -rw-r-----. 1 root root 0 9月 3 14:41 manager.2021-09-03.log 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210908023010/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"Pod重启或者被删除的时候容器中的数据丢失？大集群频繁创建PV？StorageClass帮你管理数据存储。","id":45,"section":"posts","tags":["kubernetes","storageclass"],"title":"Kubernetes 持久化数据存储 StorageClass","uri":"https://www.cnsre.cn/posts/210908023010/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210906949577/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n故障表现 在使用PVC 创建 pod 的时候发现无法创建成功。\n问题排查过程 先通过看日志发现\n1 Normal ExternalProvisioning 8s (x3 over 19s) persistentvolume-controller waiting for a volume to be created, either by external provisioner \u0026#34;fuseim.pri/ifs\u0026#34; or manually created by system administrator 最后通过在网上查阅资料发现原来是 1.20 版本 默认禁止使用 selfLink 。我的版本为 v1.20.6\n参考链接 Guthub Issues\n问题解决 当前的解决方法是编辑 /etc/kubernetes/manifests/kube-apiserver.yaml\n在 kube-apiserver 下新增一行\n具体如下\n1 2 3 4 5 spec: containers: - command: - kube-apiserver - --feature-gates=RemoveSelfLink=false 然后更新即可。\n1 kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210906949577/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"使用pv pvc 创建 pod 时出现报错。 persistentvolume-controller  waiting for a volume to be created, either by external provisioner \"fuseim.pri/ifs\" or manually created by system administrator","id":46,"section":"posts","tags":["kubernetes"],"title":"kubernetes 使用 PCV 创建 pod 报错 persistentvolume-controller  waiting for a volume to be created","uri":"https://www.cnsre.cn/posts/210906949577/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210903021487/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。所以我会用 NFS 为例，创建 PV 、PVC.\nPV 属于集群中的资源。PVC 是对这些资源的请求，也作为对资源的请求的检查。 PV 和 PVC 之间的相互作用遵循这样的生命周期.\nPersistentVolume（PV） PersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。\nPV 有两种方式来配置：静态和动态。\n静态\n集群管理员创建一些 PV。它们带有可供群集用户使用的实际存储的细节。它们存在于 Kubernetes API 中，可用于消费。 动态\n根据 StorageClasses，当管理员创建的静态 PV 都不匹配用户的 PersistentVolumeClaim 时，集群可能会尝试动态地为 PVC 创建卷。 安装并配置 nfs rpcbind 1 2 3 4 yum install -y nfs-utils rpcbind mkdir -p /home/bsh/nfs vim /etc/exports /home/bsh/nfs *(rw,sync,no_root_squash) 配置详解： ro 只读访问 rw 读写访问 sync 所有数据在请求时写入共享 async NFS在写入数据前可以相应请求 secure NFS通过1024以下的安全TCP/IP端口发送 insecure NFS通过1024以上的端口发送 wdelay 如果多个用户要写入NFS目录，则归组写入（默认） no_wdelay 如果多个用户要写入NFS目录，则立即写入，当使用async时，无需此设置。 Hide 在NFS共享目录中不共享其子目录 no_hide 共享NFS目录的子目录 subtree_check 如果共享/usr/bin之类的子目录时，强制NFS检查父目录的权限（默认） no_subtree_check 和上面相对，不检查父目录权限 all_squash 共享文件的UID和GID映射匿名用户anonymous，适合公用目录。 no_all_squash 保留共享文件的UID和GID（默认） root_squash root用户的所有请求映射成如anonymous用户一样的权限（默认） no_root_squas root用户具有根目录的完全管理访问权限 anonuid=xxx 指定NFS服务器/etc/passwd文件中匿名用户的UID 启动 nfs rpcbind\n1 2 systemctl enable nfs rpcbind systemctl start nfs rpcbind 创建PV 创建 yaml 文件\nvim tomcat-log-pv.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolume metadata: name: tomcat spec: capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: path: /home/bsh/nfs/tomcat-log server: 10.0.10.51 创建 pv\n1 kubectl apply -f tomcat-log-pv.yaml 查看pv\n1 2 3 [root@master ]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE tomcat 1Gi RWX Retain Available 26s pv 属性详解 PV的存储容量\nPV 将具有特定的存储容量。这是使用PV的capacity属性设置的。\n目前，存储大小是可以设置或请求的唯一资源。未来的属性可能包括 IOPS、吞吐量等。\nPV的访问模式\nPersistentVolume可以以资源提供者支持的任何方式挂载到主机上。如下表所示，供应商具有不同的功能，每个 PV 的访问模式都将被设置为该卷支持的特定模式。例如，NFS 可以支持多个读/写客户端，但特定的 NFS PV 可能以只读方式导出到服务器上。每个 PV 都有一套自己的用来描述特定功能的访问模式。\n存储模式包括：\nReadWriteOnce——该卷可以被单个节点以读/写模式挂载 ReadOnlyMany——该卷可以被多个节点以只读模式挂载 ReadWriteMany——该卷可以被多个节点以读/写模式挂载\n在命令行中，访问模式缩写为： RWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany\n一个卷一次只能使用一种访问模式挂载，即使它支持很多访问模式。例如，GCEPersistentDisk 可以由单个节点作为 ReadWriteOnce 模式挂载，或由多个节点以 ReadOnlyMany 模式挂载，但不能同时挂载\nPV的回收策略\npersistentVolumeReclaimPolicy属性用来指定PV的回收策略 当前的回收策略包括：\nRetain（保留）——手动回收 Recycle（回收）——基本擦除（rm -rf /thevolume/*） Delete（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和 OpenStack Cinder 卷）将被删除\n当前，只有 NFS 和 HostPath 支持回收策略。AWS EBS、GCE PD、Azure Disk 和 Cinder 卷支持删除策略。 storageClassName PV 可以具有一个类，通过将 storageClassName 属性设置为 StorageClass 的名称来指定该类。一个特定类别的 PV 只能绑定到请求该类别的 PVC。没有 storageClassName 的 PV 就没有类，它只能绑定到不需要特定类的 PVC。\nPersistentVolumeClaim（PVC） PersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂载）。\n创建PVC 创建 yaml 文件\nvim tomcat-log-pvc.yaml\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: tomcat spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi 创建 pv\n1 kubectl apply -f tomcat-log-pvc.yaml 查看pv\n1 2 3 [root@master ]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE tomcat Bound tomcat 1Gi RWX 18s 使用PVC vim tomcat.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment labels: app: tomcat spec: replicas: 3 selector: matchLabels: app: tomcat minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: wenlongxue/tomcat:tomcat-demo-62-8fe6052 imagePullPolicy: Always ports: - containerPort: 8080 resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; limits: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 180 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 volumeMounts: - mountPath: \u0026#34;/usr/local/tomcat/logs\u0026#34; name: tomcat volumes: - name: tomcat persistentVolumeClaim: claimName: tomcat 部署查看 pod\n1 2 3 4 5 6 7 # 部署 kubectl apply -f tomcat.yaml # 查看 kubectl get pods |grep tomcat tomcat-deployment-7588b5c8fd-4grh2 1/1 Running 0 31s tomcat-deployment-7588b5c8fd-l89t7 1/1 Running 0 31s tomcat-deployment-7588b5c8fd-mb8bh 1/1 Running 0 31s 最后 PVC 不关心后端存储提供者是 NFS 还是 GFS，具体使用哪种类型的存储由 PV 来定义，PVC 只和隐藏了存储实现细节的 PV 对接。\n本方式为静态分配，如果有一千个 Pod，每个 Pod 有一个 PVC，那么管理员需要人工开设一千个 PV，随着集群规模的扩大，将导致无法有效管理。\nK8S 提供了一种可以动态分配的工作机制，可以自动创建 PV，该机制依赖一个叫做 StorageClass 的 API 对象。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210903021487/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"Pod重启或者被删除的时候，容器中的数据丢失？PV PVC 来帮你管理数据存储。","id":47,"section":"posts","tags":["kubernetes","pvc"],"title":"kubernetes 使用 PV 和 PVC 管理数据存储","uri":"https://www.cnsre.cn/posts/210903021487/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210902330007/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\ningress-nginx ingress 官方网站\ningress 仓库地址\ningress-nginx v1.0 最新版本 v1.0\n适用于 Kubernetes 版本 v1.19+ （包括 v1.19 ）\nKubernetes-v1.22+ 需要使用 ingress-nginx\u0026gt;=1.0，因为 networking.k8s.io/v1beta 已经移除\n直接部署 ingress-nginx 直接部署比较简单，直接拉去 girhub 的文件就可以了，如果遇到长时间无响应，可以终止任务从新拉取。\n拉取镜像部分，可以修改为一下的镜像地址\n1 2 3 4 5 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml sed -i \u0026#39;s@k8s.gcr.io/ingress-nginx/controller:v1.0.0\\(.*\\)@cnsre/ingress-nginx-controller:v1.0.0@\u0026#39; deploy.yaml sed -i \u0026#39;s@k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.0\\(.*\\)$@cnsre/ingress-nginx-kube-webhook-certgen:v1.0@\u0026#39; deploy.yaml kubectl apply -f deploy.yaml 检查安装 Completed 状态的是正常的，可以忽略。\n1 2 3 4 5 6 7 8 9 [root@master ~]# kubectl get po -n ingress-nginx NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-pm6sw 0/1 Completed 0 22m ingress-nginx-admission-patch-m8w94 0/1 Completed 0 22m ingress-nginx-controller-7d4df87d89-272ft 1/1 Running 0 22m [root@master ~]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.96.88.139 \u0026lt;none\u0026gt; 80:30497/TCP,443:32581/TCP 22m ingress-nginx-controller-admission ClusterIP 10.96.193.26 \u0026lt;none\u0026gt; 443/TCP 22m 创建应用yaml 1 vim tomcat.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment labels: app: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: wenlongxue/tomcat:tomcat-demo-62-8fe6052 imagePullPolicy: Always ports: - containerPort: 8080 resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; limits: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 180 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 --- apiVersion: v1 kind: Service metadata: name: tomcat-service labels: app: tomcat spec: selector: app: tomcat ports: - name: tomcat-port protocol: TCP port: 8080 targetPort: 8080 type: ClusterIP 部署 tomcat 应用\n1 kubectl apply -f tomcat.yaml 创建 ingress yaml 1 vim tomcat-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tomcat annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: tomcat.cnsre.cn http: paths: - path: \u0026#34;/\u0026#34; pathType: Prefix backend: service: name: tomcat-service port: number: 8080 部署 tomcat ingress yaml\n1 kubectl apply -f tomcat-ingress.yaml 查看 ingress 对应节点的端口\n1 2 3 4 kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.96.88.139 \u0026lt;none\u0026gt; 80:30497/TCP,443:32581/TCP 54m ingress-nginx-controller-admission ClusterIP 10.96.193.26 \u0026lt;none\u0026gt; 443/TCP 54m 添加 hosts 在 hosts 文件最后追加 ingress 节点的 IP 地址\n54.xxx.xxx.xxx tomcat.cnsre.cn 然后在浏览器中访问 tomcat.cnsre.cn:30497。\n使用 hostNetwork 的方式部署 ingress-nginx 每次部署 ingres-nginx 都随机一个 nodePort ，而使用 ingres-nginx 访问的时候也要以 域名:端口 的形式去访问如何直接使用域名去访问呢？下面介绍另外一种安装方式。\n1 2 3 4 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml sed -i \u0026#39;s@k8s.gcr.io/ingress-nginx/controller:v1.0.0\\(.*\\)@cnsre/ingress-nginx-controller:v1.0.0@\u0026#39; deploy.yaml sed -i \u0026#39;s@k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.0\\(.*\\)$@cnsre/ingress-nginx-kube-webhook-certgen:v1.0@\u0026#39; deploy.yaml 优化 ingress-nginx 使用 hostNetwork 默认 ingress-nginx 随机提供 nodeport 端口，开启 hostNetwork 启用80、443端口。\n修改 Deployment 下面的 spec\n参数如下：\n1 2 3 4 5 6 7 8 9 10 ... spec: hostNetwork: true # 新增 dnsPolicy: ClusterFirst containers: - name: controller image: cnsre/ingress-nginx-controller:v1.0.0 # 更换镜像地址 imagePullPolicy: IfNotPresent lifecycle: ... 修改负载均衡问题 把 kind: Deployment 改为 kind: DaemonSet 模式，这样每台 node 上都有 ingress-nginx-controller pod 副本。\n参数如下：\n1 2 3 4 5 6 7 8 9 ... # Source: ingress-nginx/templates/controller-deployment.yaml apiVersion: apps/v1 #kind: Deployment # 注释 kind: DaemonSet # 新增 metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 ... 修改 ingressClass 问题 如果不关心 ingressClass 或者很多没有 ingressClass 配置的 ingress 对象，\n添加参数 ingress-controller --watch-ingress-without-class=true 。\n1 2 3 4 5 6 7 8 9 10 11 ... args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --watch-ingress-without-class=true # 新增 ... 部署检查 ingress 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 部署 kubectl apply -f ingress-nginx.yaml # 检查 pod [root@master ~]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-admission-create-gmnmp 0/1 Completed 0 84m 10.100.219.105 master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-admission-patch-f5sgc 0/1 Completed 0 84m 10.100.219.106 master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-b62w7 1/1 Running 0 84m 10.0.10.51 master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-lsn7h 1/1 Running 0 84m 10.0.20.222 node1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 检查端口 [root@master ~]# netstat -pntl |grep 443 tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 31248/nginx: master [root@master ~]# netstat -pntl |grep 80 tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 31248/nginx: master 创建应用yaml 1 vim tomcat.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment labels: app: tomcat spec: replicas: 2 selector: matchLabels: app: tomcat minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: tomcat spec: containers: - name: tomcat image: wenlongxue/tomcat:tomcat-demo-62-8fe6052 imagePullPolicy: Always ports: - containerPort: 8080 resources: requests: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; limits: memory: \u0026#34;2Gi\u0026#34; cpu: \u0026#34;80m\u0026#34; readinessProbe: httpGet: path: / port: 8080 initialDelaySeconds: 180 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 --- apiVersion: v1 kind: Service metadata: name: tomcat-service labels: app: tomcat spec: selector: app: tomcat ports: - name: tomcat-port protocol: TCP port: 8080 targetPort: 8080 type: ClusterIP 部署 tomcat 应用\n1 kubectl apply -f tomcat.yaml 创建 ingress yaml 1 vim tomcat-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tomcat annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: rules: - host: tomcat.cnsre.cn http: paths: - path: \u0026#34;/\u0026#34; pathType: Prefix backend: service: name: tomcat-service port: number: 8080 部署 tomcat ingress yaml 1 kubectl apply -f tomcat-ingress.yaml 添加 hosts 在 hosts 文件最后追加 ingress 节点的 IP 地址\n54.xxx.xxx.xxx tomcat.cnsre.cn 然后在浏览器中访问 tomcat.cnsre.cn。\n给 ingress-nginx 配置 HTTPS 访问 创建自签证书文件\n1 openssl req -x509 -nodes -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginx/O=nginx\u0026#34; 创建后会生成两个文件\n1 2 3 ll tls.* -rw-r--r--. 1 root root 1127 9月 2 13:04 tls.crt -rw-r--r--. 1 root root 1708 9月 2 13:04 tls.key 创建 secret\n1 kubectl create secret tls tls-secret --key tls.key --cert tls.crt 修改 tomcat-ingress yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tomcat annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; spec: tls: # 新增 - hosts: # 新增 - tomcat.cnsre.cn # 新增 secretName: tls-secret # 新增 rules: - host: tomcat.cnsre.cn http: paths: - path: \u0026#34;/\u0026#34; pathType: Prefix backend: service: name: tomcat-service port: number: 8080 修改完重新部署下\n1 kubectl apply -f tomcat-ingress.yaml 验证证书 访问tomcat.cnsre.cn\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210902330007/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"本文介绍了 ingress controller v1.0 版本的安装，以及一些简单的优化修改等。","id":48,"section":"posts","tags":["kubernetes","ingress"],"title":"kubernetes 安装 ingress controller","uri":"https://www.cnsre.cn/posts/210902330007/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210824854115/\n相关话题：https://www.cnsre.cn/tags/zabbix/\nZabbix 5.0 LTS新增功能 新版本附带了可用性，安全性和完整性方面的重大改进列表。Zabbix团队遵循的主要策略是使Zabbix尽可能可用。Zabbix是一种开源，免费的监视解决方案，现在可以在内部和云中部署。在RedHat / IBM，SuSE，Ubuntu的最新版本的平台，容器和Linux发行版中可用。现在，一键式Zabbix部署也可以在Azure，AWS，Google Cloud，IBM / RedHat Cloud，Oracle和Digital Ocean上使用。现在，在Red Hat和Azure市场上提供Zabbix技术支持服务。\n此外，Zabbix监视工具还提供了与Messenger，票务和警报系统的大量现成集成。新版本扩展了可以轻松监控的受支持服务和应用程序的列表。\n自动化和发现：新的Zabbix版本具有改进的自动化功能。新版本增加了自动发现硬件组件，与Windows相关的资源以及Java度量的高级发现的功能。 可扩展性：Zabbix UI已经过优化，可以简化对数百万个设备的监视。 新的Zabbix监视代理程序具有“官方支持”状态。新的可扩展代理为最苛刻的客户和复杂的用例提供了高级功能。它基于插件体系结构，具有使用各种方法和技术收集度量标准数据的能力。我们相信它是市场上最先进的监控代理。 安全性方面的重大改进：新的改进确保所有Zabbix组件以安全的方式进行通信，并且还使用安全协议进行出站通信，而不会以任何方式影响性能。对于在高度敏感的环境中使用Zabbix的用户而言，可配置的密码以及为度量定义黑名单和白名单的能力至关重要。 TimescaleDB的压缩：时间序列数据压缩有助于提高性能和效率，同时降低运营成本。 可用性改进：新版本针对宽屏进行了优化，除了Zabbix UI的其他增强功能之外，还引入了对第三方UI模块的支持。\nZabbix 5.0是具有5年官方支持的LTS（长期支持）版本。它结合了创新和稳定性，并包括经过时间检验的功能，这些功能已在Zabbix 4.2和4.4的非LTS版本中引入，这使其成为大型企业环境的理想选择。 硬件配置需求 参考 zabbix 5.0 中文手册\n环境 平台 CPU/内存 数据库 硬盘 监控主机数 小型 centOS 2CPU/1GB MySQL、InnoDB 普通 100 中型 centOS 2CPU/2GB MySQL、InnoDB 普通 500 大型 Red HatEnterpirse Linux 4CPU/8GB MySQL、InnoDB 或PostgreSQL RAID 10 或 SSD 大于1000 超大型 Red HatEnterpirse Linux 8CPU/16GB MySQL、InnoDB 或PostgreSQL RAID 10 或 SSD 大于10000 前端软件需求 参考 zabbix 5.0 中文手册\nZabbix 前端需要使用下列软件:\n软件 版本 备注 Apache 1.3.12 或以上 PHP 5.4.0 或以上 PHP 扩展库：\n软件 版本 备注 gd 2.0 or later PHP GD 扩展库必须支持 PNG 图像(\u0026ndash;with-png-dir)、JPEG 图像 (\u0026ndash;with-jpeg-dir) 和 FreeType 2 (\u0026ndash;with-freetype-dir). bcmath php-bcmath (\u0026ndash;enable-bcmath) ctype php-ctype (\u0026ndash;enable-ctype) libXML 2.6.15 或以上 php-xml or php5-dom，如果发布者提供独立的部署包。 xmlreader php-xmlreader，如果发布者提供独立的部署包。 xmlwriter php-xmlwriter，如果发布者提供独立的部署包。 session php-session，如果发布者提供独立的部署包。 sockets php-net-socket (\u0026ndash;enable-sockets) 。用户脚本支持所需要的组件。 mbstring php-mbstring (\u0026ndash;enable-mbstring) gettext php-gettext (\u0026ndash;with-gettext)。用于多语言翻译支持。 ldap php-ldap。只有在前端使用 LDAP 认证时才需要。 ibm_db2 使用 IBM DB2 作为 Zabbix 后端数据库所需要的组件。 mysqli 使用 MySQL 作为 Zabbix 后端数据库所需要的组件。 oci8 使用 Oracle 作为 Zabbix 后端数据库所需要的组件。 pgsql 使用 PostgreSQL 作为 Zabbix 后端数据库所需要的组件。 环境准备 1 2 3 4 5 6 CentOS Linux release 7.9.2009 (Core) nginx 1.16.1 zabbix-server 5.0.14 zabbix-agent 5.0.14 MariaDB 5.5.68 PHP 7.4.22 关闭防火墙及selinux 1 2 systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config 修改阿里云yum源 参考链接\n1 2 3 4 5 6 7 8 9 # 备份 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup # 下载新的CentOS-Base.repo curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 添加EPEL wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo # 清理缓存并生成新的缓存 yum clean all yum makecache PHP 7.4 安装配置 添加源 1 2 yum install epel-release -y rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm 安装PHP 1 yum --enablerepo=remi install php74-php -y 安装你所需要php扩展模块 1 yum --disablerepo=\u0026#34;*\u0026#34; --enablerepo=\u0026#34;centos-sclo-rh\u0026#34; install rh-php72-php-gd rh-php72-php-bcmath rh-php72-php-mbstring rh-php72-php-mysqlnd rh-php72-php-xml rh-nginx116-nginx rh-php72 rh-php72-php-fpm rh-php72-php-ldap -y 安装其他依赖 安装zabbix报错，提示Requires: libiksemel.so.3()(64bit)，该文件为内核链接文件，无法绕过只有解决了这个依赖才可能继续安装，解决的方法为缺什么补什么，它要这个就给它。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 提示 需要：Requires: libiksemel.so.3()(64bit) wget http://springdale.math.ias.edu/data/puias/unsupported/7/x86_64//iksemel-1.4-6.sdl7.x86_64.rpm yum install iksemel-1.4-6.sdl7.x86_64.rpm -y # 提示 需要：libwebp.so.4()(64bit) wget http://vault.centos.org/7.9.2009/os/Source/SPackages/libwebp-0.3.0-7.el7.src.rpm yum install libwebp -y # 提示 需要：libjpeg.so.62()(64bit) wget http://vault.centos.org/7.9.2009/os/Source/SPackages/libjpeg-turbo-1.2.90-8.el7.src.rpm yum install libjpeg-turbo -y # 提示 需要：libXpm.so.4()(64bit) wget http://vault.centos.org/7.9.2009/os/Source/SPackages/libXpm-3.5.12-1.el7.src.rpm yum install libXpm -y # 提示 需要：gd wget http://vault.centos.org/7.9.2009/updates/Source/SPackages/gd-2.0.35-27.el7_9.src.rpm yum install gd -y 修改配置文件 1 2 3 4 5 6 sed -i \u0026#34;s#max_execution_time = 30#max_execution_time = 600#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#max_input_time = 60#max_input_time = 600#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#memory_limit = 128M#memory_limit = 256M#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#post_max_size = 8M#post_max_size = 32M#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#upload_max_filesize = 2M#upload_max_filesize = 16M#g\u0026#34; /etc/php.ini sed -i \u0026#34;s/;date.timezone =/date.timezone = Asia\\/Shanghai/g\u0026#34; /etc/php.ini 运行并查看版本， 重启命令， 添加自动启动，链接php文件 1 2 3 4 5 6 7 8 9 10 #运行并查看版本 php74 -v #重启命令php-fpm systemctl restart php74-php-fpm #添加自动启动 systemctl enable php74-php-fpm #查看php7.4的安装路径 whereis php #链接php文件 ln -s /opt/remi/php74/root/usr/bin/php /usr/bin/php 安装其他依赖 安装zabbix报错，提示Requires: libiksemel.so.3()(64bit)，该文件为内核链接文件，无法绕过只有解决了这个依赖才可能继续安装，解决的方法为缺什么补什么，它要这个就给它。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 提示 需要：Requires: libiksemel.so.3()(64bit) wget http://springdale.math.ias.edu/data/puias/unsupported/7/x86_64//iksemel-1.4-6.sdl7.x86_64.rpm yum install iksemel-1.4-6.sdl7.x86_64.rpm -y # 提示 需要：libwebp.so.4()(64bit) wget http://vault.centos.org/7.9.2009/os/Source/SPackages/libwebp-0.3.0-7.el7.src.rpm yum install libwebp -y # 提示 需要：libjpeg.so.62()(64bit) wget http://vault.centos.org/7.9.2009/os/Source/SPackages/libjpeg-turbo-1.2.90-8.el7.src.rpm yum install libjpeg-turbo -y # 提示 需要：libXpm.so.4()(64bit) wget http://vault.centos.org/7.9.2009/os/Source/SPackages/libXpm-3.5.12-1.el7.src.rpm yum install libXpm -y # 提示 需要：gd wget http://vault.centos.org/7.9.2009/updates/Source/SPackages/gd-2.0.35-27.el7_9.src.rpm yum install gd -y zabbix 服务安装 安装 Zabbix 存储库 1 rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm 安装 Zabbix 服务器和代理 1 yum install zabbix-server-mysql zabbix-agent -y 安装 zabbix 前端 1 2 yum -y install yum-utils yum-config-manager --enable rhel-server-rhscl-7-rpms -y 编辑配置文件 编辑配置文件 /etc/yum.repos.d/zabbix.repo 并启用Zabbix 前端存储库\n1 2 3 4 5 vi /etc/yum.repos.d/zabbix.repo [zabbix-frontend] ... enabled=1 ... 安装前端所需软件 1 yum install zabbix-web-mysql-scl zabbix-nginx-conf-scl -y 数据库安装配置 安装数据库 1 yum install -y mariadb-server mariadb 启动服务 1 systemctl start mariadb 设置服务开启自启动 1 systemctl enable mariadb 登录数据库 1 2 3 4 mysql -uroot -p 修改默认密码 mysql\u0026gt; SET PASSWORD = PASSWORD(\u0026#39;cnsre.cn\u0026#39;); #cnsre.cn是你的新密码 如何解决ERROR 1819 (HY000): Your password does not satisfy the current policy requirements呢？\n1 2 3 4 5 6 7 8 # 修改validate_password_policy参数的值 set global validate_password_policy=0; # 再修改密码的长度 set global validate_password_length=1; # 再次执行修改密码就可以了 ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;cnsre.cn\u0026#39;; # 允许root远程登陆 GRANT ALL PRIVILEGES ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;cnsre.cn\u0026#39; WITH GRANT OPTION; 创建初始数据库 在数据库主机上运行以下代码\n1 2 3 4 5 6 7 # mysql -uroot -p password mysql\u0026gt; create database zabbix character set utf8 collate utf8_bin; mysql\u0026gt; create user zabbix@localhost identified by \u0026#39;password\u0026#39;; mysql\u0026gt; grant all privileges on zabbix.* to zabbix@localhost; mysql\u0026gt; flush privileges; mysql\u0026gt; quit; 导入模板数据 导入初始架构和数据\n方法1\n1 zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uroot -p 方法2\n1 2 3 4 5 6 # 切换到 create.sql.gz 目录 我的目录如下 cd /usr/share/doc/zabbix-server-mysql-5.0.14 gzip -d create.sql.gz mysql -uroot -p mysql\u0026gt; use zabbix; mysql\u0026gt; source /usr/usr/share/doc/zabbix-server-mysql-5.0.14/create.sql 配置Zabbix-server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 mv /etc/zabbix/zabbix_server.conf /etc/zabbix/zabbix_server.conf.bak vim /etc/zabbix/zabbix_server.conf LogFile=/var/log/zabbix/zabbix_server.log LogFileSize=0 PidFile=/var/run/zabbix/zabbix_server.pid SocketDir=/var/run/zabbix DBHost=localhost DBName=zabbix DBUser=zabbix DBPassword=zabbix DBPort=3306 SNMPTrapperFile=/var/log/snmptrap/snmptrap.log CacheSize=1024M Timeout=4 AlertScriptsPath=/usr/lib/zabbix/alertscripts ExternalScripts=/usr/lib/zabbix/externalscripts LogSlowQueries=3000 配置zabbix-agent 1 2 3 4 5 6 7 8 9 10 11 mv /etc/zabbix/zabbix_agent.conf /etc/zabbix/zabbix_agent.conf.bak vim /etc/zabbix/zabbix_agentd.conf PidFile=/var/run/zabbix/zabbix_agentd.pid LogFile=/var/log/zabbix/zabbix_agentd.log LogFileSize=0 Server= ListenPort=10050 ServerActive= Hostname= Include=/etc/zabbix/zabbix_agentd.d/ nginx 安装配置 为 Zabbix 前端配置 PHP 编辑文件 /etc/opt/rh/rh-nginx116/nginx/conf.d/zabbix.conf，取消注释和设置 listen 和 server_name 指令。\n# listen 80; # server_name example.com; 编辑文件 /etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf ，将 nginx 添加到 listen.acl_users 指令。\nlisten.acl_users = apache,nginx 然后取消注释并为设置正确的时区。\nphp_value[date.timezone] = Asia/Shanghai 启动所有服务 1 2 3 systemctl restart zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm systemctl enable zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm systemctl status zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm 检查端口\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@localhost ~]# netstat -pntl Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 18862/nginx: master tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 968/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1258/master tcp 0 0 0.0.0.0:10050 0.0.0.0:* LISTEN 18816/zabbix_agentd tcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTEN 18813/php-fpm: mast tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 18700/mysqld tcp6 0 0 :::80 :::* LISTEN 18862/nginx: master tcp6 0 0 :::22 :::* LISTEN 968/sshd tcp6 0 0 ::1:25 :::* LISTEN 1258/master tcp6 0 0 :::10050 :::* LISTEN 18816/zabbix_agentd 访问配置 zabbix 前端 连接到新安装的Zabbix前端： http://server_ip\n⚠️ 如果打开页面访问不到 zabbix 页面，将 /etc/opt/rh/rh-nginx116/nginx/nginx.conf 配置文件中的 server 模块注释掉重启即可\n默认的用户 Admin/zabbix\n好了，到这里，我们就已经完成了Zabbix 5.0 LTS 的安装,快去体验吧。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210824854115/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix LNMP Zabbix 5.0是具有5年官方支持的LTS（长期支持）版本，本文将介绍zabbix 5.0 长期支持版的 LNMP 搭建部署。","id":49,"section":"posts","tags":["zabbix","lnmp"],"title":"LNMP 方式部署 zabbix 5.0","uri":"https://www.cnsre.cn/posts/210824854115/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210823049577/\n相关话题：https://www.cnsre.cn/tags/故障集/\n故障表现 在使用 jumperver 登录 AWS ec2 实例的时候发现 ssh 配合秘钥登录的时候无法登录，\n具体报错如下：\nssh -i /path/xx.pem user@10.0.11.190 Permission denied (publickey,gssapi-keyex,gssapi-with-mic). 问题排查过程 在发现无法登录的第一时间等了AWS 平台查看底层监控是否正常\n查看到底层硬件工作正常，并没有观察到异常报错。\n通过查看业务服务，发现业务服务并没有收到影响。\n那就说明，服务器是没有问题的，只是登录认证出了问题。既然服务没有问题，接下来就慢慢排查就，就不着急了。\n接着，尝试用 aws 的 ssm (Amazon Systems Manager)尝试登录，发现能够使用 ssm 登录。\n再次回到跳板机，运行 telnet 10.0.11.190 22 端口是通的。排除网络端口问题。\n查看 ssh 登录日志\n1 ssh -i /path/xx.pem user@10.0.11.190 -vvv 查看 secure 日志\n1 tail -f /var/log/secure 问题解决 经过查看日志，总结如下:\n1 当前是从跳板机，以ssh的方式连接到故障主机，但是在连接过程中遇到如下所示报错：\n1 Permission denied (publickey,gssapi-keyex,gssapi-with-mic). 从ssh -vvv的debug日志来看，ssh client端发送了认证请求，但是ssh server端并没有完成认证过程，导致permission denied报错产生。\n2 故障主机配置了SSM agent，并且可以通过session manager打开。\n在这个基础上，在实例的/var/log/secure文件中看到如下报错内容：\n1 authentication refused: bad ownership or modes for directory /home/ec2-user/ 这个报错的意思是说，/home/ec2-user/ 目录的owner或者mode存在一些问题。\n经过查看，/home/ec2-user/ 目录配置的是777的权限，进而导致的认证失败。\n将其修改为700后，问题得到解决，可以ssh登录到故障主机。\n为什么会有777的权限呢？ 为何会将 /home/ec2-user/ 目录下所有内容修改为 777 呢？\n经过登录 Jumoserver 的审计发现，一名开发人员将 /home/ec2-user/ 权限改为了 777 原因是通过 Jumpserver 上传文件的时候没有权限，然后开发就自己将目录给了 777的权限。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210823049577/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"ssh 秘钥登录失败，报错 Permission denied ??","id":50,"section":"posts","tags":["aws"],"title":"AWS EC2 实例 SSH 无法登录故障","uri":"https://www.cnsre.cn/posts/210823049577/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210819955387/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n查看节点状态失败 1 Error from server (NotFound): the server could not find the requested resource (get services http:heapster:) 原因分析：没有heapster服务。 解决方法：安装promethus监控组件即可。\nK8S 集群服务访问失败 案例 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 curl: (60) Peer\u0026#39;s Certificate issuer is not recognized. More details here: http://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \u0026#34;bundle\u0026#34; of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn\u0026#39;t adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If you\u0026#39;d like to turn off curl\u0026#39;s verification of the certificate, use the -k (or --insecure) option. 原因分析：证书不能被识别，其原因为：自定义证书，过期等。 解决方法：更新证书即可。 案例 2 1 curl: (7) Failed connect to 10.103.22.158:3000; Connection refused 原因分析：端口映射错误，服务正常工作，但不能提供服务。 解决方法：删除svc，重新映射端口即可。 1 kubectl delete svc nginx-deployment K8S 集群服务暴露失败 1 Error from server (AlreadyExists): services \u0026#34;nginx-deployment\u0026#34; already exists 原因分析：该容器已暴露服务了。 解决方法：删除svc，重新映射端口即可。 外网无法访问 K8S 集群提供的服务 原因分析：K8S集群的type为ClusterIP，未将服务暴露至外网。\n解决方法：修改K8S集群的type为NodePort即可，于是可通过所有K8S集群节点访问服务。\npod 创建失败 案例 1 1 2 3 4 5 6 7 8 9 10 11 readiness-httpget-pod 0/1 Pending 0 0s readiness-httpget-pod 0/1 Pending 0 0s readiness-httpget-pod 0/1 ContainerCreating 0 0s readiness-httpget-pod 0/1 Error 0 2s readiness-httpget-pod 0/1 Error 1 3s readiness-httpget-pod 0/1 CrashLoopBackOff 1 4s readiness-httpget-pod 0/1 Error 2 15s readiness-httpget-pod 0/1 CrashLoopBackOff 2 26s readiness-httpget-pod 0/1 Error 3 37s readiness-httpget-pod 0/1 CrashLoopBackOff 3 52s readiness-httpget-pod 0/1 Error 4 82s 原因分析：镜像问题导致容器无法启动。\n解决方法：更换镜像。\n案例 2 原因分析：yml文件内容出错—-使用中文字符； 解决方法：修改myregistrykey内容即可。\npod 的 ready 状态未进入 1 readiness-httpget-pod 0/1 Running 0 116s 原因分析：POD的执行命令失败，无法获取资源。\n解决方法：进入容器内部，创建yaml定义的资源\npod 状态为 ErrImagePull 1 rnginx-deployment 0/1 ErrImagePull 0 10s 原因分析：image无法拉取 解决方法：更换镜像即可。 pod 一直处于 pending 状态 原因分析：由于已使用同样镜像发布了pod，导致无节点可调度。\n解决方法：删除所有pod后部署pod即可。\n探测存活 pod 状态为 CrashLoopBackOff 原因分析：镜像问题，导致容器重启失败。\n解决方法：更换镜像即可。\n创建服务 status 为 ErrImagePull 排查思路：\n1 kubectl describe pod test-nginx 原因分析：拉取镜像名称问题。 解决方法：删除错误pod；重新拉取镜像； 1 kubectl delete pod test-nginx;kubectl run test-nginx --image=10.0.0.81:5000/nginx:alpine 创建 init C容器后，其状态不正常 1 2 NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 20s 原因分析：查看日志发现，pod一直出于初始化中；然后查看pod详细信息，定位pod创建失败的原因为：初始化容器未执行完毕。 1 Error from server (BadRequest): container \u0026#34;myapp-container\u0026#34; in pod \u0026#34;myapp-pod\u0026#34; is waiting to start: PodInitializing 1 2 3 4 5 6 7 8 9 10 11 12 waiting for myservice Server: 10.96.0.10 Address: 10.96.0.10:53 ** server can\u0026#39;t find myservice.default.svc.cluster.local: NXDOMAIN *** Can\u0026#39;t find myservice.svc.cluster.local: No answer *** Can\u0026#39;t find myservice.cluster.local: No answer *** Can\u0026#39;t find myservice.default.svc.cluster.local: No answer *** Can\u0026#39;t find myservice.svc.cluster.local: No answer *** Can\u0026#39;t find myservice.cluster.local: No answer 解决方法：创建相关service，将SVC的name写入K8S集群的coreDNS服务器中，于是coreDNS就能对POD的initC容器执行过程中的域名解析了。 1 kubectl apply -f myservice.yaml 阿\n1 2 3 4 5 NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:1/2 0 27m myapp-pod 0/1 PodInitializing 0 28m myapp-pod 1/1 Running 0 28m 创建 PV 失败 原因分析：pv的name字段重复。\n解决方法：修改pv的name字段即可。\npod 无法挂载 PVC 原因分析：pod无法挂载PVC。accessModes与可使用的PV不一致，导致无法挂载PVC，由于只能挂载大于1G且accessModes为RWO的PV，故只能成功创建1个pod，第2个pod一致pending，按序创建时则第3个pod一直未被创建；\n解决方法：修改yml文件中accessModes或PV的accessModes即可。 pod 使用 PV 后，无法访问其内容 原因分析：nfs卷中没有文件或权限不对。\n解决方法：在nfs卷中创建文件并授予权限。\nkube-flannel-ds-amd64-ndsf7 插件 pod 的 status 为 Init:0/1 排查思路：kubectl -n kube-system describe pod kube-flannel-ds-amd64-ndsf7 #查询pod描述信息；\n原因分析：k8s-slave1节点拉取镜像失败。 解决方法：登录k8s-slave1，重启docker服务，手动拉取镜像。\nk8s-master节点，重新安装插件即可。 1 kubectl create -f kube-flannel.yml;kubectl get nodes 不能进入指定容器内部 原因分析：yml文件comtainers字段重复，导致该pod没有该容器。 解决方法：去掉yml文件中多余的containers字段，重新生成pod。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210819955387/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"收集了一些常见的 kubernetes 常见的故障，以及解决 k8s 常见故障的一些方法。","id":51,"section":"posts","tags":["kubernetes"],"title":"kubernetes 常见故障以及解决方法","uri":"https://www.cnsre.cn/posts/210819955387/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210816936340/\n相关话题：https://www.cnsre.cn/tags/nginx/\nlocation 路径匹配 匹配规则 location 路径正则匹配：\n符号 说明 ~ 正则匹配，区分大小写 ~* 正则匹配，不区分大小写 ^~ 普通字符匹配，如果该选项匹配，则，只匹配改选项，不再向下匹配其他选项 = 普通字符匹配，精确匹配 @ 定义一个命名的 location，用于内部定向，例如 error_page，try_files 匹配优先级 路径匹配，优先级\n精确匹配：\n=前缀的指令严格匹配这个查询。\n如果找到，停止搜索。 普通字符匹配：\n所有剩下的常规字符串，最长的匹配。\n如果这个匹配使用^〜前缀，搜索停止。 正则匹配：\n正则表达式，在配置文件中定义的顺序，匹配到一个结果，搜索停止； 默认匹配：\n如果第3条规则产生匹配的话，结果被使用。\n否则，如同从第2条规则被使用。 举例 通过一个实例，简单说明一下匹配优先级：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 location = / { # 精确匹配 / ，主机名后面不能带任何字符串 [ configuration A ] } location / { # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求 # 但是正则和最长字符串会优先匹配 [ configuration B ] } location /documents/ { # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索 # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条 [ configuration C ] } location ~ /documents/Abc { # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索 # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条 [ configuration CC ] } location ^~ /images/ { # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。 [ configuration D ] } location ~* \\.(gif|jpg|jpeg)$ { # 匹配所有以 gif,jpg或jpeg 结尾的请求 # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则 [ configuration E ] } location /images/ { # 字符匹配到 /images/，继续往下，会发现 ^~ 存在 [ configuration F ] } location /images/abc { # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在 # F与G的放置顺序是没有关系的 [ configuration G ] } location ~ /images/abc/ { # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用 [ configuration H ] } location ~* /js/.*/\\.js 按照上面的location写法，以下的匹配示例成立：\n/ -\u0026gt; config A： 精确完全匹配，即使/index.html也匹配不了\n/downloads/download.html -\u0026gt; config B： 匹配B以后，往下没有任何匹配，采用B\n/images/1.gif -\u0026gt; configuration D： 匹配到F，往下匹配到D，停止往下\n/images/abc/def -\u0026gt; config D： 最长匹配到G，往下匹配D，停止往下你可以看到 任何以/images/开头的都会匹配到D并停止，FG写在这里是没有任何意义的，H是永远轮不到的，这里只是为了说明匹配顺序\n/documents/document.html -\u0026gt; config C： 匹配到C，往下没有任何匹配，采用C\n/documents/1.jpg -\u0026gt; configuration E： 匹配到C，往下正则匹配到E\n/documents/Abc.jpg -\u0026gt; config CC： 最长匹配到C，往下正则顺序匹配到CC，不会往下到E\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210816936340/\n相关话题：https://www.cnsre.cn/tags/nginx/\n","description":"nginx 反向代理，路径映射的过程是什么？如何配置路径映射规则？","id":52,"section":"posts","tags":["nginx"],"title":"Nginx Location 路径匹配","uri":"https://www.cnsre.cn/posts/210816936340/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210811113507/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n日志中能透露的信息很多，从日志中，可以知道程序的运行状态，可以知道系统是否正常等，但是对于日志的监控却很让人头疼，要是能监控日志的变化情况，就可以及时的知道系统发生了什么，从而做出相应的对策。对于日志监控，zabbix有话说，zabbix从较早的版本就有了日志监控的功能。\n日志监控功能 zabbix-agent支持日志文件的监控，可以对日志文件关键字进行监控，然后告警，日志监控支持普通的日志文件，支持日志轮询，切割的文件。当日志文件中出现特殊的字符串（告警，报错的字符串）可以发通知给客户\n日志监控必须满足以下条件：\nzabbix-agent必须运行，且工作方式必须是主动模式 日志的Item必须设置，必须指定文件名 zabbix-agent有读取日志的权限 日志监控的监控指标 1 2 3 4 5 6 7 log[/path/to/file/file_name,,,,,,,] logrt[path/to/file/regexpo_describing_filename_pattern,,,,,,,] log.count[/path/to/file/file_name,,,,,,] logrt.cunt[path/to/file/regexpo_describing_filename_pattern,,,,,,] 参数 含义 file_name 日志文件所在的路径或者绝对路径名 regexp 匹配正则表达式 encoding 在Linux/unix系统下默认编码为UTF-8，在Windows系统下默认编码为ANSI maxlines 每次给zabbix-server或者zabbix-Proxy发送的日志的最大行数，此参数会高于zabbix-agent.conf中的MaxLinesPerSecond参数值，通过此参数，可以控制一次发送的日志的数据条数，如果发送次数过多，可能会对zabbix-agnet的负载和I/O有很大的影响 mode all为默认参数，表示匹配所有的日志，包括以前存的日志也会进行匹配 skip 表示跳过已存在的日志数据，只有新的日志才会进行匹配 output 表示匹配输出的正则表达式，1~9表示返回的匹配的第几个字符串，表示返回匹配的全部字符串 maxdelay 以秒为单位的最大延迟，用用于忽略老的日志数据，及时获取获取当前的日志数据。（4.0+）当处理日志过多，在更新周期内达到maxlines的发送上限，但还有日志未发送时，会导致大量堆积，在严重的情况下，会造成日志处理速度跟不上，使用此参数忽略过期的日志发送0是默认值，永远不会忽略日志文件行输入可以是浮点数（float）\u0026gt;0.0,忽略较旧的行，以获得在maxdelay秒内分析最新行，会丢弃在规定时间内的无法发送的数据 options 日志轮询、切割方式（4.0+）rotate，日志轮询、切割，默认值copytruncate，先拷贝文件，然后清空日志的轮询方式，copytruncate不能与maxdelay一起使用，如使用此参数，maxdelay必须为0或者未指定[size=12.0000pt] 例：监控zabbix_server的日志\n创建监控项，选择zabbix_server客户端（主动式）\n这四个就是日志监控的指标，根据需求选择合适的键值\n我的zabbix_server的日志路径事/usr/local/zabbix/logs/zabbix_server,后面跟的正则表达式事error，只有当出现error字段时，才会有数据，后面的参数可以不写，但是还是要写逗号的。信息类型选择文本或者是字符\n这样这个日志监控的监控项就已经做好了\n接下来就要做的就是做触发器了\n触发器中监控项选择刚才创建的监控项\n功能选择diff()，结果选择等于1\n迭代选择无，这样再次匹配的话，告警就不会恢复\n测试，往zabbix_server.log文件内写入error，测试是否会报警\n如果zabbix用户对日志没有读取权限，则会提示权限拒绝导致数据获取失败 对于不方便设置权限的日志文件，可以使zabbix_agent采用root权限运行\n在zabbix_agentd.conf文件中设置AllowRoot参数设置为1\nAllowRoot=1\n日志监控的数据库记录 日志数据存储在history_log表中\n查询日志\nmysql\u0026gt;select * from history_log; 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210811113507/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix-agent对日志文件的监控，可以对日志文件关键字进行监控，然后告警","id":53,"section":"posts","tags":["zabbix","日志"],"title":"Zabbix 监控日志","uri":"https://www.cnsre.cn/posts/210811113507/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210810958573/\n相关话题：https://www.cnsre.cn/tags/nginx/\nNginx 是用于 Web 服务、反向代理、缓存、负载平衡、媒体流等的开源软件。在这将提到一些经常使用的 Nginx 经典配置以及安全性的一些配置。请根据您的实际需求对这些配置进行调整。\n侦听端口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 server { # 标准HTTP协议 listen 80; # 标准HTTPS协议 listen 443 ssl; # 使用 http2 listen 443 ssl http2; # 使用IPv6 监听 80 listen [::]:80; # 仅限使用IPv6 listen [::]:80 ipv6only=on; } 访问日志 1 2 3 4 5 6 7 server { # 日志文件的相对或完整路径 access_log /path/to/file.log; # 选择 \u0026#39;on\u0026#39; 或者 \u0026#39;off\u0026#39; access_log on; } 域名 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 server { # 监听单个域名 server_name cnsre.cn; # 监听多个域名 server_name cnsre.cn www.cnsre.cn; # 监听所有域名 server_name *.cnsre.cn; # 监听所有顶级域名 server_name cnsre.*; # 监听未指定的主机名（侦听IP地址本身） server_name \u0026#34;\u0026#34;; } 静态资源 1 2 3 4 5 6 7 8 server { listen 80; server_name cnsre.cn; location / { root /path/to/website; } } 重定向 1 2 3 4 5 6 7 8 9 10 11 12 13 14 server { listen 80; server_name www.cnsre.cn; return 301 http://cnsre.cn$request_uri; } server { listen 80; server_name www.cnsre.cn; location /redirect-url { return 301 http://otherdomain.com; } } 反向代理 1 2 3 4 5 6 7 8 9 10 server { listen 80; server_name cnsre.cn; location / { proxy_pass http://0.0.0.0:3000; # 其中 0.0.0.0:3000 是您的应用程序服务器（例如：node.js）绑定在 0.0.0.0 上，监听端口 3000 } } 负载均衡 1 2 3 4 5 6 7 8 9 10 11 12 13 14 upstream node_js { server 0.0.0.0:3000; server 0.0.0.0:4000; server 1.1.1.1; } server { listen 80; server_name cnsre.cn; location / { proxy_pass http://www.cnsre.cn; } } SSL 协议 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 server { listen 443 ssl; server_name cnsre.cn; ssl on; ssl_certificate /path/to/cert.pem; ssl_certificate_key /path/to/privatekey.pem; ssl_stapling on; ssl_stapling_verify on; ssl_trusted_certificate /path/to/fullchain.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_session_timeout 1h; ssl_session_cache shared:SSL:50m; add_header Strict-Transport-Security max-age=15768000; } # HTTP 到 HTTPS 的永久重定向 server { listen 80; server_name cnsre.cn; return 301 https://$host$request_uri; } 禁止任何敏感的请求路径 1 2 3 4 5 6 7 8 9 location ~ /\\.git { deny all; } ## Disable .htaccess and other hidden files location ~ /\\.(?!well-known).* { deny all; access_log off; log_not_found off; } 禁止不必要的 HTTP 请求方法 最常用的 HTTP 请求方法是 GET、POST、HEAD。对于任何其他未使用的方法，我们应该返回 444。\n1 2 3 4 5 #只允许这些请求方法 if ($request_method !~ ^(GET|HEAD|POST)$ ) { return 444; } # 不接受删除，搜索和其他方法 添加请求速率限制 限速 会拦截很多恶意请求，也是防御网站的网络级和应用级DDoS攻击的常用工具。我们可以为单个 IP 添加最大请求限制。\n1 2 3 4 5 6 7 8 9 limit_req_zone $binary_remote_addr zone=ip:10m rate=5r/s; server { listen 80; location / { limit_req zone=ip burst=12 delay=8; proxy_pass http://cnsre.cn; } } 点击劫持攻击 点击劫持攻击 会导致用户在不知不觉中下载恶意软件、访问恶意网页、提供凭据或敏感信息。\n我们可以X-FRAME-OPTIONS在 HTTP Header 中注入以防止点击劫持攻击（甚至可以通过某些方式绕过）。这是通过在 nginx.conf 文件中添加以下内容来实现的\n1 add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34;; X-XSS 保护 注入具有 X-XSS 保护的 HTTP 标头以减轻跨站点脚本攻击。修改 nginx.conf 文件添加以下内容\n1 add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; 如果你还对安全标头感兴趣，点击这里了解这些标头。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210810958573/\n相关话题：https://www.cnsre.cn/tags/nginx/\n","description":"Nginx 是用于 Web 服务、反向代理、缓存、负载平衡、媒体流等的开源软件。在这将提到一些经常使用的 Nginx 经典配置以及安全性的一些配置。请根据您的实际需求对这些配置进行调整。","id":54,"section":"posts","tags":["nginx"],"title":"Nginx 常用配置以及安全配置案例","uri":"https://www.cnsre.cn/posts/210810958573/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210624108255/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n安装 go 语言环境 Golang 官网下载地址：golang官网\n打开官网下载地址选择对应的系统版本, 复制下载链接\n这里我选择的是\ngo1.16.5.linux-amd64.tar.gz\n下载解压 下载安装包\n1 wget https://dl.google.com/go/go1.16.5.linux-amd64.tar.gz 解压到/usr/loacl目录下\n1 tar -C /usr/local -zxvf go1.16.5.linux-amd64.tar.gz 添加环境变量 添加/usr/loacl/go/bin 目录到 PATH 变量中。添加到 /etc/profile\n1 2 3 4 5 6 vim /etc/profile # 在最后一行添加 export GOROOT=/usr/local/go export PATH=$PATH:$GOROOT/bin # 保存退出后source一下 source /etc/profile 验证 执行go version，如果现实版本号，则Go环境安装成功。\n1 2 [root@master ~]# go version go version go1.16.5 linux/amd64 查看当前的证书时间 执行命令 查看当前证书时间\n1 kubeadm alpha certs check-expiration 下载源码 打开github kubernetes 选择对应的版本下载\n下载并解压\n因为我是 v1.20.6 版本所以下载对应的\n1 2 wget https://github.com/kubernetes/kubernetes/archive/refs/tags/v1.20.6.zip unzip v1.20.6.zip 修改 constants.go 文件\nvim cmd/kubeadm/app/constants/constants.go 找到 CertificateValidity ，修改如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cd kubernetes-1.20.6 vim cmd/kubeadm/app/constants/constants.go .... const ( // KubernetesDir is the directory Kubernetes owns for storing various configuration files KubernetesDir = \u0026#34;/etc/kubernetes\u0026#34; // ManifestsSubDirName defines directory name to store manifests ManifestsSubDirName = \u0026#34;manifests\u0026#34; // TempDirForKubeadm defines temporary directory for kubeadm // should be joined with KubernetesDir. TempDirForKubeadm = \u0026#34;tmp\u0026#34; // CertificateValidity defines the validity for all the signed certificates generated by kubeadm CertificateValidity = time.Hour * 24 * 365 * 100 # 修改此内容 .... 编译 kubeadm 1 make WHAT=cmd/kubeadm 返回如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@master kubernetes-1.20.6]# make WHAT=cmd/kubeadm +++ [0624 10:59:21] Building go targets for linux/amd64: ./vendor/k8s.io/code-generator/cmd/prerelease-lifecycle-gen +++ [0624 10:59:25] Building go targets for linux/amd64: ./vendor/k8s.io/code-generator/cmd/deepcopy-gen +++ [0624 10:59:33] Building go targets for linux/amd64: ./vendor/k8s.io/code-generator/cmd/defaulter-gen +++ [0624 10:59:44] Building go targets for linux/amd64: ./vendor/k8s.io/code-generator/cmd/conversion-gen +++ [0624 11:00:04] Building go targets for linux/amd64: ./vendor/k8s.io/kube-openapi/cmd/openapi-gen +++ [0624 11:00:19] Building go targets for linux/amd64: ./vendor/github.com/go-bindata/go-bindata/go-bindata +++ [0624 11:00:20] Building go targets for linux/amd64: cmd/kubeadm 编译完生成如下目录和二进制文件\n1 2 3 4 5 6 7 8 9 10 [root@master kubernetes-1.20.6]# ll _output/bin/ 总用量 75680 -rwxr-xr-x. 1 root root 5943296 6月 24 10:59 conversion-gen -rwxr-xr-x. 1 root root 5689344 6月 24 10:59 deepcopy-gen -rwxr-xr-x. 1 root root 5709824 6月 24 10:59 defaulter-gen -rwxr-xr-x. 1 root root 3555111 6月 24 10:59 go2make -rwxr-xr-x. 1 root root 1966080 6月 24 11:00 go-bindata -rwxr-xr-x. 1 root root 39325696 6月 24 11:01 kubeadm -rwxr-xr-x. 1 root root 9650176 6月 24 11:00 openapi-gen -rwxr-xr-x. 1 root root 5656576 6月 24 10:59 prerelease-lifecycle-gen 备份文件 备份 kubeadm 和证书文件\n1 2 cp /usr/bin/kubeadm{,.bak20210624} cp -r /etc/kubernetes/pki{,.bak20210624} 查看备份文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [root@master kubernetes-1.20.6]# ll /usr/bin/kubeadm* -rwxr-xr-x. 1 root root 39325696 6月 24 11:05 /usr/bin/kubeadm -rwxr-xr-x. 1 root root 39210880 6月 24 11:02 /usr/bin/kubeadm.bak20210624 [root@master kubernetes-1.20.6 ll /etc/kubernetes/pki* /etc/kubernetes/pki: 总用量 56 -rw-r--r--. 1 root root 1289 6月 24 11:05 apiserver.crt -rw-r--r--. 1 root root 1139 6月 24 11:05 apiserver-etcd-client.crt -rw-------. 1 root root 1675 6月 24 11:05 apiserver-etcd-client.key -rw-------. 1 root root 1679 6月 24 11:05 apiserver.key -rw-r--r--. 1 root root 1147 6月 24 11:05 apiserver-kubelet-client.crt -rw-------. 1 root root 1675 6月 24 11:05 apiserver-kubelet-client.key -rw-r--r--. 1 root root 1066 6月 22 15:01 ca.crt -rw-------. 1 root root 1675 6月 22 15:01 ca.key drwxr-xr-x. 2 root root 162 6月 22 15:01 etcd -rwxr-xr-x. 1 root root 1078 6月 22 15:01 front-proxy-ca.crt -rw-------. 1 root root 1675 6月 22 15:01 front-proxy-ca.key -rw-r--r--. 1 root root 1103 6月 24 11:05 front-proxy-client.crt -rw-------. 1 root root 1679 6月 24 11:05 front-proxy-client.key -rw-------. 1 root root 1675 6月 22 15:01 sa.key -rw-------. 1 root root 451 6月 22 15:01 sa.pub /etc/kubernetes/pki.bak20210624: 总用量 56 -rw-r--r--. 1 root root 1289 6月 24 11:04 apiserver.crt -rw-r--r--. 1 root root 1135 6月 24 11:04 apiserver-etcd-client.crt -rw-------. 1 root root 1675 6月 24 11:04 apiserver-etcd-client.key -rw-------. 1 root root 1679 6月 24 11:04 apiserver.key -rw-r--r--. 1 root root 1143 6月 24 11:04 apiserver-kubelet-client.crt -rw-------. 1 root root 1675 6月 24 11:04 apiserver-kubelet-client.key -rw-r--r--. 1 root root 1066 6月 24 11:04 ca.crt -rw-------. 1 root root 1675 6月 24 11:04 ca.key drwxr-xr-x. 2 root root 162 6月 24 11:04 etcd -rwxr-xr-x. 1 root root 1078 6月 24 11:04 front-proxy-ca.crt -rw-------. 1 root root 1675 6月 24 11:04 front-proxy-ca.key -rw-r--r--. 1 root root 1103 6月 24 11:04 front-proxy-client.crt -rw-------. 1 root root 1679 6月 24 11:04 front-proxy-client.key -rw-------. 1 root root 1675 6月 24 11:04 sa.key -rw-------. 1 root root 451 6月 24 11:04 sa.pub 替换 kubeadm 将新生成的 kubeadm 进行替换\n1 cp _output/bin/kubeadm /usr/bin/kubeadm 生成新的证书 1 2 cd /etc/kubernetes/pki kubeadm alpha certs renew all 返回内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@master pki]# kubeadm alpha certs renew all Command \u0026#34;all\u0026#34; is deprecated, please use the same command under \u0026#34;kubeadm certs\u0026#34; [renew] Reading configuration from the cluster... [renew] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed certificate for serving the Kubernetes API renewed certificate the apiserver uses to access etcd renewed certificate for the API server to connect to kubelet renewed certificate embedded in the kubeconfig file for the controller manager to use renewed certificate for liveness probes to healthcheck etcd renewed certificate for etcd nodes to communicate with each other renewed certificate for serving etcd renewed certificate for the front proxy client renewed certificate embedded in the kubeconfig file for the scheduler manager to use renewed Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates. 验证结果 到这里，证书就替换完成了。接下来验证下证书时间是否延长。\n1 kubeadm alpha certs check-expiration 返回信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@master pki]# kubeadm alpha certs check-expiration Command \u0026#34;check-expiration\u0026#34; is deprecated, please use the same command under \u0026#34;kubeadm certs\u0026#34; [check-expiration] Reading configuration from the cluster... [check-expiration] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf May 31, 2121 03:05 UTC 99y no apiserver May 31, 2121 03:05 UTC 99y ca no apiserver-etcd-client May 31, 2121 03:05 UTC 99y etcd-ca no apiserver-kubelet-client May 31, 2121 03:05 UTC 99y ca no controller-manager.conf May 31, 2121 03:05 UTC 99y no etcd-healthcheck-client May 31, 2121 03:05 UTC 99y etcd-ca no etcd-peer May 31, 2121 03:05 UTC 99y etcd-ca no etcd-server May 31, 2121 03:05 UTC 99y etcd-ca no front-proxy-client May 31, 2121 03:05 UTC 99y front-proxy-ca no scheduler.conf May 31, 2121 03:05 UTC 99y no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Jun 20, 2031 07:01 UTC 9y no etcd-ca Jun 20, 2031 07:01 UTC 9y no front-proxy-ca Jun 20, 2031 07:01 UTC 9y no 查看 node 状态\n1 2 3 4 [root@master pki]# kubectl get node NAME STATUS ROLES AGE VERSION master Ready control-plane,master 44h v1.20.6 node1 Ready \u0026lt;none\u0026gt; 43h v1.20.6 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210624108255/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"kubeadm  更新证书，修改证书时间为99年","id":55,"section":"posts","tags":["kubeadm"],"title":"kubeadm  更新证书","uri":"https://www.cnsre.cn/posts/210624108255/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210621550392/\n相关话题：https://www.cnsre.cn/tags/prometheus/\n简介 Prometheus 介绍 Prometheus 是一套开源的监控 \u0026amp; 报警 \u0026amp; 时间序列数据库的组合,起始是由 SoundCloud 公司开发的。成立于 2012 年，之后许多公司和组织接受和采用 prometheus,他们便将它独立成开源项目，并且有公司来运作.该项目有非常活跃的社区和开发人员，目前是独立的开源项目，任何公司都可以使用它，2016 年，Prometheus 加入了云计算基金会，成为 kubernetes 之后的第二个托管项目.google SRE 的书内也曾提到跟他们 BorgMon 监控系统相似的实现是 Prometheus。现在最常见的 Kubernetes 容器管理系统中，通常会搭配 Prometheus 进行监控。\nKubernetes Operator 介绍 Prometheus-operator 已经改名为 Kube-promethues 在 Kubernetes 的支持下，管理和伸缩 Web 应用、移动应用后端以及 API 服务都变得比较简单了。其原因是这些应用一般都是无状态的，所以 Deployment 这样的基础 Kubernetes API 对象就可以在无需附加操作的情况下，对应用进行伸缩和故障恢复了。\n而对于数据库、缓存或者监控系统等有状态应用的管理，就是个挑战了。这些系统需要应用领域的知识，来正确的进行伸缩和升级，当数据丢失或不可用的时候，要进行有效的重新配置。我们希望这些应用相关的运维技能可以编码到软件之中，从而借助 Kubernetes` 的能力，正确的运行和管理复杂应用。\nOperator 这种软件，使用 TPR (第三方资源，现在已经升级为 CRD) 机制对 Kubernetes API 进行扩展，将特定应用的知识融入其中，让用户可以创建、配置和管理应用。和 Kubernetes 的内置资源一样，Operator 操作的不是一个单实例应用，而是集群范围内的多实例。\nPrometheus Operator 介绍 Kubernetes 的 Prometheus Operator 为 Kubernetes 服务和 Prometheus 实例的部署和管理提供了简单的监控定义。\n安装完毕后，Prometheus Operator 提供了以下功能：\n创建/毁坏： 在 Kubernetes namespace 中更容易启动一个 Prometheus 实例，一个特定的应用程序或团队更容易使用Operator。 简单配置： 配置 Prometheus 的基础东西，比如在 Kubernetes 的本地资源 versions, persistence, retention policies, 和 replicas。 Target Services 通过标签： 基于常见的 Kubernetes label 查询，自动生成监控 target 配置；不需要学习 Prometheus 特定的配置语言。 架构 Prometheus 为一个监控系统， Prometheus 项目的作用和工作方式，其实可以用如下所示的一张官方示意图来解释：\n可以看到， Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。有了这套核心监控机制， Prometheus 剩下的组件就是用来配合这套机制的运行。比如 Pushgateway ，可以允许被监控对象以 Push 的方式向 Prometheus 推送 Metrics 数据。而 Alertmanager ，则可以根据 Metrics 信息灵活地设置报警。当然， Prometheus 最受用户欢迎的功能，还是通过 Grafana 对外暴露出的、可以灵活配置的监控数据可视化界面。\nPrometheus Operator Operator： Operator 资源会根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server ，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。\nPrometheus： Prometheus 资源是声明性地描述 Prometheus 部署的期望状态。\nPrometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。\nServiceMonitor： ServiceMonitor 也是一个自定义资源，它描述了一组被 Prometheus 监控的 targets 列表。该资源通过 Labels 来选取对应的 Service Endpoint ，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。\nService： Service 资源主要用来对应 Kubernetes 集群中的 Metrics Server Pod ，来提供给 ServiceMonitor 选取让 Prometheus Server 来获取信息。简单的说就是 Prometheus Node Exporter Service 、Mysql Exporter Service 等等。\nAlertmanager： Alertmanager 也是一个自定义资源类型，由 Operator 根据资源描述内容来部署 Alertmanager 集群\n安装前须知 在k8s中部署Prometheus监控的方法 通常在k8s中部署 prometheus 监控可以采取的方法有以下三种\n通过yaml手动部署 operator部署 通过helm chart部署 kube-prometheus 兼容说明 kube-prometheus stack Kubernetes 1.18 Kubernetes 1.19 Kubernetes 1.20 Kubernetes 1.21 release-0.5 ✔ ✗ ✗ ✗ release-0.6 ✗ ✔ ✗ ✗ release-0.7 ✗ ✔ ✔ ✗ release-0.8 ✗ ✗ ✔ ✔ HEAD ✗ ✗ ✔ ✔ 环境参数 系统参数：\nKube-promethues 版本： 0.7.0\nKubernetes 版本： v1.19.5\n项目 Github 地址： https://github.com/coreos/kube-prometheus\n拉取 kube-prometheus 0.7 先从 Github 上将源码拉取下来，利用源码项目已经写好的 kubernetes 的 yaml 文件进行一系列集成镜像的安装，如 grafana 、prometheus 等等。\n1 wget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.7.0.zip 解压\n1 cd kube-prometheus-0.7.0/ yaml 文件分类 由于它的文件都存放在项目源码的 manifests 文件夹下，所以需要进入其中进行启动这些 kubernetes 应用 yaml 文件。又由于这些文件堆放在一起，不利于分类启动，所以这里将它们分类。\n进入源码的 manifests 文件夹：\n1 cd manifests/ 创建文件夹并且将 yaml 文件分类：\n1 2 3 4 5 6 7 8 9 10 11 # 创建文件夹 mkdir -p node-exporter alertmanager grafana kube-state-metrics prometheus serviceMonitor adapter # 移动 yaml 文件，进行分类到各个文件夹下 mv *-serviceMonitor* serviceMonitor/ mv grafana-* grafana/ mv kube-state-metrics-* kube-state-metrics/ mv alertmanager-* alertmanager/ mv node-exporter-* node-exporter/ mv prometheus-adapter* adapter/ mv prometheus-* prometheus/ 基本目录结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 [root@master kube-prometheus-0.7.0]# tree manifests/ manifests/ |-- adapter | |-- prometheus-adapter-apiService.yaml | |-- prometheus-adapter-clusterRoleAggregatedMetricsReader.yaml | |-- prometheus-adapter-clusterRoleBindingDelegator.yaml | |-- prometheus-adapter-clusterRoleBinding.yaml | |-- prometheus-adapter-clusterRoleServerResources.yaml | |-- prometheus-adapter-clusterRole.yaml | |-- prometheus-adapter-configMap.yaml | |-- prometheus-adapter-deployment.yaml | |-- prometheus-adapter-roleBindingAuthReader.yaml | |-- prometheus-adapter-serviceAccount.yaml | `-- prometheus-adapter-service.yaml |-- alertmanager | |-- alertmanager-alertmanager.yaml | |-- alertmanager-secret.yaml | |-- alertmanager-serviceAccount.yaml | `-- alertmanager-service.yaml |-- grafana | |-- grafana-dashboardDatasources.yaml | |-- grafana-dashboardDefinitions.yaml | |-- grafana-dashboardSources.yaml | |-- grafana-deployment.yaml | |-- grafana-serviceAccount.yaml | `-- grafana-service.yaml |-- kube-state-metrics | |-- kube-state-metrics-clusterRoleBinding.yaml | |-- kube-state-metrics-clusterRole.yaml | |-- kube-state-metrics-deployment.yaml | |-- kube-state-metrics-serviceAccount.yaml | `-- kube-state-metrics-service.yaml |-- node-exporter | |-- node-exporter-clusterRoleBinding.yaml | |-- node-exporter-clusterRole.yaml | |-- node-exporter-daemonset.yaml | |-- node-exporter-serviceAccount.yaml | `-- node-exporter-service.yaml |-- prometheus | |-- prometheus-clusterRoleBinding.yaml | |-- prometheus-clusterRole.yaml | |-- prometheus-prometheus.yaml | |-- prometheus-roleBindingConfig.yaml | |-- prometheus-roleBindingSpecificNamespaces.yaml | |-- prometheus-roleConfig.yaml | |-- prometheus-roleSpecificNamespaces.yaml | |-- prometheus-rules.yaml | |-- prometheus-serviceAccount.yaml | `-- prometheus-service.yaml |-- serviceMonitor | |-- alertmanager-serviceMonitor.yaml | |-- grafana-serviceMonitor.yaml | |-- kube-state-metrics-serviceMonitor.yaml | |-- node-exporter-serviceMonitor.yaml | |-- prometheus-adapter-serviceMonitor.yaml | |-- prometheus-operator-serviceMonitor.yaml | |-- prometheus-serviceMonitorApiserver.yaml | |-- prometheus-serviceMonitorCoreDNS.yaml | |-- prometheus-serviceMonitorKubeControllerManager.yaml | |-- prometheus-serviceMonitorKubelet.yaml | |-- prometheus-serviceMonitorKubeScheduler.yaml | `-- prometheus-serviceMonitor.yaml `-- setup |-- 0namespace-namespace.yaml |-- prometheus-operator-0alertmanagerConfigCustomResourceDefinition.yaml |-- prometheus-operator-0alertmanagerCustomResourceDefinition.yaml |-- prometheus-operator-0podmonitorCustomResourceDefinition.yaml |-- prometheus-operator-0probeCustomResourceDefinition.yaml |-- prometheus-operator-0prometheusCustomResourceDefinition.yaml |-- prometheus-operator-0prometheusruleCustomResourceDefinition.yaml |-- prometheus-operator-0servicemonitorCustomResourceDefinition.yaml |-- prometheus-operator-0thanosrulerCustomResourceDefinition.yaml |-- prometheus-operator-clusterRoleBinding.yaml |-- prometheus-operator-clusterRole.yaml |-- prometheus-operator-deployment.yaml |-- prometheus-operator-serviceAccount.yaml `-- prometheus-operator-service.yaml 修改 Service 端口设置 修改 prometheus-service.yaml 文件\n1 vim prometheus/prometheus-service.yaml 修改 prometheus Service 端口类型为 NodePort，设置 NodePort 端口为 30100：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Service metadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoring spec: type: NodePort # 增加配置 ports: - name: web port: 9090 targetPort: web nodePort: 30100 # 增加配置 selector: app: prometheus prometheus: k8s sessionAffinity: ClientIP 修改 Grafana Service 修改 grafana-service.yaml 文件：\n1 vim grafana/grafana-service.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Service metadata: labels: app: grafana name: grafana namespace: monitoring spec: type: NodePort # 增加配置 ports: - name: http port: 3000 targetPort: http nodePort: 30101 # 增加配置 selector: app: grafana 安装Prometheus Operator 所有文件都在 manifests 目录下执行。 安装 Operator 1 kubectl apply -f setup/ 查看 Pod，等 pod 创建起来在进行下一步：\n1 2 3 [root@master manifests]# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE prometheus-operator-7649c7454f-q5f2r 2/2 Running 0 50s 这会创建一个名为 monitoring 的命名空间，以及相关的 CRD 资源对象声明和 Prometheus Operator 控制器。前面中我们介绍过 CRD 和 Operator 的使用，当我们声明完 CRD 过后，就可以来自定义资源清单了，但是要让我们声明的自定义资源对象生效就需要安装对应的 Operator 控制器，这里我们都已经安装了，所以接下来就可以来用 CRD 创建真正的自定义资源对象了。其实在 manifests 目录下面的就是我们要去创建的 Prometheus 、Alertmanager 以及各种监控对象的资源清单。\n安装其它组件 没有特殊的定制需求我们可以直接一键安装：\n1 2 3 4 5 6 7 kubectl apply -f adapter/ kubectl apply -f alertmanager/ kubectl apply -f node-exporter/ kubectl apply -f kube-state-metrics/ kubectl apply -f grafana/ kubectl apply -f prometheus/ kubectl apply -f serviceMonitor/ 查看 Pod 状态：\n1 2 3 4 5 6 7 8 9 10 11 12 13 [root@master manifests]# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 16m alertmanager-main-1 2/2 Running 0 16m alertmanager-main-2 2/2 Running 0 16m grafana-6f777cf998-sv4mz 1/1 Running 0 16m kube-state-metrics-587bfd4f97-mhj94 3/3 Running 0 16m node-exporter-cgbl2 2/2 Running 0 16m node-exporter-k9fqg 2/2 Running 0 16m prometheus-adapter-69b8496df6-954s6 1/1 Running 0 16m prometheus-k8s-0 2/2 Running 0 16m prometheus-k8s-1 2/2 Running 0 16m prometheus-operator-7649c7454f-q5f2r 2/2 Running 0 16m 查看 Prometheus \u0026amp; Grafana 查看 Prometheus 打开地址：http://node_ip:30100 查看 Prometheus 采集的目标，看其各个采集服务状态有没有错误。\n查看 Grafana 打开地址：http://node_ip:30101 查看 Grafana 图表，看其 Kubernetes 集群是否能正常显示。\n默认用户名：admin\n默认密码：admin\n查看仪表盘：\n微信报警 获取企业 ID 等信息 配置前参数说明:\n获取企业微信的对外接口\n企业微信的 secret_api\n企业信息ID corp_id\nwechat_api_url:\n企业微信对外接口 https://qyapi.weixin.qq.com/cgi-bin/\napi_secret: 应用管理\u0026ndash;应用\u0026ndash;你的应用\nagent_id: 应用管理\u0026ndash;应用\u0026ndash;你的应用\ncorp_id: 我的企业\u0026ndash; 企业信息\u0026ndash;下拉至最下\u0026ndash;企业ID\nto_party: 通讯录\u0026ndash;组\u0026ndash;部门ID\nto_user： 通讯录\u0026ndash;组\u0026ndash;部门ID\u0026ndash;选中接收告警的人\u0026ndash;账号\n登录企业微信\u0026ndash;应用管理\u0026ndash;应用\u0026ndash;下拉到最下面点击 创建应用\n需要记着 AgentId、Secret\n点击 我的企业\u0026ndash; 企业信息\u0026ndash;下拉至最下\u0026ndash;企业ID\n通讯录\u0026ndash;组\u0026ndash;部门ID\n将对应秘钥填入yaml 配置 alertmanager.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cd alertmanager/ cat \u0026lt;\u0026lt;EOF \u0026gt; alertmanager.yaml global: resolve_timeout: 5m receivers: - name: wechat wechat_configs: - agent_id: \u0026#34;1000005\u0026#34; api_secret: _UCBVsNYTQwY75TFKQ4V6jj2YQKsyxxxxx corp_id: ww022bebbed74fxxxx send_resolved: true to_user: xuewenlong route: group_by: - job group_interval: 5m group_wait: 30s receiver: wechat repeat_interval: 1h routes: - match: alertname: Watchdog receiver: wechat templates: - /etc/alertmanager/config/wechat.tmpl EOF 个性化配置报警模板，网上有很多例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 cat \u0026lt;\u0026lt;EOF \u0026gt; wechat.tmpl {{ define \u0026#34;wechat.default.message\u0026#34; }} {{- if gt (len .Alerts.Firing) 0 -}} {{- range $index, $alert := .Alerts -}} {{- if eq $index 0 }} ==========异常告警========== 告警类型: {{ $alert.Labels.alertname }} 告警级别: {{ $alert.Labels.severity }} 告警详情: {{ $alert.Annotations.message }}{{ $alert.Annotations.description}};{{$alert.Annotations.summary}} 故障时间: {{ ($alert.StartsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }} {{- if gt (len $alert.Labels.instance) 0 }} 实例信息: {{ $alert.Labels.instance }} {{- end }} {{- if gt (len $alert.Labels.namespace) 0 }} 命名空间: {{ $alert.Labels.namespace }} {{- end }} {{- if gt (len $alert.Labels.node) 0 }} 节点信息: {{ $alert.Labels.node }} {{- end }} {{- if gt (len $alert.Labels.pod) 0 }} 实例名称: {{ $alert.Labels.pod }} {{- end }} ============END============ {{- end }} {{- end }} {{- end }} {{- if gt (len .Alerts.Resolved) 0 -}} {{- range $index, $alert := .Alerts -}} {{- if eq $index 0 }} ==========异常恢复========== 告警类型: {{ $alert.Labels.alertname }} 告警级别: {{ $alert.Labels.severity }} 告警详情: {{ $alert.Annotations.message }}{{ $alert.Annotations.description}};{{$alert.Annotations.summary}} 故障时间: {{ ($alert.StartsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }} 恢复时间: {{ ($alert.EndsAt.Add 28800e9).Format \u0026#34;2006-01-02 15:04:05\u0026#34; }} {{- if gt (len $alert.Labels.instance) 0 }} 实例信息: {{ $alert.Labels.instance }} {{- end }} {{- if gt (len $alert.Labels.namespace) 0 }} 命名空间: {{ $alert.Labels.namespace }} {{- end }} {{- if gt (len $alert.Labels.node) 0 }} 节点信息: {{ $alert.Labels.node }} {{- end }} {{- if gt (len $alert.Labels.pod) 0 }} 实例名称: {{ $alert.Labels.pod }} {{- end }} ============END============ {{- end }} {{- end }} {{- end }} {{- end }} EOF 部署secret 1 2 kubectl delete secret alertmanager-main -n monitoring kubectl create secret generic alertmanager-main --from-file=alertmanager.yaml --from-file=wechat.tmpl -n monitoring 查看容器中的配置文件\n1 2 3 kubectl exec -it alertmanager-main-0 -n monitoring /bin/sh ls /etc/alertmanager/config/ # 如果能看到 alertmanager.yaml wechat.tmpl 两个文件就证明更新成功了。 验证 更新后就会收到消息\n自动发现规则 在k8s中会部署非常多的service和 pod，如果要一个一个手动的添加监控非常麻烦且不必要，使用自动发现机制。将会减少很多不必要的工作。\n创建 yaml 文件 创建prometheus-additional.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 cd prometheus/ cat prometheus-additional.yaml - job_name: \u0026#39;kubernetes-service-endpoints\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name 要想自动被发现，只需要在 service 的配置清单中加上 annotations: prometheus.io/scrape=true 。\n创建 secret 然后用这个文件创建一个 secret\n1 kubectl -n monitoring create secret generic additional-config --from-file=prometheus-additional.yaml 修改 prometheus 配置清单 在 prometheus 的配置清单中添加以下配置：\n1 2 3 additionalScrapeConfigs: name: additional-config key: prometheus-additional.yaml 具体如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 cat prometheus-prometheus.yaml apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.26.0 prometheus: k8s name: k8s namespace: monitoring spec: alerting: alertmanagers: - apiVersion: v2 name: alertmanager-main namespace: monitoring port: web externalLabels: {} image: quay.io/prometheus/prometheus:v2.26.0 nodeSelector: kubernetes.io/os: linux podMetadata: labels: app.kubernetes.io/component: prometheus app.kubernetes.io/name: prometheus app.kubernetes.io/part-of: kube-prometheus app.kubernetes.io/version: 2.26.0 podMonitorNamespaceSelector: {} podMonitorSelector: {} probeNamespaceSelector: {} probeSelector: {} replicas: 2 resources: requests: memory: 400Mi ruleSelector: matchLabels: prometheus: k8s role: alert-rules securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 1000 additionalScrapeConfigs: # 添加内容 name: additional-configs # 添加内容 key: prometheus-additional.yaml # 添加内容 serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: {} serviceMonitorSelector: {} version: 2.26.0 然后更新 prometheus 的配置\n1 kubectl apply -f prometheus-prometheus.yaml 修改 clusterrole 权限 Prometheus 绑定了一个名为 prometheus-k8s 的 ServiceAccount 对象，而这个对象绑定的是一个名为 prometheus-k8s 的 ClusterRole （prometheus-clusterRole.yaml）\n需要让他有 Service 或者 Pod 的 list 权限\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat prometheus-clusterRole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/metrics - configmaps verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes - pods - services - endpoints - nodes/proxy verbs: - get - list - watch - nonResourceURLs: - /metrics verbs: - get 更新 clusterrole\n1 kubectl apply -f prometheus-clusterRole.yaml 等待一段时间可以发现自动发现成功\n配置自动发现，首先 annotations 里需要配置 prometheus.io/scrape=true ，其次你的应用要有 exporter 去收集信息。\n1 2 3 4 annotations: prometheus.io/scrape: \u0026#39;true\u0026#39; # 添加内容 prometheus.io/path: \u0026#39;/metrics\u0026#39; # 添加内容 prometheus.io/port: \u0026#39;80\u0026#39; # 添加内容 比如 Nginx 配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx annotations: prometheus.io/scrape: \u0026#39;true\u0026#39; prometheus.io/path: \u0026#39;/metrics\u0026#39; prometheus.io/port: \u0026#39;80\u0026#39; spec: replicas: 2 selector: matchLabels: app: nginx minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: nginx annotations: prometheus.io/scrape: \u0026#39;true\u0026#39; prometheus.io/path: \u0026#39;/metrics\u0026#39; prometheus.io/port: \u0026#39;80\u0026#39; spec: containers: - name: nginx image: wenlongxue/nginx:waynex-ce995b8 imagePullPolicy: Always ports: - containerPort: 80 resources: requests: memory: \u0026#34;100Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;100Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; readinessProbe: httpGet: path: /index.html port: 80 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 --- apiVersion: v1 kind: Service metadata: name: nginx-service labels: app: nginx annotations: prometheus.io/scrape: \u0026#39;true\u0026#39; # 添加内容 prometheus.io/path: \u0026#39;/metrics\u0026#39; # 添加内容 prometheus.io/port: \u0026#39;80\u0026#39; # 添加内容 spec: selector: app: nginx ports: - name: nginx-port protocol: TCP port: 80 nodePort: 30081 targetPort: 80 type: NodePort 钉钉告警 配置 prometheus-operate 钉钉告警\n创建 webhook 的配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # vim dingTalk-webhook-configmap.yml apiVersion: v1 kind: ConfigMap metadata: namespace: monitoring name: dingtalk-webhook-config data: config.yml: | # Request timeout timeout: 5s ## Customizable templates path templates: - /etc/prometheus-webhook-dingtalk/templates/*.tmpl ## You can also override default template using `default_message` ## The following example to use the \u0026#39;legacy\u0026#39; template from v0.3.0 # default_message: # title: \u0026#39;{{ template \u0026#34;legacy.title\u0026#34; . }}\u0026#39; # text: \u0026#39;{{ template \u0026#34;legacy.content\u0026#34; . }}\u0026#39; ## Targets, previously was known as \u0026#34;profiles\u0026#34; targets: guiji: url: https://oapi.dingtalk.com/robot/send?access_token=1a3ed90fefa5d1a8f4d61f063xxxxxxxxxxxxxxxxx message: title: \u0026#39;{{ template \u0026#34;ding.link.title\u0026#34; . }}\u0026#39; text: \u0026#39;{{ template \u0026#34;ding.link.content\u0026#34; . }}\u0026#39; mention: all: true mobiles: [\u0026#39;153xxxxxxxx\u0026#39;] 创建告警模板配置文件： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 # vim dingTalk-webhook-template.yml apiVersion: v1 kind: ConfigMap metadata: namespace: monitoring name: dingtalk-webhook-template data: template.tmpl: | {{ define \u0026#34;__subject\u0026#34; }}[{{ .Status | toUpper }}{{ if eq .Status \u0026#34;firing\u0026#34; }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join \u0026#34; \u0026#34; }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join \u0026#34; \u0026#34; }}{{ end }}){{ end }}{{ end }} {{ define \u0026#34;__alertmanagerURL\u0026#34; }}{{ .ExternalURL }}/#/alerts?receiver={{ .Receiver }}{{ end }} {{ define \u0026#34;__text_alert_list\u0026#34; }}{{ range . }} **Labels** {{ range .Labels.SortedPairs }}\u0026amp;gt; - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **Annotations** {{ range .Annotations.SortedPairs }}\u0026amp;gt; - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **Source:** [{{ .GeneratorURL }}]({{ .GeneratorURL }}) {{ end }}{{ end }} {{ define \u0026#34;default.__text_alert_list\u0026#34; }}{{ range . }} --- **告警级别:** {{ .Labels.severity | upper }} **运营团队:** {{ .Labels.team | upper }} **触发时间:** {{ dateInZone \u0026#34;2006.01.02 15:04:05\u0026#34; (.StartsAt) \u0026#34;Asia/Shanghai\u0026#34; }} **事件信息:** {{ range .Annotations.SortedPairs }}\u0026amp;gt; - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **事件标签:** {{ range .Labels.SortedPairs }}{{ if and (ne (.Name) \u0026#34;severity\u0026#34;) (ne (.Name) \u0026#34;summary\u0026#34;) (ne (.Name) \u0026#34;team\u0026#34;) }}\u0026amp;gt; - {{ .Name }}: {{ .Value | markdown | html }} {{ end }}{{ end }} {{ end }} {{ end }} {{ define \u0026#34;default.__text_alertresovle_list\u0026#34; }}{{ range . }} --- **告警级别:** {{ .Labels.severity | upper }} **运营团队:** {{ .Labels.team | upper }} **触发时间:** {{ dateInZone \u0026#34;2006.01.02 15:04:05\u0026#34; (.StartsAt) \u0026#34;Asia/Shanghai\u0026#34; }} **结束时间:** {{ dateInZone \u0026#34;2006.01.02 15:04:05\u0026#34; (.EndsAt) \u0026#34;Asia/Shanghai\u0026#34; }} **事件信息:** {{ range .Annotations.SortedPairs }}\u0026amp;gt; - {{ .Name }}: {{ .Value | markdown | html }} {{ end }} **事件标签:** {{ range .Labels.SortedPairs }}{{ if and (ne (.Name) \u0026#34;severity\u0026#34;) (ne (.Name) \u0026#34;summary\u0026#34;) (ne (.Name) \u0026#34;team\u0026#34;) }}\u0026amp;gt; - {{ .Name }}: {{ .Value | markdown | html }} {{ end }}{{ end }} {{ end }} {{ end }} {{/* Default */}} {{ define \u0026#34;default.title\u0026#34; }}{{ template \u0026#34;__subject\u0026#34; . }}{{ end }} {{ define \u0026#34;default.content\u0026#34; }}#### \\[{{ .Status | toUpper }}{{ if eq .Status \u0026#34;firing\u0026#34; }}:{{ .Alerts.Firing | len }}{{ end }}\\] **[{{ index .GroupLabels \u0026#34;alertname\u0026#34; }}]({{ template \u0026#34;__alertmanagerURL\u0026#34; . }})** {{ if gt (len .Alerts.Firing) 0 -}} ![警报 图标](https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=3626076420,1196179712\u0026amp;amp;fm=15\u0026amp;amp;gp=0.jpg) **====侦测到故障====** {{ template \u0026#34;default.__text_alert_list\u0026#34; .Alerts.Firing }} {{- end }} {{ if gt (len .Alerts.Resolved) 0 -}} {{ template \u0026#34;default.__text_alertresovle_list\u0026#34; .Alerts.Resolved }} {{- end }} {{- end }} {{/* Legacy */}} {{ define \u0026#34;legacy.title\u0026#34; }}{{ template \u0026#34;__subject\u0026#34; . }}{{ end }} {{ define \u0026#34;legacy.content\u0026#34; }}#### \\[{{ .Status | toUpper }}{{ if eq .Status \u0026#34;firing\u0026#34; }}:{{ .Alerts.Firing | len }}{{ end }}\\] **[{{ index .GroupLabels \u0026#34;alertname\u0026#34; }}]({{ template \u0026#34;__alertmanagerURL\u0026#34; . }})** {{ template \u0026#34;__text_alert_list\u0026#34; .Alerts.Firing }} {{- end }} {{/* Following names for compatibility */}} {{ define \u0026#34;ding.link.title\u0026#34; }}{{ template \u0026#34;default.title\u0026#34; . }}{{ end }} {{ define \u0026#34;ding.link.content\u0026#34; }}{{ template \u0026#34;default.content\u0026#34; . }}{{ end }} 创建 webhook 的资源配置清单 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # cat dingTalk-webhook-deployment.yml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: monitoring name: dingtalk-webhook labels: app: dingtalk-webhook spec: selector: matchLabels: app: dingtalk-webhook replicas: 1 template: metadata: labels: app: dingtalk-webhook spec: containers: - name: dingtalk-webhook image: harbor.zsf.com/public/prometheus-webhook-dingtalk args: - --config.file=/etc/prometheus-webhook-dingtalk/config.yml #- --ding.profile=guiji=https://oapi.dingtalk.com/robot/send?access_token=5752a9d10727165d116b883b4e7d312b781a3ed90 ports: - containerPort: 8060 protocol: TCP volumeMounts: - mountPath: \u0026#34;/etc/prometheus-webhook-dingtalk\u0026#34; name: dingtalk-webhook-confing subPath: config.yml - mountPath: \u0026#34;/etc/prometheus-webhook-dingtalk/templates\u0026#34; name: dingtalk-webhook-template subPath: template.tmpl volumes: - name: dingtalk-webhook-confing configMap: name: dingtalk-webhook-config - name: dingtalk-webhook-template configMap: name: dingtalk-webhook-template --- apiVersion: v1 kind: Service metadata: namespace: monitoring name: dingtalk-webhook labels: app: dingtalk-webhook spec: selector: app: dingtalk-webhook ports: - name: http port: 8060 targetPort: 8060 protocol: TCP 解决ControllerManager、Scheduler监控问题 默认安装后访问 prometheus ，会发现有以下有三个报警：\nWatchdog 、KubeControllerManagerDown 、 KubeSchedulerDown\nWatchdog 是一个正常的报警，这个告警的作用是：如果 alermanger 或者 prometheus 本身挂掉了就发不出告警了，因此一般会采用另一个监控来监控 prometheus ，或者自定义一个持续不断的告警通知，哪一天这个告警通知不发了，说明监控出现问题了。 prometheus operator 已经考虑了这一点，本身携带一个 watchdog ，作为对自身的监控。\n如果需要关闭，删除或注释掉 Watchdog 部分\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim kube-prometheus-prometheusRule.yaml ... - name: general.rules rules: - alert: TargetDown annotations: message: \\\u0026#39;xxx\\\u0026#39; expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job, namespace, service)) \u0026gt; 10 for: 10m labels: severity: warning # - alert: Watchdog # annotations: # message: | # This is an alert meant to ensure that the entire alerting pipeline is functional. # This alert is always firing, therefore it should always be firing in Alertmanager # and always fire against a receiver. There are integrations with various notification # mechanisms that send a notification when this alert is not firing. For example the # \u0026#34;DeadMansSnitch\u0026#34; integration in PagerDuty. # expr: vector(1) # labels: # severity: none 其他两个 KubeControllerManagerDown 、 KubeSchedulerDown 需要自定义监控，在这边不详细说。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210621550392/\n相关话题：https://www.cnsre.cn/tags/prometheus/\n","description":"Prometheus 是一套开源的监控 \u0026 报警 \u0026 时间序列数据库的组合,我们使用 Kubernetes 1.19 来安装 Prometheus-Oprator。","id":56,"section":"posts","tags":["kubernetes","prometheus"],"title":"Kubernetes 1.19 安装 Prometheus-Oprator","uri":"https://www.cnsre.cn/posts/210621550392/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210609216215/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n存活探针 Kubelet使用liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness探针将捕获到deadlock，重启处于该状态下的容器，使应用程序在存在bug的情况下依然能够继续运行下去（谁的程序还没几个bug呢）。\nKubelet使用readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态。该信号的作用是控制哪些Pod应该作为service的后端。如果Pod处于非就绪状态，那么它们将会被从service的load balancer中移除。\n重启策略 （RestartPolicy ） Always：当容器终止退出后，总是重启容器，默认策略。 OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。 Never：当容器终止退出，从不重启容器。 probe有以下两种类型：\nlivenessProbe：如果检查失败，将杀死容器，根据Pod的restartPolicy来操作。 readinessProbe： 如果检查失败，Kubernetes会把Pod从service endpoints中剔除 Probe支持以下三种检查方法：\nhttpGet：发送HTTP请求，返回200-400范围状态码为成功。 exec：执行Shell命令返回状态码是0为成功。 tcpSocket：发起TCP Socket建立成功。 健康检查的方法 httpGet tcpSocket 方法一 httpGet nginx使用httpGet健康检查的方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 复制代码apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:1.19 ports: - containerPort: 80 resources: requests: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /index.html port: 80 initialDelaySeconds: 10 #pod启动10秒执行第一次检查 periodSeconds: 5 #第一次检查后每隔5秒检查一次** volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html hostPath: path: /home/k8s/data/nginx --- apiVersion: v1 kind: Service metadata: name: nginx spec: type: NodePort ports: - port: 8080 nodePort: 30080 selector: app: nginx 方法二：tcpSocket nginx使用tcpSocket健康检查的方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 apiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 1 selector: matchLabels: app: nginx minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: name: nginx labels: app: nginx spec: containers: - name: nginx image: nginx:1.19 ports: - containerPort: 80 resources: requests: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;500Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 10 periodSeconds: 5 volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html hostPath: path: /home/k8s/data/nginx --- apiVersion: v1 kind: Service metadata: name: nginx spec: type: NodePort ports: - port: 8080 nodePort: 30080 selector: app: nginx 滚动发布 k8s创建副本应用程序的最佳方法就是部署(Deployment)，部署自动创建副本集(ReplicaSet)，副本集可以精确地控制每次替换的Pod数量，从而可以很好的实现滚动更新。具体来说，k8s每次使用一个新的副本控制器(replication controller)来替换已存在的副本控制器，从而始终使用一个新的Pod模板来替换旧的pod模板。\n步骤如下:\n创建一个新的replication controller。 增加或减少pod副本数量，直到满足当前批次期望的数量。 删除旧的replication controller 实战练习 Q： 假如一个nginx服务 起了3个pod，我做了发版，然后没想到这次发版有问题。健康检查也没过！这种情况下发布，会把3个pod都替换掉？还是说检测到一个pod健康检查有问题，下一个就pod就不发布了？已经发布的pod会对外提供服务吗？\n带着这些疑问，来做个实验。\n创建nginx服务 创建一个nginx服务，选择1个副本。\n具体yaml如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: wenlongxue/xxxx:6f965dd # 需要修改自己的镜像 imagePullPolicy: Always ports: - containerPort: 80 resources: requests: memory: \u0026#34;600Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;600Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; readinessProbe: httpGet: path: /actuator/health port: 80 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 --- apiVersion: v1 kind: Service metadata: name: nginx-service labels: app: nginx spec: selector: app: nginx ports: - name: nginx-port protocol: TCP port: 80 nodePort: 30081 targetPort: 80 type: NodePort 查看当前pod 另起一个窗口\n1 kubectl get pods -l app=nginx -w 发布健康检查有问题的版本 修改yaml\n将副本数replicas: 1 设置为replicas: 3\n将健康检查path: / 修改为一个不存在的静态页面，这样健康检查就不过了。\n具体代码\n1 2 3 4 5 6 7 8 9 10 11 12 ... spec: replicas: 3 ... readinessProbe: httpGet: path: /cnsre.html port: 80 periodSeconds: 5 timeoutSeconds: 3 successThreshold: 1 failureThreshold: 30 执行滚动发布并观察 执行更新滚动发布\n1 kubectl apply -f nginx.yaml 查看pod状态\n1 kubectl get pods -l app=nginx 观察滚动更新过程\n1 kubectl get pods -l app=nginx -w 健康检查滚动发布实验结果 通过实验发现，新发版，健康检查没过！这种情况下发布，会把之前的pod保留，新发布的pod 运行，但是不对外提供服务。\n带着这些疑问，来做个实验。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210609216215/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"kubernetes/k8s 使用探针存货实现不停机滚动发布","id":57,"section":"posts","tags":["kubernetes"],"title":"k8s 存活探针，滚动更新","uri":"https://www.cnsre.cn/posts/210609216215/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210608607296/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n环境需求 kubernetes 未安装参考使用kubeadm安装kubernetes 1.21 jenkins github/gitee/gitlab 静态页面 镜像仓库(我使用的 hub.docker)\n如果你未准备好的以上环境，点击链接参考文章进行安装。 安装 jenkins 我这边的jenkins是将jenkins 安装在了k8s的宿主机中，并没有用容器运行。\n安装JDK 1 yum install -y java 安装jenkins 方法1 方法2 方法1 添加Jenkins库到yum库，Jenkins将从这里下载安装。\n1 2 3 wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key yum install -y jenkins 方法2 如果不能安装就到官网下载jenkis的rmp包，官网地址（http://pkg.jenkins-ci.org/redhat-stable/）\n1 2 wget http://pkg.jenkins-ci.org/redhat-stable/jenkins-2.7.3-1.1.noarch.rpm rpm -ivh jenkins-2.7.3-1.1.noarch.rpm 安装 Jenkins 插件 安装完jenkins，设置完密码，安装完默认插件以后，我们需要安装 Pipeline 需要的插件。\n修改下方地址 然后提交\n1 2 3 # jenkins插件清华大学镜像地址 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json http://mirror.xmission.com/jenkins/updates/update-center.json 安装git和pipeline插件\n创建 jenkins Pipeline 添加Pipeline job\n创建job完成后，配置Pipeline script，点击Pipeline语法，来自动生成我们需要的配置。\n如下图，我们Git方式，配置Git仓库地址，再添加认证相关。\n分别填写github/gitee的账号和密码然后点击添加\n添加完成后，在凭据中，选中刚添加的，这样我们就可以生成Pipeline脚本了。点击下方生成流水线脚本，然后复制方框内的内容。\n编写 Pipeline 脚本 编写我们所需要的Pipeline脚本如下，将其粘贴到script的拉取代码模块中，并修改分支master为${branch}，其他模块内容自行编写。\n文章链接\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 node { stage(\u0026#39;拉取代码\u0026#39;) { //拉取代码 git credentialsId: \u0026#39;b34b0b78-e99f-4aa0-a616-xxxxxx\u0026#39;, url: \u0026#39;https://gitee.com/xxx/xxxxx.git\u0026#39; //将上次打包的代删除掉，并将拉取到的新代码代码打包 sh \u0026#39;rm -rf *.tar \u0026amp;\u0026amp; cd waynex \u0026amp;\u0026amp; tar -cvf waynex.tar ./*\u0026#39; script { //获取comment id env.imageTag = sh (script: \u0026#39;git rev-parse --short HEAD ${GIT_COMMIT}\u0026#39;, returnStdout: true).trim() } } // 项目打包到镜像并推送到镜像仓库 stage(\u0026#39;构建推送镜像\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; REPOSITORY=wenlongxue/waynex:${imageTag} cat \u0026gt; Dockerfile \u0026lt;\u0026lt; EOF FROM wenlongxue/nginx ## 设置工作目录 WORKDIR /usr/local/nginx/html RUN rm -rf /usr/local/nginx/html/* ADD waynex/waynex.tar /usr/local/nginx/html/ ## 启动nginx CMD [\u0026#34;nginx\u0026#34;,\u0026#34;-g\u0026#34;,\u0026#34;daemon off;\u0026#34;] EOF docker build -t waynex:${imageTag} . docker login docker tag waynex:${imageTag} $REPOSITORY docker push $REPOSITORY \u0026#39;\u0026#39;\u0026#39; } // 部署到k8s主机 stage(\u0026#39;部署到k8s\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; REPOSITORY=wenlongxue/waynex:${imageTag} cat \u0026gt; nginx.yaml \u0026lt;\u0026lt; EOF apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment #该配置的类型，我们使用的是 Deployment metadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签 spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx minReadySeconds: 1 progressDeadlineSeconds: 60 revisionHistoryLimit: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: \\$REPOSITORY #使用之前推送的镜像 imagePullPolicy: Always #总是拉取最新的。默认为,本地有则使用本地镜像,不拉取 ports: - containerPort: 80 resources: requests: memory: \u0026#34;600Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;600Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 #pod启动10秒执行第一次检查 periodSeconds: 5 #第一次检查后每隔5秒检查一次 --- apiVersion: v1 kind: Service metadata: #译名为元数据，即Deployment的一些基本属性和信息 name: nginx-service #Service 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组 app: nginx #为该Deployment设置key为app，value为nginx的标签 spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 30081 #通过任意节点的 30080 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer EOF sudo kubectl apply -f nginx.yaml \u0026#39;\u0026#39;\u0026#39; } } 在Pipeline脚本里面我们指定了一个branch参数，所以我们需要传递一个参数变量，这里我们选择参数化构建，默认值为master分支。\n然后保存配置。\n发布测试 回到主界面，我们开始构建任务：\n查看构建成功后的图形构建过程：\n通过浏览器来访问项目：http://你的IP:30080/\n至此部署完成\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210608607296/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"k8s jenkins Pipeline部署Nginx","id":58,"section":"posts","tags":["jenkins","pipeline","kubernetes"],"title":"kubernetes使用jenkins Pipeline 部署Nginx","uri":"https://www.cnsre.cn/posts/210608607296/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210603028465/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n部署 nginx Deployment 如果你已经完成了Kubernetes的搭建，那我跟我一块来部署第一个应用程序吧。没有完成 Kubernetes 集群搭建的，请参考文档 使用 kubeadm 安装 kubernetes 1.21\n创建 YAML 文件 创建文件 nginx-deploy.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1\t#与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment\t#该配置的类型，我们使用的是 Deployment metadata:\t#译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment\t#Deployment 的名称 labels:\t#标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx\t#为该Deployment设置key为app，value为nginx的标签 spec:\t#这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1\t#使用该Deployment创建一个应用程序实例 selector:\t#标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template:\t#这是选择或创建的Pod的模板 metadata:\t#Pod的元数据 labels:\t#Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec:\t#期望Pod实现的功能（即在pod中部署） containers:\t#生成container，与docker中的container是同一种 - name: nginx\t#container的名称 image: nginx:1.7.9\t#使用镜像nginx:1.7.9创建container，该container默认80端口可访问 应用 YAML 文件\n1 kubectl apply -f nginx-deploy.yaml 查看部署结果\n1 2 3 4 5 # 查看 Deployment kubectl get deployments # 查看 Pod kubectl get pods 如上图可分别查看到一个名为 nginx-deployment 的 Deployment 和一个名为 nginx-deployment-xxxxxxx 的 Pod\nkubectl 常用命令 kubectl get 显示资源列表\n1 2 3 4 5 6 7 8 9 10 # kubectl get 资源类型 #获取类型为Deployment的资源列表 kubectl get deployments #获取类型为Pod的资源列表 kubectl get pods #获取类型为Node的资源列表 kubectl get nodes 名称空间 在命令后增加 -A 或 \u0026ndash;all-namespaces 可查看所有 名称空间中 的对象，使用参数 -n 可查看指定名称空间的对象，例如\n1 2 3 4 5 # 查看所有名称空间的 Deployment kubectl get deployments -A kubectl get deployments --all-namespaces # 查看 kube-system 名称空间的 Deployment kubectl get deployments -n kube-system kubectl describe 显示有关资源的详细信息\n1 2 3 4 5 6 7 # kubectl describe 资源类型 资源名称 #查看名称为nginx-XXXXXX的Pod的信息 kubectl describe pod nginx-XXXXXX\t#查看名称为nginx的Deployment的信息 kubectl describe deployment nginx\tkubectl logs 查看pod中的容器的打印日志\n1 2 3 4 5 # kubectl logs Pod名称 #查看名称为nginx-pod-XXXXXXX的Pod内的容器打印的日志 #本案例中的 nginx-pod 没有输出日志，所以您看到的结果是空的 kubectl logs -f nginx-pod-XXXXXXX kubectl exec 在pod中的容器环境内执行命令\n1 2 3 4 # kubectl exec Pod名称 操作命令 # 在名称为nginx-pod-xxxxxx的Pod中运行bash kubectl exec -it nginx-pod-xxxxxx /bin/bash 为nginx Deployment 创建 Service 创建文件 nginx-service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Service metadata:\t#译名为元数据，即Deployment的一些基本属性和信息 name: nginx-service\t#Service 的名称 labels:\t#标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组 app: nginx\t#为该Deployment设置key为app，value为nginx的标签 spec:\t#这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector:\t#标签选择器 app: nginx\t#选择包含标签 app:nginx 的 Pod ports: - name: nginx-port\t#端口的名字 protocol: TCP\t#协议类型 TCP/UDP port: 80\t#集群内的其他容器组可通过 80 端口访问 Service nodePort: 30080 #通过任意节点的 30080 端口访问 Service targetPort: 80\t#将请求转发到匹配 Pod 的 80 端口 type: NodePort\t#Serive的类型，ClusterIP/NodePort/LoaderBalancer 执行命令\n1 kubectl apply -f nginx-service.yaml 检查执行结果\n1 kubectl get services -o wide 如上图可查看到名称为 nginx-service 的服务。\n访问服务 1 curl \u0026lt;任意节点的 IP\u0026gt;:30080 伸缩应用程序 伸缩的实现可以通过更改 nginx-deployment.yaml 文件中部署的 replicas（副本数）来完成\n1 2 spec: replicas: 2 #使用该Deployment创建两个应用程序实例 修改 nginx-deploy.yaml 文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本 kind: Deployment #该配置的类型，我们使用的是 Deployment metadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签 spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 2 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 执行命令\n1 kubectl apply -f nginx-deployment.yaml 查看结果\n1 watch kubectl get pods -o wide 如上图，你将会看到有两个应用程序在运行，运行了多个应用程序实例，可以在不停机的情况下执行滚动更新。\n滚动更新 滚动更新允许以下操作：\n将应用程序从准上线环境升级到生产环境（通过更新容器镜像） 回滚到以前的版本 持续集成和持续交付应用程序，无需停机\n如果需要滚动更新执行如下命令即可 1 kubectl apply -f nginx-deployment.yaml 查看过程及结果\n执行命令，可观察到 pod 逐个被替换的过程。\n1 watch kubectl get pods -l app=nginx 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210603028465/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"使用kubernetes|k8s部署一个应用程序","id":59,"section":"posts","tags":["kubernetes"],"title":"kubernetes部署一个应用程序","uri":"https://www.cnsre.cn/posts/210603028465/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210602036084/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n配置要求 至少2台 2核4G 的服务器 本文档中，CPU必须为 x86架构 CentOS 7.8 或 CentOS Stream 8 安装后的软件版本为 Kubernetes v1.21.x calico 3.17.1 nginx-ingress 1.9.1 Containerd.io 1.4.3 操作系统兼容性 CentOS版本 本文档是否兼容 备注 CentOS Stream 8 😄 已验证 CentOS 7.8 😄 已验证 CentOS 7.7 😞 未验证 CentOS 7.6 😞 未验证 Kubernetes v1.21 开始，默认移除 docker 的依赖，如果宿主机上安装了 docker 和 containerd，将优先使用 docker 作为容器运行引擎，如果宿主机上未安装 docker 只安装了 containerd，将使用 containerd 作为容器运行引擎； 本文使用 containerd 作为容器运行引擎； kubeadm 是 Kubernetes 官方支持的安装方式，“二进制” 不是。本文档采用 kubernetes.io 官方推荐的 kubeadm 工具安装 kubernetes 集群。 检查 centos / hostname 1 2 3 4 5 6 7 8 9 10 11 # 在 master 节点和 worker 节点都要执行 cat /etc/redhat-release # 此处 hostname 的输出将会是该机器在 Kubernetes 集群中的节点名字 # 不能使用 localhost 作为节点的名字 hostname # 请使用 lscpu 命令，核对 CPU 信息 # Architecture: x86_64 本安装文档不支持 arm 架构 # CPU(s): 2 CPU 内核数量不能低于 2 lscpu 修改 hostname 1 2 3 4 5 6 # 修改 hostname hostnamectl set-hostname your-new-host-name # 查看修改结果 hostnamectl status # 设置 hostname 解析 echo \u0026#34;127.0.0.1 $(hostname)\u0026#34; \u0026gt;\u0026gt; /etc/hosts 检查网络 在所有节点执行命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [root@demo-master-a-1 ~]$ ip route show default via 172.21.0.1 dev eth0 169.254.0.0/16 dev eth0 scope link metric 1002 172.21.0.0/20 dev eth0 proto kernel scope link src 172.21.0.12 [root@demo-master-a-1 ~]$ ip address 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:16:3e:12:a4:1b brd ff:ff:ff:ff:ff:ff inet 172.17.216.80/20 brd 172.17.223.255 scope global dynamic eth0 valid_lft 305741654sec preferred_lft 305741654sec ip route show 命令中，可以知道机器的默认网卡，通常是 eth0，如 default via 172.21.0.23 dev eth0 ip address 命令中，可显示默认网卡的 IP 地址，Kubernetes 将使用此 IP 地址与集群内的其他节点通信，如 172.17.216.80 所有节点上 Kubernetes 所使用的 IP 地址必须可以互通（无需 NAT 映射、无安全组或防火墙隔离） 安装 使用 root 身份在所有节点执行如下代码，以安装软件：\ncontainerd nfs-utils kubectl / kubeadm / kubelet 快速安装 手动安装 请将脚本最后的 1.21.0 替换成您需要的版本号（必须是 1.21 的小版本，不能是 1.19.1 等） 脚本中间的 v1.21.x 不要替换\ndocker hub 镜像请根据自己网络的情况任选一个\n第四行为腾讯云 docker hub 镜像 第六行为DaoCloud docker hub 镜像 第八行为华为云 docker hub 镜像 第十行为阿里云 docker hub 镜像 1 2 3 4 5 6 7 8 9 10 11 # 在 master 节点和 worker 节点都要执行 # 最后一个参数 1.21.0 用于指定 kubenetes 版本，支持所有 1.21.x 版本的安装 # 腾讯云 docker hub 镜像 # export REGISTRY_MIRROR=\u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; # DaoCloud 镜像 # export REGISTRY_MIRROR=\u0026#34;http://f1361db2.m.daocloud.io\u0026#34; # 华为云镜像 # export REGISTRY_MIRROR=\u0026#34;https://05f073ad3c0010ea0f4bc00b7105ec20.mirror.swr.myhuaweicloud.com\u0026#34; # 阿里云 docker hub 镜像 export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com curl -sSL https://kuboard.cn/install-script/v1.21.x/install_kubelet.sh | sh -s 1.21.0 手动执行以下代码，结果与快速安装相同。 请将脚本第79行（已高亮）的 ${1} 替换成您需要的版本号，例如 1.21.0\ndocker hub 镜像请根据自己网络的情况任选一个\n第四行为腾讯云 docker hub 镜像 第六行为DaoCloud docker hub 镜像 第八行为阿里云 docker hub 镜像 1 2 3 4 5 6 7 8 # 在 master 节点和 worker 节点都要执行 # 最后一个参数 1.21.0 用于指定 kubenetes 版本，支持所有 1.21.x 版本的安装 # 腾讯云 docker hub 镜像 # export REGISTRY_MIRROR=\u0026#34;https://mirror.ccs.tencentyun.com\u0026#34; # DaoCloud 镜像 # export REGISTRY_MIRROR=\u0026#34;http://f1361db2.m.daocloud.io\u0026#34; # 阿里云 docker hub 镜像 export REGISTRY_MIRROR=https://registry.cn-hangzhou.aliyuncs.com 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 #!/bin/bash # 在 master 节点和 worker 节点都要执行 # 安装 containerd # 参考文档如下 # https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Setup required sysctl params, these persist across reboots. cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply sysctl params without reboot sysctl --system # 卸载旧版本 yum remove -y containerd.io # 设置 yum repository yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # 安装 containerd yum install -y containerd.io-1.4.3 mkdir -p /etc/containerd containerd config default \u0026gt; /etc/containerd/config.toml sed -i \u0026#34;s#k8s.gcr.io#registry.aliyuncs.com/k8sxio#g\u0026#34; /etc/containerd/config.toml sed -i \u0026#39;/containerd.runtimes.runc.options/a\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ SystemdCgroup = true\u0026#39; /etc/containerd/config.toml sed -i \u0026#34;s#https://registry-1.docker.io#${REGISTRY_MIRROR}#g\u0026#34; /etc/containerd/config.toml systemctl daemon-reload systemctl enable containerd systemctl restart containerd # 安装 nfs-utils # 必须先安装 nfs-utils 才能挂载 nfs 网络存储 yum install -y nfs-utils yum install -y wget # 关闭 防火墙 systemctl stop firewalld systemctl disable firewalld # 关闭 SeLinux setenforce 0 sed -i \u0026#34;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#34; /etc/selinux/config # 关闭 swap swapoff -a yes | cp /etc/fstab /etc/fstab_bak cat /etc/fstab_bak |grep -v swap \u0026gt; /etc/fstab # 配置K8S的yum源 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 卸载旧版本 yum remove -y kubelet kubeadm kubectl # 安装kubelet、kubeadm、kubectl # 将 ${1} 替换为 kubernetes 版本号，例如 1.20.1 yum install -y kubelet-${1} kubeadm-${1} kubectl-${1} crictl config runtime-endpoint /run/containerd/containerd.sock # 重启 docker，并启动 kubelet systemctl daemon-reload systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet containerd --version kubelet --version 如果此时执行 systemctl status kubelet 命令，将得到 kubelet 启动失败的错误提示，请忽略此错误，因为必须完成后续步骤中 kubeadm init 的操作，kubelet 才能正常启动 初始化 master 节点 APISERVER_NAME 不能是 master 的 hostname APISERVER_NAME 必须全为小写字母、数字、小数点，不能包含减号 POD_SUBNET 所使用的网段不能与 master节点/worker节点 所在的网段重叠。该字段的取值为一个 CIDR 值，如果您对 CIDR 这个概念还不熟悉，请仍然执行 export POD_SUBNET=10.100.0.1/16 命令，不做修改 快速初始化 手动初始化 请将脚本最后的 1.21.0 替换成您需要的版本号（必须是 1.21 的小版本，不能是 1.19.1 等） 脚本中间的 v1.21.x 不要替换\n1 2 3 4 5 6 7 8 9 10 # 只在 master 节点执行 # 替换 x.x.x.x 为 master 节点实际 IP（请使用内网 IP） # export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令 export MASTER_IP=x.x.x.x # 替换 apiserver.demo 为 您想要的 dnsName export APISERVER_NAME=apiserver.demo # Kubernetes 容器组所在的网段，该网段安装完成后，由 kubernetes 创建，事先并不存在于您的物理网络中 export POD_SUBNET=10.100.0.1/16 echo \u0026#34;${MASTER_IP} ${APISERVER_NAME}\u0026#34; \u0026gt;\u0026gt; /etc/hosts curl -sSL https://kuboard.cn/install-script/v1.21.x/init_master.sh | sh -s 1.21.0 手动执行以下代码，结果与快速安装相同。 请将脚本第79行（已高亮）的 ${1} 替换成您需要的版本号，例如 1.21.0\n1 2 3 4 5 6 7 8 9 # 只在 master 节点执行 # 替换 x.x.x.x 为 master 节点的内网IP # export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令 export MASTER_IP=x.x.x.x # 替换 apiserver.demo 为 您想要的 dnsName export APISERVER_NAME=apiserver.demo # Kubernetes 容器组所在的网段，该网段安装完成后，由 kubernetes 创建，事先并不存在于您的物理网络中 export POD_SUBNET=10.100.0.1/16 echo \u0026#34;${MASTER_IP} ${APISERVER_NAME}\u0026#34; \u0026gt;\u0026gt; /etc/hosts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #!/bin/bash # 只在 master 节点执行 # 脚本出错时终止执行 set -e if [ ${#POD_SUBNET} -eq 0 ] || [ ${#APISERVER_NAME} -eq 0 ]; then echo -e \u0026#34;\\033[31;1m请确保您已经设置了环境变量 POD_SUBNET 和 APISERVER_NAME \\033[0m\u0026#34; echo 当前POD_SUBNET=$POD_SUBNET echo 当前APISERVER_NAME=$APISERVER_NAME exit 1 fi # 查看完整配置选项 https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2 rm -f ./kubeadm-config.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; ./kubeadm-config.yaml --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration kubernetesVersion: v${1} imageRepository: registry.aliyuncs.com/k8sxio controlPlaneEndpoint: \u0026#34;${APISERVER_NAME}:6443\u0026#34; networking: serviceSubnet: \u0026#34;10.96.0.0/16\u0026#34; podSubnet: \u0026#34;${POD_SUBNET}\u0026#34; dnsDomain: \u0026#34;cluster.local\u0026#34; dns: type: CoreDNS imageRepository: swr.cn-east-2.myhuaweicloud.com${2} imageTag: 1.8.0 --- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration cgroupDriver: systemd EOF # kubeadm init # 根据您服务器网速的情况，您需要等候 3 - 10 分钟 echo \u0026#34;\u0026#34; echo \u0026#34;抓取镜像，请稍候...\u0026#34; kubeadm config images pull --config=kubeadm-config.yaml echo \u0026#34;\u0026#34; echo \u0026#34;初始化 Master 节点\u0026#34; kubeadm init --config=kubeadm-config.yaml --upload-certs # 配置 kubectl rm -rf /root/.kube/ mkdir /root/.kube/ cp -i /etc/kubernetes/admin.conf /root/.kube/config # 安装 calico 网络插件 # 参考文档 https://docs.projectcalico.org/v3.13/getting-started/kubernetes/self-managed-onprem/onpremises echo \u0026#34;\u0026#34; echo \u0026#34;安装calico-3.17.1\u0026#34; rm -f calico-3.17.1.yaml kubectl create -f https://kuboard.cn/install-script/v1.21.x/calico-operator.yaml wget https://kuboard.cn/install-script/v1.21.x/calico-custom-resources.yaml sed -i \u0026#34;s#192.168.0.0/16#${POD_SUBNET}#\u0026#34; calico-custom-resources.yaml kubectl create -f calico-custom-resources.yaml 检查 master 初始化结果 1 2 3 4 5 6 7 # 只在 master 节点执行 # 执行如下命令，等待 3-10 分钟，直到所有的容器组处于 Running 状态 watch kubectl get pod -n kube-system -o wide # 查看 master 节点初始化结果 kubectl get nodes -o wide 初始化 worker节点 获得 join命令参数 在 master 节点上执行\n1 2 3 4 # 只在 master 节点执行 kubeadm token create --print-join-command # 返回如下 kubeadm join apiserver.demo:6443 --token 9ukzcs.qp4ozxbn1knc13sx --discovery-token-ca-cert-hash sha256:0cb945ea0c6329b4df58cf358e2be697fd31a85f55c23d47356f06f69a27283d 该 token 的有效时间为 2 个小时，2小时内，您可以使用此 token 初始化任意数量的 worker 节点。 初始化worker 针对所有的 worker 节点执行\n1 2 3 4 5 6 7 8 9 # 只在 worker 节点执行 # 替换 x.x.x.x 为 master 节点的内网 IP export MASTER_IP=x.x.x.x # 替换 apiserver.demo 为初始化 master 节点时所使用的 APISERVER_NAME export APISERVER_NAME=apiserver.demo echo \u0026#34;${MASTER_IP} ${APISERVER_NAME}\u0026#34; \u0026gt;\u0026gt; /etc/hosts # 替换为 master 节点上 kubeadm token create 命令的输出 kubeadm join apiserver.demo:6443 --token 9ukzcs.qp4ozxbn1knc13sx --discovery-token-ca-cert-hash sha256:0cb945ea0c6329b4df58cf358e2be697fd31a85f55c23d47356f06f69a27283d 检查初始化结果 在 master 节点上执行\n1 2 3 4 5 6 7 # 只在 master 节点执行 kubectl get nodes -o wide # 输出结果如下所示 [root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 54m v1.21.0 node1 Ready \u0026lt;none\u0026gt; 35m v1.21.0 安装 Ingress Controller 快速安装 卸载IngressController 在 master 节点上执行\n部分情况下，下面的这条指令，您需要执行两次才能成功。\n1 2 # 只在 master 节点执行 kubectl apply -f https://kuboard.cn/install-script/v1.21.x/nginx-ingress.yaml 在 master 节点上执行\n只在您想选择其他 Ingress Controller 的情况下卸载\n1 2 # 只在 master 节点执行 kubectl delete -f https://kuboard.cn/install-script/v1.21.x/nginx-ingress.yaml 配置域名解析 将域名 cnsre.cn 解析到 demo-worker-a-2 的 IP 地址 z.z.z.z （也可以是 demo-worker-a-1 的地址 y.y.y.y）\n验证配置 在浏览器访问 cnsre.cn，将得到 404 NotFound 错误页面\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210602036084/\n相关话题：https://www.cnsre.cn/tags/kubernetes/\n","description":"使用kubeadm安装kubernetes 1.21","id":60,"section":"posts","tags":["kubeadm","kubernetes"],"title":"使用kubeadm安装kubernetes 1.21","uri":"https://www.cnsre.cn/posts/210602036084/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210531132227/\n相关话题：https://www.cnsre.cn/tags/shell/\n文件类型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ls -rtl # 按时间倒叙列出所有目录和文件 ll -rt touch file # 创建空白文件 rm -rf dirname # 不提示删除非空目录(-r:递归删除 -f强制) dos2unix # windows文本转linux文本 unix2dos # linux文本转windows文本 enca filename # 查看编码 安装 yum install -y enca md5sum # 查看md5值 ln sourcefile newfile # 硬链接 ln -s sourcefile newfile # 符号连接 readlink -f /data # 查看连接真实目录 cat file | nl |less # 查看上下翻页且显示行号 q退出 head # 查看文件开头内容 head -c 10m # 截取文件中10M内容 split -C 10M # 将文件切割大小为10M -C按行 tail -f file # 查看结尾 监视日志文件 tail -F file # 监视日志并重试, 针对文件被mv的情况可以持续读取 file # 检查文件类型 umask # 更改默认权限 uniq # 删除重复的行 uniq -c # 重复的行出现次数 uniq -u # 只显示不重复行 paste a b # 将两个文件合并用tab键分隔开 paste -d\u0026#39;+\u0026#39; a b # 将两个文件合并指定\u0026#39;+\u0026#39;符号隔开 paste -s a # 将多行数据合并到一行用tab键隔开 chattr +i /etc/passwd # 不得任意改变文件或目录 -i去掉锁 -R递归 more # 向下分面器 locate aaa # 搜索 wc -l file # 查看行数 cp filename{,.bak} # 快速备份一个文件 cp a b # 拷贝不提示 既不使用别名 cp -i rev # 将行中的字符逆序排列 comm -12 2 3 # 行和行比较匹配 echo \u0026#34;10.45aa\u0026#34; |cksum # 字符串转数字编码，可做校验，也可用于文件校验 iconv -f gbk -t utf8 source.txt \u0026gt; new.txt # 转换编码 xxd /boot/grub/stage1 # 16进制查看 hexdump -C /boot/grub/stage1 # 16进制查看 rename source new file # 重命名 可正则 watch -d -n 1 \u0026#39;df; ls -FlAt /path\u0026#39; # 实时某个目录下查看最新改动过的文件 cp -v /dev/dvd /rhel4.6.iso9660 # 制作镜像 diff suzu.c suzu2.c \u0026gt; sz.patch # 制作补丁 patch suzu.c \u0026lt; sz.patch # 安装补丁 sort排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 -t # 指定排序时所用的栏位分隔字符 -n # 依照数值的大小排序 -r # 以相反的顺序来排序 -f # 排序时，将小写字母视为大写字母 -d # 排序时，处理英文字母、数字及空格字符外，忽略其他的字符 -c # 检查文件是否已经按照顺序排序 -b # 忽略每行前面开始处的空格字符 -M # 前面3个字母依照月份的缩写进行排序 -k # 指定域 -m # 将几个排序好的文件进行合并 -T # 指定临时文件目录,默认在/tmp -o # 将排序后的结果存入指定的文 sort -n # 按数字排序 sort -nr # 按数字倒叙 sort -u # 过滤重复行 sort -m a.txt c.txt # 将两个文件内容整合到一起 sort -n -t\u0026#39; \u0026#39; -k 2 -k 3 a.txt # 第二域相同，将从第三域进行升降处理 sort -n -t\u0026#39;:\u0026#39; -k 3r a.txt # 以:为分割域的第三域进行倒叙排列 sort -k 1.3 a.txt # 从第三个字母起进行排序 sort -t\u0026#34; \u0026#34; -k 2n -u a.txt # 以第二域进行排序，如果遇到重复的，就删除 find查找 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # linux文件无创建时间 # Access 使用时间 # Modify 内容修改时间 # Change 状态改变时间(权限、属主) # 时间默认以24小时为单位,当前时间到向前24小时为0天,向前48-72小时为2天 # -and 且 匹配两个条件 参数可以确定时间范围 -mtime +2 -and -mtime -4 # -or 或 匹配任意一个条件 find /etc -name \u0026#34;*http*\u0026#34; # 按文件名查找 find . -type f # 查找某一类型文件 find / -perm # 按照文件权限查找 find / -user # 按照文件属主查找 find / -group # 按照文件所属的组来查找文件 find / -atime -n # 文件使用时间在N天以内 find / -atime +n # 文件使用时间在N天以前 find / -mtime +n # 文件内容改变时间在N天以前 find / -ctime +n # 文件状态改变时间在N天前 find / -mmin +30 # 按分钟查找内容改变 find / -size +1000000c -print # 查找文件长度大于1M字节的文件 find /etc -name \u0026#34;*passwd*\u0026#34; -exec grep \u0026#34;xuesong\u0026#34; {} \\; # 按名字查找文件传递给-exec后命令 find . -name \u0026#39;t*\u0026#39; -exec basename {} \\; # 查找文件名,不取路径 find . -type f -name \u0026#34;err*\u0026#34; -exec rename err ERR {} \\; # 批量改名(查找err 替换为 ERR {}文件 find path -name *name1* -or -name *name2* # 查找任意一个关键字 vim编辑器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 常用配置 set smartindent set tabstop=4 set shiftwidth=4 set expandtab set softtabstop=4 set noautoindent set nosmartindent set paste set clipboard=unnamed gconf-editor # 配置编辑器 /etc/vimrc # 配置文件路径 vim +24 file # 打开文件定位到指定行 vim file1 file2 # 打开多个文件 vim -r file # 恢复上次异常关闭的文件 .file.swp vim -O2 file1 file2 # 垂直分屏 vim -on file1 file2 # 水平分屏 Ctrl+ U # 向前翻页 Ctrl+ D # 向后翻页 Ctrl+ww # 在窗口间切换 Ctrl+w +or-or= # 增减高度 :sp filename # 上下分割打开新文件 :vs filename # 左右分割打开新文件 :set nu # 打开行号 :set nonu # 取消行号 :nohl # 取消高亮 :set paste # 取消缩进 :set autoindent # 设置自动缩进 :set ff # 查看文本格式 :set binary # 改为unix格式 :%s/str/newstr/g # 全部替换 :200 # 跳转到200 1 文件头 G # 跳到行尾 dd # 删除当前行 并复制 可直接p粘贴 11111dd # 删除11111行，可用来清空文件 r # 替换单个字符 R # 替换多个字符 u # 撤销上次操作 * # 全文匹配当前光标所在字符串 $ # 行尾 0 # 行首 X # 文档加密 v = # 自动格式化代码 Ctrl+v # 可视模式 Ctrl+v I ESC # 多行操作 Ctrl+v s ESC # 批量取消注释 归档解压缩 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 tar zxvpf gz.tar.gz dir # 解包指定tar.gz中的内容 不指定目录则全解压 tar zcvpf /$path/gz.tar.gz * # 打包gz 注意*最好用相对路径 tar zcf /$path/gz.tar.gz * # 打包正确不提示 tar ztvpf gz.tar.gz # 查看gz tar xvf 1.tar -C dir # 解包tar 放到指定目录 tar -cvf 1.tar * # 打包tar tar tvf 1.tar # 查看tar tar -rvf 1.tar filename # 给tar追加文件 tar --exclude=/home/dmtsai --exclude=*.tar -zcvf myfile.tar.gz /home/* /etc # 打包/home, /etc ，但排除 /home/dmtsai tar -N \u0026#34;2005/06/01\u0026#34; -zcvf home.tar.gz /home # 在 /home 当中，比 2005/06/01 新的文件才备份 tar -zcvfh home.tar.gz /home # 打包目录中包括连接目录 tar zcf - ./ | ssh root@IP \u0026#34;tar zxf - -C /xxxx\u0026#34; # 一边压缩一边解压 zgrep str 1.gz # 查看压缩包中文件字符行 bzip2 -dv 1.tar.bz2 # 解压bzip2 bzip2 -v 1.tar # bzip2压缩 bzcat # 查看bzip2 gzip file # 直接压缩文件 # 压缩后源文件消失 gunzip file.gz # 直接解压文件 # 解压后源文件消失 gzip -r dir/ # 递归压缩目录 gzip -r -d dir/ # 递归解压目录 gzip -dv 1.tar.gz # 解压gzip到tar gzip -v 1.tar # 压缩tar到gz unzip zip.zip # 解压zip zip zip.zip * # 压缩zip rar a rar.rar *.jpg # 压缩文件为rar包 unrar x rar.rar # 解压rar包 SVN GIT SVN GIT SVN 1 2 3 4 5 --force # 强制覆盖 /usr/bin/svn --username user --password passwd co $Code ${SvnPath}src/ # 检出整个项目 /usr/bin/svn --username user --password passwd up $Code ${SvnPath}src/ # 更新项目 /usr/bin/svn --username user --password passwd export $Code$File ${SvnPath}src/$File # 导出个别文件 /usr/bin/svn --username user --password passwd export -r 版本号 svn路径 本地路径 --force # 导出指定版本 GIT 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 git clone git@10.10.10.10:gittest.git ./gittest/ # 克隆项目到指定目录 git clone -b develop --depth=1 http://git.a.com/d.git # 克隆指定分支 克隆一层 git status # Show the working tree(工作树) status git log -n 1 --stat # 查看最后一次日志文件 git branch -a # 列出远程跟踪分支(remote-tracking branches)和本地分支 git checkout developing # 切换到developing分支 git checkout -b release # 切换分支没有从当前分支创建 git checkout -b release origin/master # 从远程分支创建本地镜像分支 git push origin --delete release # 从远端删除分区，服务端有可能设置保护不允许删除 git push origin release # 把本地分支提交到远程 git pull # 更新项目 需要cd到项目目录中 git fetch -f -p # 抓取远端代码但不合并到当前 git reset --hard origin/master # 和远端同步分支 git add . # 更新所有文件 git commit -m \u0026#34;gittest up\u0026#34; # 提交操作并添加备注 git push # 正式提交到远程git服务器 git push [-u origin master] # 正式提交到远程git服务器(master分支) git tag [-a] dev-v-0.11.54 [-m \u0026#39;fix #67\u0026#39;] # 创建tag,名为dev-v-0.11.54,备注fix #67 git tag -l dev-v-0.11.54 # 查看tag(dev-v-0.11.5) git push origin --tags # 提交tag git reset --hard # 本地恢复整个项目 git rm -r -n --cached ./img # -n执行命令时,不会删除任何文件,而是展示此命令要删除的文件列表预览 git rm -r --cached ./img # 执行删除命令 需要commit和push让远程生效 git init --bare smc-content-check.git # 初始化新git项目 需要手动创建此目录并给git用户权限 chown -R git:git smc-content-check.git git config --global credential.helper store # 记住密码 git config [--global] user.name \u0026#34;your name\u0026#34; # 设置你的用户名, 希望在一个特定的项目中使用不同的用户或e-mail地址, 不要--global选项 git config [--global] user.email \u0026#34;your email\u0026#34; # 设置你的e-mail地址, 每次Git提交都会使用该信息 git config [--global] user.name # 查看用户名 git config [--global] user.email # 查看用户e-mail git config --global --edit # 编辑~/.gitconfig(User-specific)配置文件, 值优先级高于/etc/gitconfig(System-wide) git config --edit # 编辑.git/config(Repository specific)配置文件, 值优先级高于~/.gitconfig git cherry-pick \u0026lt;commit id\u0026gt; # 用于把另一个本地分支的commit修改应用到当前分支 需要push到远程 git log --pretty=format:\u0026#39;%h: %s\u0026#39; 9378b62..HEAD # 查看指定范围更新操作 commit id git config --global core.ignorecase false # 设置全局大小写敏感 git ls-remote --heads origin refs/heads/test # 查看 从远端拉一份新的{ # You have not concluded your merge (MERGE_HEAD exists) git拉取失败 git fetch --hard origin/master git reset --hard origin/master } 删除远程分支并新建{ git checkout master git branch -r -d origin/test # 删除远程分支 但有时候并没有删除 可以尝试使用下面的语句 git push origin :test # 推送一个空分支到远程分支，相当于删除远程分支 git branch -d test # 删除本地test分支, -D 强制 git branch -a |grep test git checkout -b test git push origin test git reset --hard origin/test } 迁移git项目{ git branch -r | grep -v \u0026#39;\\-\u0026gt;\u0026#39; | while read remote; do git branch --track \u0026#34;${remote#origin/}\u0026#34; \u0026#34;$remote\u0026#34;; done git fetch --all git pull --all git remote set-url origin git@git.github.cn:server/gw.git git push --all } 恢复rm删除的文件 1 2 3 4 5 6 # debugfs针对 ext2 # ext3grep针对 ext3 # extundelete针对 ext4 df -T # 首先查看磁盘分区格式 umount /data/ # 卸载挂载,数据丢失请首先卸载挂载,或重新挂载只读 ext3grep /dev/sdb1 --ls --inode 2 # 记录信息继续查找目录下文件inode信息 ext3grep /dev/sdb1 --ls --inode 131081 # 此处是inode ext3grep /dev/sdb1 --restore-inode 49153 # 记录下inode信息开始恢复目录 openssl 1 2 3 4 5 6 7 8 openssl rand 15 -base64 # 口令生成 openssl sha1 filename # 哈希算法校验文件 openssl md5 filename # MD5校验文件 openssl base64 filename.txt # base64编码/解码文件(发送邮件附件之类功能会可以使用) openssl base64 -d filename.bin # base64编码/解码二进制文件 openssl enc -aes-128-cbc filename.aes-128-cbc # 加密文档 # 推荐使用的加密算法是bf(Blowfish)和-aes-128-cbc(运行在CBC模式的128位密匙AES加密算法)，加密强度有保障 openssl enc -d -aes-128-cbc -in filename.aes-128-cbc \u0026gt; filename # 解密文档 软件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 rpm{ rpm -ivh lynx # rpm安装 rpm -e lynx # 卸载包 rpm -e lynx --nodeps # 强制卸载 rpm -qa # 查看所有安装的rpm包 rpm -qa | grep lynx # 查找包是否安装 rpm -ql # 软件包路径 rpm -Uvh # 升级包 rpm --test lynx # 测试 rpm -qc # 软件包配置文档 rpm --initdb # 初始化rpm 数据库 rpm --rebuilddb # 重建rpm数据库 在rpm和yum无响应的情况使用 先 rm -f /var/lib/rpm/__db.00* 在重建 } yum{ yum list # 所有软件列表 yum install 包名 # 安装包和依赖包 yum -y update # 升级所有包版本,依赖关系，系统版本内核都升级 yum -y update 软件包名 # 升级指定的软件包 yum -y upgrade # 不改变软件设置更新软件，系统版本升级，内核不改变 yum search mail # yum搜索相关包 yum grouplist # 软件包组 yum -y groupinstall \u0026#34;Virtualization\u0026#34; # 安装软件包组 repoquery -ql gstreamer # 不安装软件查看包含文件 yum clean all # 清除var下缓存 } yum使用epel源{ # 包下载地址: http://download.fedoraproject.org/pub/epel # 选择版本5\\6\\7 rpm -Uvh http://mirrors.hustunique.com/epel//6/x86_64/epel-release-6-8.noarch.rpm # 自适配版本 yum install epel-release } 自定义yum源{ find /etc/yum.repos.d -name \u0026#34;*.repo\u0026#34; -exec mv {} {}.bak \\; vim /etc/yum.repos.d/yum.repo [yum] #http baseurl=http://10.0.0.1/centos5.5 #挂载iso #mount -o loop CentOS-5.8-x86_64-bin-DVD-1of2.iso /data/iso/ #本地 #baseurl=file:///data/iso/ enable=1 #导入key rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-5 } 编译{ 源码安装{ ./configure --help # 查看所有编译参数 ./configure --prefix=/usr/local/ # 配置参数 make # 编译 # make -j 8 # 多线程编译,速度较快,但有些软件不支持 make install # 安装包 make clean # 清除编译结果 } perl程序编译{ perl Makefile.PL make make test make install } python程序编译{ python file.py # 源码包编译安装 python setup.py build python setup.py install } 编译c程序{ gcc -g hello.c -o hello } } 系统 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 wall # 给其它用户发消息 whereis ls # 搜索程序名，而且只搜索二进制文件 which # 查找命令是否存在,及存放位置 locate # 不是实时查找，查找的结果不精确，但查找速度很快 每天更新 /var/lib/locatedb clear # 清空整个屏幕 reset # 重新初始化屏幕 cal # 显示月历 echo -n 123456 | md5sum # md5加密 mkpasswd # 随机生成密码 -l位数 -C大小 -c小写 -d数字 -s特殊字符 netstat -ntupl | grep port # 是否打开了某个端口 ntpdate cn.pool.ntp.org # 同步时间, pool.ntp.org: public ntp time server for everyone(http://www.pool.ntp.org/zh/) tzselect # 选择时区 #+8=(5 9 1 1) # (TZ=\u0026#39;Asia/Shanghai\u0026#39;; export TZ)括号内写入 /etc/profile /sbin/hwclock -w # 时间保存到硬件 /etc/shadow # 账户影子文件 LANG=en # 修改语言 vim /etc/sysconfig/i18n # 修改编码 LANG=\u0026#34;en_US.UTF-8\u0026#34; export LC_ALL=C # 强制字符集 vi /etc/hosts # 查询静态主机名 alias # 别名 watch uptime # 监测命令动态刷新 监视 ipcs -a # 查看Linux系统当前单个共享内存段的最大值 ldconfig # 动态链接库管理命令 ldd `which cmd` # 查看命令的依赖库 dist-upgrade # 会改变配置文件,改变旧的依赖关系，改变系统版本 /boot/grub/grub.conf # grub启动项配置 ps -mfL \u0026lt;PID\u0026gt; # 查看指定进程启动的线程 线程数受 max user processes 限制 ps uxm |wc -l # 查看当前用户占用的进程数 [包括线程] max user processes top -p PID -H # 查看指定PID进程及线程 lsof |wc -l # 查看当前文件句柄数使用数量 open files lsof |grep /lib # 查看加载库文件 sysctl -a # 查看当前所有系统内核参数 sysctl -p # 修改内核参数/etc/sysctl.conf，让/etc/rc.d/rc.sysinit读取生效 strace -p pid # 跟踪系统调用 ps -eo \u0026#34;%p %C %z %a\u0026#34;|sort -k3 -n # 把进程按内存使用大小排序 strace uptime 2\u0026gt;\u0026amp;1|grep open # 查看命令打开的相关文件 grep Hugepagesize /proc/meminfo # 内存分页大小 mkpasswd -l 8 -C 2 -c 2 -d 4 -s 0 # 随机生成指定类型密码 echo 1 \u0026gt; /proc/sys/net/ipv4/tcp_syncookies # 使TCP SYN Cookie 保护生效 # \u0026#34;SYN Attack\u0026#34;是一种拒绝服务的攻击方式 grep Swap /proc/25151/smaps |awk \u0026#39;{a+=$2}END{print a}\u0026#39; # 查询某pid使用的swap大小 redir --lport=33060 --caddr=10.10.10.78 --cport=3306 # 端口映射 yum安装 用supervisor守护 开机启动脚本顺序 1 2 3 4 5 /etc/profile /etc/profile.d/*.sh ~/bash_profile ~/.bashrc /etc/bashrc 进程管理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ps -eaf # 查看所有进程 kill -9 PID # 强制终止某个PID进程 kill -15 PID # 安全退出 需程序内部处理信号 cmd \u0026amp; # 命令后台运行 nohup cmd \u0026amp; # 后台运行不受shell退出影响 ctrl+z # 将前台放入后台(暂停) jobs # 查看后台运行程序 bg 2 # 启动后台暂停进程 fg 2 # 调回后台进程 pstree # 进程树 vmstat 1 9 # 每隔一秒报告系统性能信息9次 sar # 查看cpu等状态 lsof file # 显示打开指定文件的所有进程 lsof -i:32768 # 查看端口的进程 renice +1 180 # 把180号进程的优先级加1 exec sh a.sh # 子进程替换原来程序的pid， 避免supervisor无法强制杀死进程 ps 1 2 3 4 5 6 7 8 9 10 ps aux |grep -v USER | sort -nk +4 | tail # 显示消耗内存最多的10个运行中的进程，以内存使用量排序.cpu +3 # USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND %CPU # 进程的cpu占用率 %MEM # 进程的内存占用率 VSZ # 进程虚拟大小,单位K(即总占用内存大小,包括真实内存和虚拟内存) RSS # 进程使用的驻留集大小即实际物理内存大小 START # 进程启动时间和日期 占用的虚拟内存大小 = VSZ - RSS ps -eo pid,lstart,etime,args # 查看进程启动时间 top 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 前五行是系统整体的统计信息。 第一行: 任务队列信息，同 uptime 命令的执行结果。内容如下： 01:06:48 当前时间 up 1:22 系统运行时间，格式为时:分 1 user 当前登录用户数 load average: 0.06, 0.60, 0.48 系统负载，即任务队列的平均长度。 三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 第二、三行:为进程和CPU的信息。当有多个CPU时，这些内容可能会超过两行。内容如下： Tasks: 29 total 进程总数 1 running 正在运行的进程数 28 sleeping 睡眠的进程数 0 stopped 停止的进程数 0 zombie 僵尸进程数 Cpu(s): 0.3% us 用户空间占用CPU百分比 1.0% sy 内核空间占用CPU百分比 0.0% ni 用户进程空间内改变过优先级的进程占用CPU百分比 98.7% id 空闲CPU百分比 0.0% wa 等待输入输出的CPU时间百分比 0.0% hi 0.0% si 第四、五行:为内存信息。内容如下： Mem: 191272k total 物理内存总量 173656k used 使用的物理内存总量 17616k free 空闲内存总量 22052k buffers 用作内核缓存的内存量 Swap: 192772k total 交换区总量 0k used 使用的交换区总量 192772k free 空闲交换区总量 123988k cached 缓冲的交换区总量。 内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖， 该数值即为这些内容已存在于内存中的交换区的大小。 相应的内存再次被换出时可不必再对交换区写入。 进程信息区,各列的含义如下: # 显示各个进程的详细信息 序号 列名 含义 a PID 进程id b PPID 父进程id c RUSER Real user name d UID 进程所有者的用户id e USER 进程所有者的用户名 f GROUP 进程所有者的组名 g TTY 启动进程的终端名。不是从终端启动的进程则显示为 ? h PR 优先级 i NI nice值。负值表示高优先级，正值表示低优先级 j P 最后使用的CPU，仅在多CPU环境下有意义 k %CPU 上次更新到现在的CPU时间占用百分比 l TIME 进程使用的CPU时间总计，单位秒 m TIME+ 进程使用的CPU时间总计，单位1/100秒 n %MEM 进程使用的物理内存百分比 o VIRT 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES p SWAP 进程使用的虚拟内存中，被换出的大小，单位kb。 q RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA r CODE 可执行代码占用的物理内存大小，单位kb s DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb t SHR 共享内存大小，单位kb u nFLT 页面错误次数 v nDRT 最后一次写入到现在，被修改过的页面数。 w S 进程状态。 D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 父进程在但并不等待子进程 x COMMAND 命令名/命令行 y WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags 任务标志，参考 sched.h inux操作系统提供的信号 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 kill -l # 查看linux提供的信号 trap \u0026#34;echo aaa\u0026#34; 2 3 15 # shell使用 trap 捕捉退出信号 # 发送信号一般有两种原因: # 1(被动式) 内核检测到一个系统事件.例如子进程退出会像父进程发送SIGCHLD信号.键盘按下control+c会发送SIGINT信号 # 2(主动式) 通过系统调用kill来向指定进程发送信号 # 进程结束信号 SIGTERM 和 SIGKILL 的区别: SIGTERM 比较友好，进程能捕捉这个信号，根据您的需要来关闭程序。在关闭程序之前，您可以结束打开的记录文件和完成正在做的任务。在某些情况下，假如进程正在进行作业而且不能中断，那么进程可以忽略这个SIGTERM信号。 # 如果一个进程收到一个SIGUSR1信号，然后执行信号绑定函数，第二个SIGUSR2信号又来了，第一个信号没有被处理完毕的话，第二个信号就会丢弃。 SIGHUP 1 A # 终端挂起或者控制进程终止 SIGINT 2 A # 键盘终端进程(如control+c) SIGQUIT 3 C # 键盘的退出键被按下 SIGILL 4 C # 非法指令 SIGABRT 6 C # 由abort(3)发出的退出指令 SIGFPE 8 C # 浮点异常 SIGKILL 9 AEF # Kill信号 立刻停止 SIGSEGV 11 C # 无效的内存引用 SIGPIPE 13 A # 管道破裂: 写一个没有读端口的管道 SIGALRM 14 A # 闹钟信号 由alarm(2)发出的信号 SIGTERM 15 A # 终止信号,可让程序安全退出 kill -15 SIGUSR1 30,10,16 A # 用户自定义信号1 SIGUSR2 31,12,17 A # 用户自定义信号2 SIGCHLD 20,17,18 B # 子进程结束自动向父进程发送SIGCHLD信号 SIGCONT 19,18,25 # 进程继续（曾被停止的进程） SIGSTOP 17,19,23 DEF # 终止进程 SIGTSTP 18,20,24 D # 控制终端（tty）上按下停止键 SIGTTIN 21,21,26 D # 后台进程企图从控制终端读 SIGTTOU 22,22,27 D # 后台进程企图从控制终端写 缺省处理动作一项中的字母含义如下: A 缺省的动作是终止进程 B 缺省的动作是忽略此信号，将该信号丢弃，不做处理 C 缺省的动作是终止进程并进行内核映像转储(dump core),内核映像转储是指将进程数据在内存的映像和进程在内核结构中的部分内容以一定格式转储到文件系统，并且进程退出执行，这样做的好处是为程序员提供了方便，使得他们可以得到进程当时执行时的数据值，允许他们确定转储的原因，并且可以调试他们的程序。 D 缺省的动作是停止进程，进入停止状况以后还能重新进行下去，一般是在调试的过程中（例如ptrace系统调用） E 信号不能被捕获 F 信号不能被忽略 系统性能状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 vmstat 1 9 r # 等待执行的任务数。当这个值超过了cpu线程数，就会出现cpu瓶颈。 b # 等待IO的进程数量,表示阻塞的进程。 swpd # 虚拟内存已使用的大小，如大于0，表示机器物理内存不足，如不是程序内存泄露，那么该升级内存。 free # 空闲的物理内存的大小 buff # 已用的buff大小，对块设备的读写进行缓冲 cache # cache直接用来记忆我们打开的文件,给文件做缓冲，(把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) inact # 非活跃内存大小，即被标明可回收的内存，区别于free和active -a选项时显示 active # 活跃的内存大小 -a选项时显示 si # 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露，要查找耗内存进程解决掉。 so # 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 bi # 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte bo # 块设备每秒发送的块数量，例如读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 in # 每秒CPU的中断次数，包括时间中断。in和cs这两个值越大，会看到由内核消耗的cpu时间会越多 cs # 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用。 us # 用户进程执行消耗cpu时间(user time) us的值比较高时，说明用户进程消耗的cpu时间多，但是如果长期超过50%的使用，那么我们就该考虑优化程序算法或其他措施 sy # 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 id # 空闲 CPU时间，一般来说，id + us + sy = 100,一般认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 wt # 等待IOCPU时间。Wa过高时，说明io等待比较严重，这可能是由于磁盘大量随机访问造成的，也有可能是磁盘的带宽出现瓶颈。 如果 r 经常大于4，且id经常少于40，表示cpu的负荷很重。 如果 pi po 长期不等于0，表示内存不足。 如果 b 队列经常大于3，表示io性能不好。 日志管理 1 2 3 4 5 6 7 8 9 10 history # 历时命令默认1000条 HISTTIMEFORMAT=\u0026#34;%Y-%m-%d %H:%M:%S \u0026#34; # 让history命令显示具体时间 history -c # 清除记录命令 cat $HOME/.bash_history # 历史命令记录文件 lastb -a # 列出登录系统失败的用户相关信息 清空二进制日志记录文件 echo \u0026gt; /var/log/btmp last # 查看登陆过的用户信息 清空二进制日志记录文件 echo \u0026gt; /var/log/wtmp 默认打开乱码 who /var/log/wtmp # 查看登陆过的用户信息 lastlog # 用户最后登录的时间 tail -f /var/log/messages # 系统日志 tail -f /var/log/secure # ssh日志 man 1 2 3 4 5 6 7 8 9 10 man 2 read # 查看read函数的文档 1 使用者在shell中可以操作的指令或可执行档 2 系统核心可呼叫的函数与工具等 3 一些常用的函数(function)与函数库(library),大部分是C的函数库(libc) 4 装置档案的说明，通常在/dev下的档案 5 设定档或者是某些档案的格式 6 游戏games 7 惯例与协定等，例如linux档案系统、网络协定、ascll code等说明 8 系统管理员可用的管理指令 9 跟kernel有关的文件 selinux 1 2 3 4 5 6 7 sestatus -v # 查看selinux状态 getenforce # 查看selinux模式 setenforce 0 # 设置selinux为宽容模式(可避免阻止一些操作) semanage port -l # 查看selinux端口限制规则 semanage port -a -t http_port_t -p tcp 8000 # 在selinux中注册端口类型 vi /etc/selinux/config # selinux配置文件 SELINUX=enfoceing # 关闭selinux 把其修改为 SELINUX=disabled 查看剩余内存 1 2 3 4 free -m #-/+ buffers/cache: 6458 1649 #6458M为真实使用内存 1649M为真实剩余内存(剩余内存+缓存+缓冲器) #linux会利用所有的剩余内存作为缓存，所以要保证linux运行速度，就需要保证内存的缓存大小 系统信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 uname -a # 查看Linux内核版本信息 cat /proc/version # 查看内核版本 cat /etc/issue # 查看系统版本 lsb_release -a # 查看系统版本 需安装 centos-release locale -a # 列出所有语系 locale # 当前环境变量中所有编码 hwclock # 查看时间 who # 当前在线用户 w # 当前在线用户 whoami # 查看当前用户名 logname # 查看初始登陆用户名 uptime # 查看服务器启动时间 sar -n DEV 1 10 # 查看网卡网速流量 dmesg # 显示开机信息 lsmod # 查看内核模块 硬件信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 more /proc/cpuinfo # 查看cpu信息 lscpu # 查看cpu信息 cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c # 查看cpu型号和逻辑核心数 getconf LONG_BIT # cpu运行的位数 cat /proc/cpuinfo | grep \u0026#39;physical id\u0026#39; |sort| uniq -c # 物理cpu个数 cat /proc/cpuinfo | grep flags | grep \u0026#39; lm \u0026#39; | wc -l # 结果大于0支持64位 cat /proc/cpuinfo|grep flags # 查看cpu是否支持虚拟化 pae支持半虚拟化 IntelVT 支持全虚拟化 more /proc/meminfo # 查看内存信息 dmidecode # 查看全面硬件信息 dmidecode | grep \u0026#34;Product Name\u0026#34; # 查看服务器型号 dmidecode | grep -P -A5 \u0026#34;Memory\\s+Device\u0026#34; | grep Size | grep -v Range # 查看内存插槽 cat /proc/mdstat # 查看软raid信息 cat /proc/scsi/scsi # 查看Dell硬raid信息(IBM、HP需要官方检测工具) lspci # 查看硬件信息 lspci|grep RAID # 查看是否支持raid lspci -vvv |grep Ethernet # 查看网卡型号 lspci -vvv |grep Kernel|grep driver # 查看驱动模块 modinfo tg2 # 查看驱动版本(驱动模块) ethtool -i em1 # 查看网卡驱动版本 ethtool em1 # 查看网卡带宽 开机启动模式 1 2 3 vi /etc/inittab id:3:initdefault: # 3为多用户命令 #ca::ctrlaltdel:/sbin/shutdown -t3 -r now # 注释此行 禁止 ctrl+alt+del 关闭计算机 定时任务 1 2 3 4 5 6 7 8 9 10 11 at 5pm + 3 days /bin/ls # 单次定时任务 指定三天后下午5:00执行/bin/ls crontab -e # 编辑周期任务 #分钟 小时 天 月 星期 命令或脚本 1,30 1-3/2 * * * 命令或脚本 \u0026gt;\u0026gt; file.log 2\u0026gt;\u0026amp;1 echo \u0026#34;40 7 * * 2 /root/sh\u0026#34;\u0026gt;\u0026gt;/var/spool/cron/work # 普通用户可直接写入定时任务 crontab -l # 查看自动周期性任务 crontab -r # 删除自动周期性任务 cron.deny和cron.allow # 禁止或允许用户使用周期任务 service crond start|stop|restart # 启动自动周期性服务 * * * * * echo \u0026#34;d\u0026#34; \u0026gt;\u0026gt;d$(date +\\%Y\\%m\\%d).log # 让定时任务直接生成带日期的log 需要转义% date 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 星期日[SUN] 星期一[MON] 星期二[TUE] 星期三[WED] 星期四[THU] 星期五[FRI] 星期六[SAT] 一月[JAN] 二月[FEB] 三月[MAR] 四月[APR] 五月[MAY] 六月[JUN] 七月[JUL] 八月[AUG] 九月[SEP] 十月[OCT] 十一月[NOV] 十二月[DEC] date -s 20091112 # 设日期 date -s 18:30:50 # 设时间 date -d \u0026#34;7 days ago\u0026#34; +%Y%m%d # 7天前日期 date -d \u0026#34;5 minute ago\u0026#34; +%H:%M # 5分钟前时间 date -d \u0026#34;1 month ago\u0026#34; +%Y%m%d # 一个月前 date -d \u0026#39;1 days\u0026#39; +%Y-%m-%d # 一天后 date -d \u0026#39;1 hours\u0026#39; +%H:%M:%S # 一小时后 date +%Y-%m-%d -d \u0026#39;20110902\u0026#39; # 日期格式转换 date +%Y-%m-%d_%X # 日期和时间 date +%N # 纳秒 date -d \u0026#34;2012-08-13 14:00:23\u0026#34; +%s # 换算成秒计算(1970年至今的秒数) date -d \u0026#34;@1363867952\u0026#34; +%Y-%m-%d-%T # 将时间戳换算成日期 date -d \u0026#34;1970-01-01 UTC 1363867952 seconds\u0026#34; +%Y-%m-%d-%T # 将时间戳换算成日期 date -d \u0026#34;`awk -F. \u0026#39;{print $1}\u0026#39; /proc/uptime` second ago\u0026#34; +\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34; # 格式化系统启动时间(多少秒前) limits.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ulimit -SHn 65535 # 临时设置文件描述符大小 进程最大打开文件柄数 还有socket最大连接数, 等同配置 nofile ulimit -SHu 65535 # 临时设置用户最大进程数 ulimit -a # 查看 /etc/security/limits.conf # 文件描述符大小 open files # lsof |wc -l 查看当前文件句柄数使用数量 * soft nofile 16384 # 设置太大，进程使用过多会把机器拖死 * hard nofile 32768 # 用户最大进程数 max user processes # echo $((`ps uxm |wc -l`-`ps ux |wc -l`)) 查看当前用户占用的进程数 [包括线程] user soft nproc 16384 user hard nproc 32768 # 如果/etc/security/limits.d/有配置文件，将会覆盖/etc/security/limits.conf里的配置 # 即/etc/security/limits.d/的配置文件里就不要有同样的参量设置 /etc/security/limits.d/90-nproc.conf # centos6.3的默认这个文件会覆盖 limits.conf user soft nproc 16384 user hard nproc 32768 sysctl -p # 修改配置文件后让系统生效 百万长链接设置 1 2 3 4 5 6 7 # 内存消耗需要较大 vim /root/.bash_profile # 添加如下2行,退出bash重新登陆 # 一个进程不能使用超过NR_OPEN文件描述符 echo 20000500 \u0026gt; /proc/sys/fs/nr_open # 当前用户最大文件数 ulimit -n 10000000 无法分配内存 1 2 3 4 fork: Cannot allocate memory # 报错不一定是内存不够用，进程数或者线程数满了也会报这个错误， 可以适当增加 kernel.pid_max 的值， cat /proc/sys/kernel/pid_max # 默认3.2w iptables 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 内建三个表：nat mangle 和 filter filter预设规则表，有INPUT、FORWARD 和 OUTPUT 三个规则链 vi /etc/sysconfig/iptables # 配置文件 INPUT # 进入 FORWARD # 转发 OUTPUT # 出去 ACCEPT # 将封包放行 REJECT # 拦阻该封包 DROP # 丢弃封包不予处理 -A # 在所选择的链(INPUT等)末添加一条或更多规则 -D # 删除一条 -E # 修改 -p # tcp、udp、icmp 0相当于所有all !取反 -P # 设置缺省策略(与所有链都不匹配强制使用此策略) -s # IP/掩码 (IP/24) 主机名、网络名和清楚的IP地址 !取反 -j # 目标跳转，立即决定包的命运的专用内建目标 -i # 进入的（网络）接口 [名称] eth0 -o # 输出接口[名称] -m # 模块 --sport # 源端口 --dport # 目标端口 iptables -F # 将防火墙中的规则条目清除掉 # 注意: iptables -P INPUT ACCEPT iptables-restore \u0026lt; 规则文件 # 导入防火墙规则 /etc/init.d/iptables save # 保存防火墙设置 /etc/init.d/iptables restart # 重启防火墙服务 iptables -L -n # 查看规则 iptables -t nat -nL # 查看转发 iptables实例 iptables -L INPUT # 列出某规则链中的所有规则 iptables -X allowed # 删除某个规则链 ,不加规则链，清除所有非内建的 iptables -Z INPUT # 将封包计数器归零 iptables -N allowed # 定义新的规则链 iptables -P INPUT DROP # 定义过滤政策 iptables -A INPUT -s 192.168.1.1 # 比对封包的来源IP # ! 192.168.0.0/24 ! 反向对比 iptables -A INPUT -d 192.168.1.1 # 比对封包的目的地IP iptables -A INPUT -i eth0 # 比对封包是从哪片网卡进入 iptables -A FORWARD -o eth0 # 比对封包要从哪片网卡送出 eth+表示所有的网卡 iptables -A INPUT -p tcp # -p ! tcp 排除tcp以外的udp、icmp。-p all所有类型 iptables -D INPUT 8 # 从某个规则链中删除一条规则 iptables -D INPUT --dport 80 -j DROP # 从某个规则链中删除一条规则 iptables -R INPUT 8 -s 192.168.0.1 -j DROP # 取代现行规则 iptables -I INPUT 8 --dport 80 -j ACCEPT # 插入一条规则 iptables -A INPUT -i eth0 -j DROP # 其它情况不允许 iptables -A INPUT -p tcp -s IP -j DROP # 禁止指定IP访问 iptables -A INPUT -p tcp -s IP --dport port -j DROP # 禁止指定IP访问端口 iptables -A INPUT -s IP -p tcp --dport port -j ACCEPT # 允许在IP访问指定端口 iptables -A INPUT -p tcp --dport 22 -j DROP # 禁止使用某端口 iptables -A INPUT -i eth0 -p icmp -m icmp --icmp-type 8 -j DROP # 禁止icmp端口 iptables -A INPUT -i eth0 -p icmp -j DROP # 禁止icmp端口 iptables -t filter -A INPUT -i eth0 -p tcp --syn -j DROP # 阻止所有没有经过你系统授权的TCP连接 iptables -A INPUT -f -m limit --limit 100/s --limit-burst 100 -j ACCEPT # IP包流量限制 iptables -A INPUT -i eth0 -s 192.168.62.1/32 -p icmp -m icmp --icmp-type 8 -j ACCEPT # 除192.168.62.1外，禁止其它人ping我的主机 iptables -A INPUT -p tcp -m tcp --dport 80 -m state --state NEW -m recent --update --seconds 5 --hitcount 20 --rttl --name WEB --rsource -j DROP # 可防御cc攻击(未测试) iptables配置实例文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Generated by iptables-save v1.2.11 on Fri Feb 9 12:10:37 2007 *filter :INPUT ACCEPT [637:58967] :FORWARD DROP [0:0] :OUTPUT ACCEPT [5091:1301533] # 允许的IP或IP段访问 建议多个 -A INPUT -s 127.0.0.1 -p tcp -j ACCEPT -A INPUT -s 192.168.0.0/255.255.0.0 -p tcp -j ACCEPT # 开放对外开放端口 -A INPUT -p tcp --dport 80 -j ACCEPT # 指定某端口针对IP开放 -A INPUT -s 192.168.10.37 -p tcp --dport 22 -j ACCEPT # 拒绝所有协议(INPUT允许) -A INPUT -p tcp -m tcp --tcp-flags FIN,SYN,RST,PSH,URG RST -j DROP # 允许已建立的或相关连的通行 -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 拒绝ping -A INPUT -p tcp -m tcp -j REJECT --reject-with icmp-port-unreachable COMMIT # Completed on Fri Feb 9 12:10:37 2007 iptables配置实例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 允许某段IP访问任何端口 iptables -A INPUT -s 192.168.0.3/24 -p tcp -j ACCEPT # 设定预设规则 (拒绝所有的数据包，再允许需要的,如只做WEB服务器.还是推荐三个链都是DROP) iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT ACCEPT # 注意: 直接设置这三条会掉线 # 开启22端口 iptables -A INPUT -p tcp --dport 22 -j ACCEPT # 如果OUTPUT 设置成DROP的，要写上下面一条 iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT # 注:不写导致无法SSH.其他的端口一样,OUTPUT设置成DROP的话,也要添加一条链 # 如果开启了web服务器,OUTPUT设置成DROP的话,同样也要添加一条链 iptables -A OUTPUT -p tcp --sport 80 -j ACCEPT # 做WEB服务器,开启80端口 ,其他同理 iptables -A INPUT -p tcp --dport 80 -j ACCEPT # 做邮件服务器,开启25,110端口 iptables -A INPUT -p tcp --dport 110 -j ACCEPT iptables -A INPUT -p tcp --dport 25 -j ACCEPT # 允许icmp包通过,允许ping iptables -A OUTPUT -p icmp -j ACCEPT (OUTPUT设置成DROP的话) iptables -A INPUT -p icmp -j ACCEPT (INPUT设置成DROP的话) # 允许loopback!(不然会导致DNS无法正常关闭等问题) IPTABLES -A INPUT -i lo -p all -j ACCEPT (如果是INPUT DROP) IPTABLES -A OUTPUT -o lo -p all -j ACCEPT(如果是OUTPUT DROP) 添加网段转发 1 2 3 4 5 # 例如通过vpn上网 echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward # 在内核里打开ip转发功能 iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -j MASQUERADE # 添加网段转发 iptables -t nat -A POSTROUTING -s 10.0.0.0/255.0.0.0 -o eth0 -j SNAT --to 192.168.10.158 # 原IP网段经过哪个网卡IP出去 iptables -t nat -nL # 查看转发 端口映射 1 2 3 4 5 6 7 8 # 内网通过有外网IP的机器映射端口 # 内网主机添加路由 route add -net 10.10.20.0 netmask 255.255.255.0 gw 10.10.20.111 # 内网需要添加默认网关，并且网关开启转发 # 网关主机 echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward # 在内核里打开ip转发功能 iptables -t nat -A PREROUTING -d 外网IP -p tcp --dport 9999 -j DNAT --to 10.10.20.55:22 # 进入 iptables -t nat -A POSTROUTING -s 10.10.20.0/24 -j SNAT --to 外网IP # 转发回去 iptables -t nat -nL # 查看转发 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210531132227/\n相关话题：https://www.cnsre.cn/tags/shell/\n","description":"使用Linux shell是一些程序员每天的基本工作，但经常会忘记一些有用的shell命令和技巧。当然，命令我能记住，但我不敢说能记得如何用它执行某个特定任务。需要注意一点的是，有些用法需要在你的Linux系统里安装额外的软件。","id":61,"section":"posts","tags":["shell"],"title":"shell常用命令","uri":"https://www.cnsre.cn/posts/210531132227/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210531100355/\n相关话题：https://www.cnsre.cn/tags/aws/\n使用 CloudWatch 代理收集指标和日志\n下载 CloudWatch 代理软件包 sudo yum install amazon-cloudwatch-agent 点击查看其他平台软件包\n配置文件\n1 2 3 4 5 6 配置文件路径及名称 /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json #配置启动 sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json #启动服务 systemctl restart amazon-cloudwatch-agent.service 点击查看内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 { \u0026#34;agent\u0026#34;: { \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;run_as_user\u0026#34;: \u0026#34;cwagent\u0026#34; }, \u0026#34;metrics\u0026#34;: { \u0026#34;aggregation_dimensions\u0026#34;: [ [ \u0026#34;InstanceId\u0026#34; ] ], \u0026#34;append_dimensions\u0026#34;: { \u0026#34;AutoScalingGroupName\u0026#34;: \u0026#34;${aws:AutoScalingGroupName}\u0026#34;, \u0026#34;ImageId\u0026#34;: \u0026#34;${aws:ImageId}\u0026#34;, \u0026#34;InstanceId\u0026#34;: \u0026#34;${aws:InstanceId}\u0026#34;, \u0026#34;InstanceType\u0026#34;: \u0026#34;${aws:InstanceType}\u0026#34; }, \u0026#34;metrics_collected\u0026#34;: { \u0026#34;procstat\u0026#34;: [ { \u0026#34;pid_file\u0026#34;: \u0026#34;/var/run/sshd.pid\u0026#34;, \u0026#34;measurement\u0026#34;: [ \u0026#34;cpu_usage\u0026#34;, \u0026#34;memory_rss\u0026#34; ] }, { \u0026#34;pid_file\u0026#34;: \u0026#34;/var/run/sshd.pid\u0026#34;, \u0026#34;measurement\u0026#34;: [ \u0026#34;read_bytes\u0026#34;, \u0026#34;read_count\u0026#34;, \u0026#34;write_bytes\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 10 }, \u0026#34;cpu\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;cpu_usage_idle\u0026#34;, \u0026#34;cpu_usage_iowait\u0026#34;, \u0026#34;cpu_usage_user\u0026#34;, \u0026#34;cpu_usage_system\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;totalcpu\u0026#34;: false }, \u0026#34;disk\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;used_percent\u0026#34;, \u0026#34;inodes_free\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;diskio\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;io_time\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 }, \u0026#34;swap\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;swap_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 } } } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 { \u0026#34;logs\u0026#34;: { \u0026#34;logs_collected\u0026#34;: { \u0026#34;files\u0026#34;: { \u0026#34;collect_list\u0026#34;: [{ \u0026#34;file_path\u0026#34;: \u0026#34;/logArchive/hcaextension/info*.log\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;RGC-Prod-3in1oven\u0026#34;, \u0026#34;log_stream_name\u0026#34;: \u0026#34;info.logs\u0026#34; }, { \u0026#34;file_path\u0026#34;: \u0026#34;/logArchive/hcaextension/http*.log\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;RGC-Prod-3in1oven\u0026#34;, \u0026#34;log_stream_name\u0026#34;: \u0026#34;http.logs\u0026#34; } ] } } }, \u0026#34;metrics\u0026#34;: { \u0026#34;append_dimensions\u0026#34;: { \u0026#34;AutoScalingGroupName\u0026#34;: \u0026#34;${aws:AutoScalingGroupName}\u0026#34;, \u0026#34;ImageId\u0026#34;: \u0026#34;${aws:ImageId}\u0026#34;, \u0026#34;InstanceId\u0026#34;: \u0026#34;${aws:InstanceId}\u0026#34;, \u0026#34;InstanceType\u0026#34;: \u0026#34;${aws:InstanceType}\u0026#34; }, \u0026#34;metrics_collected\u0026#34;: { \u0026#34;cpu\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;cpu_usage_idle\u0026#34;, \u0026#34;cpu_usage_iowait\u0026#34;, \u0026#34;cpu_usage_user\u0026#34;, \u0026#34;cpu_usage_system\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 180, \u0026#34;totalcpu\u0026#34;: false }, \u0026#34;disk\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;used_percent\u0026#34;, \u0026#34;inodes_free\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 180, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;diskio\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;io_time\u0026#34;, \u0026#34;write_bytes\u0026#34;, \u0026#34;read_bytes\u0026#34;, \u0026#34;writes\u0026#34;, \u0026#34;reads\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 180, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 180 }, \u0026#34;netstat\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;tcp_established\u0026#34;, \u0026#34;tcp_time_wait\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 180 }, \u0026#34;statsd\u0026#34;: { \u0026#34;metrics_aggregation_interval\u0026#34;: 60, \u0026#34;metrics_collection_interval\u0026#34;: 180, \u0026#34;service_address\u0026#34;: \u0026#34;:8125\u0026#34; }, \u0026#34;swap\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;swap_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 180 } } } } 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210531100355/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"安装配置CloudWatchAgent，使用 CloudWatch 代理收集指标和日志。","id":62,"section":"posts","tags":["cloudwatch","aws"],"title":"安装和配置CloudWatchAgent","uri":"https://www.cnsre.cn/posts/210531100355/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210518420293/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n自从接触zabbix后，就一直想着怎么才能把告警推送到个人微信上。有这样的想法主要是个人微信的使用频率远远要比钉钉，企业微信，邮箱，飞书等使用频率要高。比如我，就遇到过在周末的时候，因为没有及时登录钉钉查看zabbix告警通知，导致一些告警没来得及处理，对环境产生了影响。\n前段时间朋友给推荐了pushplus，一开始主要在jenkins做构建通知用的，后来就想，能不能使用zabbix的告警通知推送到微信上呢？\n先介绍下pushplus吧,pushplus 官方介绍\npushplus(推送加)集成了微信、企业微信、钉钉、短信、邮件等实时信息推送平台\n你只需要调用简单的API，即可帮助你迅速完成消息的推送，使用简单方便\n我们的所做的一切只是为了让推送变的更简单\npushplus 登录官网注册pushplus 访问官网后点击登录,微信扫描即可注册。\n获取pushplus Token 注册成功后。点击一对多，然后新建一个群组。群组编码作为群组的唯一标示，后续需要使用。群组名称随意填写。\n创建成功之后点击群组上的查看二维码，将二维码发给需要加入群组的同事。以后推送的消息加入群组的用户都会收的到。在“订阅人”中可以主动的移除不想要的用户。\n然后保存你的token和群组编码。\nzabbix_server端设置 查看pushplus调用方式.\n附上脚本\n脚本很简单，shell直接调用接口即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/bin export PATH LANG=en_US.UTF-8 ################################################# \u0026#34; # 脚本名称 : zabbix告警推送个人微信脚本 # \u0026#34; # 作 者 : sre运维博客 # \u0026#34; # 网 址 : https:www.cnsre.cn # \u0026#34; ################################################# \u0026#34; token=c651d07axxxxxxxxxxx topic=$1 title=$2 content=$3 url=http://www.pushplus.plus/send json=\u0026#34;{\\\u0026#34;token\\\u0026#34;: \\\u0026#34;$token\\\u0026#34;, \\\u0026#34;title\\\u0026#34;: \\\u0026#34;$title\\\u0026#34;, \\\u0026#34;content\\\u0026#34;: \\\u0026#34;$3\\\u0026#34;, \\\u0026#34;template\\\u0026#34;: \\\u0026#34;html\\\u0026#34;, \\\u0026#34;topic\\\u0026#34;: \\\u0026#34;$topic\\\u0026#34;}\u0026#34; curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#34;$json\u0026#34; $url token 在 pushplus 官网中一对一，一对多中查看 你的token topic 在 创建的群组 中的 群组编码 title 消息标题 content 具体消息内容，根据不同template支持不同格式 url http://www.pushplus.plus/send 楼下网友留言说脚本无法使用，后来经过查看发现是注册地址的问题\n需要注意你的注册的官网地址 hxtrip 还是 pushplus 。\n不过我还是推荐使用 pushplus 因为 pushplus 在国内是备案了的。 将脚本部署在alertscripts目录下，如果你不知道在那，你可以回想一下你钉钉告警，或者是企业微信告警的脚本放在那。如果实在记不起来，那就find找下alertscripts目录吧\nzabbix web端设置 创建报警媒介类型 登录zabbix web端点击管理\u0026ndash;报警媒介类型\u0026ndash;创建媒体类型\n名称：随意写\n类型：选择脚本\n脚本名称：按照实际名称填写\n脚本参数:\n{ALERT.SENDTO} {ALERT.SUBJECT} {ALERT.MESSAGE} 创建动作 点击配置\u0026ndash;动作\u0026ndash;创建动作\n动作\n名称：随意写\n条件：根据自己的告警需求填写\n操作\n默认操作步骤持续时间：1h\n默认标题：你也可以随意写。也可以用下面的。\n1 故障{TRIGGER.STATUS},服务器:{HOSTNAME1}发生:{TRIGGER.NAME}故障! 故障通知！！ 消息内容:和标题一样\n1 2 3 4 5 6 7 8 9 告警主机:{HOSTNAME1}\u0026lt;br\u0026gt; IP地址：{HOST.CONN} \u0026lt;br\u0026gt; 告警时间:{EVENT.DATE} {EVENT.TIME}(UTC)\u0026lt;br\u0026gt; 告警等级:{TRIGGER.SEVERITY}\u0026lt;br\u0026gt; 告警信息: {TRIGGER.NAME}\u0026lt;br\u0026gt; 告警项目:{TRIGGER.KEY1}\u0026lt;br\u0026gt; 问题详情:{ITEM.NAME}:{ITEM.VALUE}\u0026lt;br\u0026gt; 当前状态:{TRIGGER.STATUS}:{ITEM.VALUE1}\u0026lt;br\u0026gt; 事件ID:{EVENT.ID}\u0026lt;br\u0026gt; 操作步骤\u0026ndash;点击新的\n选择发送到用户，点击添加\u0026ndash;admin\u0026ndash;选择\n仅送到，选择你刚创建的，最后点击添加\n最为为这样\n恢复操作\n恢复操作和操作一样\n添加完后\n创建用户报警媒介 选择管理\u0026ndash;用户\u0026ndash;admin\n测试告警 添加完过后，接下来就测试吧。\n我这边选择了一个触发器调了下告警的值，最后展示下告警以及恢复。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210518420293/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"使用pushplus将zabbix的告警推送至个人微信，这样就不用在非工作日心心念念的去钉钉、邮箱等去查看告警了","id":63,"section":"posts","tags":["zabbix","pushplus"],"title":"zabbix告警推送至个人微信","uri":"https://www.cnsre.cn/posts/210518420293/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210517344530/\n相关话题：https://www.cnsre.cn/tags/zabbix/\naws 端操作 先在EC2 实例中选中磁盘 然后打开跟设备 修改大小后保存\nec2 端操作 lsblk 查看当前设备的磁盘编号\ndf -T -H 查看扩容前的空间大小并确定磁盘格式\ngrowpart /dev/nvme0n1 1 把扩容的空间挂载到磁盘上\ncentos7执行划分空间命令\nsudo xfs_growfs -d / 把空闲的空间划分至 /\ncentos6执行划分空间命令\nresize2fs /dev/nvme0n1p1\ndf -h 查看验证\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210517344530/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"aws ec2实例跟空间容量不足如何处理？教你如何快速在aws上扩容ec2实例根空间，扩容ec2根磁盘。","id":64,"section":"posts","tags":["ec2","aws"],"title":"AWS扩容EC2实例根空间","uri":"https://www.cnsre.cn/posts/210517344530/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210514620165/\n相关话题：https://www.cnsre.cn/tags/aws/\n问题描述 ALB 负载均衡 RGC-Dev-ALB.xxx.cn-north-1.elb.amazonaws.com.cn 解析到2个IP 54.223.xxx.xx和52.81.xxx.xx， 发现每2次请求会失败一次，在进一步测试抓包发现没有收到52.81.xxx.xxx的返回信息。\n问题分析 随后检查ALB建立在两个子网（subnet-a1xxxxx和subnet-f3xxxxx）\n其中54.223.xxx.xx在subnet-f32xxxx中，子网路由表rtb-49xxxx中0.0.0.0/0 指向IGW，因此客户端可以主动访问到54.223.xxx.xx。\n52.81.xxx.xx在subnet-a1xxx中，子网路由表rtb-24xxx中0.0.0.0/0指向了nat gateway(nat-0axxxxxxxxxx), 这将导致客户端无法连接到52.81.xxx.xx， 因此也不会收到52.81.xxx.xx的回包。\n请知晓，对于面向公网的ALB，需要将ALB部署在公有子网中， 即子网路由表0.0.0.0/0需要指向IGW。\n解决办法 目前有2个解决办法 1） 修改子网路由表rtb-24xxx， 将0.0.0.0/0指向igw， 请知晓， 这个修改将影响所有关联了rtb-24xxx这个路由表的子网，\n如果对应子网中的资源没有公网地址，修改完成后将失去访问公网的能力，此外对于子网中有公网地址的资源，将直接从公网路由可达。\n2） 修改ALB的子网，可以在EC2的控制台找到“负载均衡” ，选择对应的ALB， 在“描述” \u0026gt; “基本配置” \u0026gt;\u0026ldquo;可用区\u0026rdquo; \u0026gt; 点击“编辑子网”, 将subnet-a1xxx 修改为同AZ的公有子网（即路由表0.0.0.0/0指向igw的子网）\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210514620165/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"AWS负载均衡每2次请求会失败一次，进行测试抓包发现有一次没有收到的返回信息。","id":65,"section":"posts","tags":["aws","alb","负载均衡"],"title":"AWS使用ALB负载均衡遇到的问题","uri":"https://www.cnsre.cn/posts/210514620165/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210513036112/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n故障现象 ubuntu 16 升级18 之后 安装了zabbix agent 今天突然agent掉了 上去的时候发现\n报错：\nGot singnal [singnal:15(SIGTERM),sender_pid:31392,sender_uid:0,reason:0]. Exiting ...a 后来打算-c 启动然后发现 /usr/sbin/zabbix_agentd 丢了\n解决方法 我尝试着去检查selinux，防火墙，安全组，以及zabbix-agent配置文件等等都没有发现异常的地方，在同一批部署的agent中其他的agent 也暂时没有出现这种情况。\n问了很多朋友，因为没有找到原因，日志中给到的信息也不是很多，最后选择了重装zabbix_agent.\n最后一次重装，我做选择了手动修改zabbix的配置文件，验证问题，还有待观察。如果各位大佬有知道原因，能够留言或者私信的话我想我会十分感谢。\n下面放一个zabbix官方wiki 网名为SwitchZabbix的网友遇到同样问题的分享。\nhttps://www.zabbix.com/forum/zabbix-troubleshooting-and-problems/369895-zabbix-is-not-running-on-frontend\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210513036112/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"ubuntu zabbix无法启动","id":66,"section":"posts","tags":["zabbix","ubuntu"],"title":"ubuntu zabbix无法启动","uri":"https://www.cnsre.cn/posts/210513036112/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210510512134/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n硬件配置需求 环境 平台 CPU/内存 数据库 硬盘 监控主机数 小型 centOS 2CPU/1GB MySQL、InnoDB 普通 100 中型 centOS 2CPU/2GB MySQL、InnoDB 普通 500 大型 Red HatEnterpirse Linux 4CPU/8GB MySQL、InnoDB 或PostgreSQL RAID 10 或 SSD 大于1000 超大型 Red HatEnterpirse Linux 8CPU/16GB MySQL、InnoDB 或PostgreSQL RAID 10 或 SSD 大于10000 zabbix版本 4.4 Latest https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm 3.0 LTS 稳定版 http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm 4.0 LTS 正式版 https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm pre-4.0版 http://repo.zabbix.com/zabbix/3.5/rhel/7/x86_64/zabbix-release-3.5-1.el7.noarch.rpm 环境准备 Linux 7.7.1908 nginx 1.16.1 zabbix-server 4.4 zabbix-agent 4.4 mysql 5.7.29 php 5.4.16 关闭防火墙及selinux 1 2 systemctl stop firewalld \u0026amp;\u0026amp; systemctl disable firewalld sed -i \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/\u0026#39;/etc/selinux/config zabbix安装配置 安装MySQL数据库与知识库 1 rpm -i https://repo.zabbix.com/zabbix/4.4/rhel/7/x86_64/zabbix-release-4.4-1.el7.noarch.rpm 安装Zabbix服务器、前端、代理 1 yum install zabbix-server-mysql zabbix-web-mysql zabbix-agent -y 配置Zabbix-server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 mv /etc/zabbix/zabbix_server.conf /etc/zabbix/zabbix_server.conf.bak vim /etc/zabbix/zabbix_server.conf LogFile=/var/log/zabbix/zabbix_server.log LogFileSize=0 PidFile=/var/run/zabbix/zabbix_server.pid SocketDir=/var/run/zabbix DBHost=localhost DBName=zabbix DBUser=zabbix DBPassword=zabbix DBPort=3306 SNMPTrapperFile=/var/log/snmptrap/snmptrap.log CacheSize=1024M Timeout=4 AlertScriptsPath=/usr/lib/zabbix/alertscripts ExternalScripts=/usr/lib/zabbix/externalscripts LogSlowQueries=3000 数据库安装配置 安装数据库 1 2 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm yum localinstall mysql57-community-release-el7-8.noarch.rpm -y 检查mysql源是否安装成功 1 2 3 4 5 yum repolist enabled | grep \u0026#34;mysql.*-community.*\u0026#34; yum install mysql-community-server -y ### 启动MySQL服务 systemctl start mysqld 登录数据库 1 2 3 4 5 6 grep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log #登陆数据库 mysql -uroot -p 修改默认密码 mysql\u0026gt; SET PASSWORD = PASSWORD(\u0026#39;Lenovo@123\u0026#39;); #Lenvoo@123是你的新密码 ⚠️ 如何解决\nERROR 1819 (HY000): Your password does not satisfy the current policy requirements 1 2 3 4 5 6 7 8 修改validate_password_policy参数的值 set global validate_password_policy=0; 再修改密码的长度 set global validate_password_length=1; 再次执行修改密码就可以了 ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;Lenovo@123\u0026#39;; 允许root远程登陆* GRANT ALL PRIVILEGES ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Lenovo@123\u0026#39; WITH GRANT OPTION; 创建zabbix数据库和用户 1 2 3 4 5 6 7 8 9 10 11 12 13 mysql –uroot –p mysql\u0026gt; create database zabbix character set utf8; Query OK, 1 row affected (0.00 sec) mysql\u0026gt; grant all privileges on zabbix.* to \u0026#39;zabbix\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;zabbix\u0026#39;; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql\u0026gt; flush privileges; Query OK, 0 rows affected (0.00 sec) 导入模板数据 方法1 方法2 方法1\n1 2 mysql\u0026gt; use zabbix; mysql\u0026gt; source /usr/share/doc/zabbix-server-mysql-4.4.9/create.sql 方法2\n1 [root@localhost ~]# zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix nginx安装配置 安装nginx 1 2 wget http://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.18.0-1.el7.ngx.x86_64.rpm yum install nginx -y 配置nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 vim /etc/nginx/conf.d/default.conf ... location / { root /usr/share/zabbix; index index.php; } ... location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/zabbix$fastcgi_script_name; include fastcgi_params; } ... 修改zabbix前端文件权限 1 2 chown nginx:nginx /usr/share/zabbix/* chmod -R 755 /usr/share/zabbix/* 启动nginx 1 systemctl restart nginx PHP安装配置 安装php 1 yum install php-fpm php-gd php-mbstring php-bcmath php-gd php-xmlwriter php-xmlreader -y 修改配置文件 1 2 3 4 5 6 sed -i \u0026#34;s#max_execution_time = 30#max_execution_time = 600#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#max_input_time = 60#max_input_time = 600#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#memory_limit = 128M#memory_limit = 256M#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#post_max_size = 8M#post_max_size = 32M#g\u0026#34; /etc/php.ini sed -i \u0026#34;s#upload_max_filesize = 2M#upload_max_filesize = 16M#g\u0026#34; /etc/php.ini sed -i \u0026#34;s/;date.timezone =/date.timezone = Asia\\/Shanghai/g\u0026#34; /etc/php.ini 启动php-fpm 1 systemctl start php-fpm 启动所有服务 1 2 3 4 5 systemctl start zabbix-server systemctl start zabbix-agent systemctl start nginx systemctl start mysqld systemctl start php-fpm 设置开机启动项 1 systemctl enable zabbix-server zabbix-agent mysqld nginx php-fpm 检查端口 1 2 3 4 5 6 [root@zabbix-server]# netstat -pntl tcp 0 0 127.0.0.1:9000 0.0.0.0:* LISTEN 5118/php-fpm: maste tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 5046/nginx: master tcp6 0 0 :::10050 :::* LISTEN 5577/zabbix_agentd tcp6 0 0 :::10051 :::* LISTEN 4821/zabbix_server tcp6 0 0 :::3306 :::* LISTEN 1703/mysqld 连接到新安装的Zabbix前端： http://server_ip\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210510512134/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"使用LNMP 架构安装zabbix 4.4","id":67,"section":"posts","tags":["zabbix"],"title":"LNMP安装zabbix4.4","uri":"https://www.cnsre.cn/posts/210510512134/"},{"content":" \u0026nbsp\u0026nbsp留言板\n📢快来留言发表你的看法吧~\u0026nbsp\u0026nbsp\n","description":"📢有什么想说的吗？快来留言发表你的看法吧~","id":68,"section":"posts","tags":null,"title":"留言板","uri":"https://www.cnsre.cn/posts/comment/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210426034157/\n相关话题：https://www.cnsre.cn/tags/jenkins/\n前言 开发乱发版？不发邮箱就要上生产？但是有需要自己发布生产以外的其他环境。怎么办？最后决定把jenkins构建的权限给开发，这样的话效率会增加。运维也不必每次帮助开发去构建发布。同时取消掉生产环境的权限。避免乱发版。\n规划 jenkins 四个项目分别对应三个人负责。项目下的只负责 CI、UAT.CD、Pilot.CD\n简单的说就是 三个人负责自己对应的项目，但是涉及到PROD 环境。\n项目 负责人 Broadlink jihua HiLink xiongfeng Miniprogram xiefei SpeedBoat jihua job 展示\n配置 安装插件 插件：Role-based Authorization Strategy\n版本：2.3.2\n系统管理\u0026mdash;\u0026gt;管理插件\n全局安全配置 系统管理\u0026mdash;\u0026gt;全局安全配置\n创建用户 系统管理\u0026mdash;\u0026gt;管理用户\n创建角色 系统管理\u0026mdash;\u0026gt;Manage and Assign Roles\n创建一个Global roles 我们先创建一个全局角色user，赋予全局标签下面的Read权限。\n创建规划项目的角色 分别创建两个项目的角色，按照规划的分配权限，Pattern用于给项目匹配 job，使用正则表达式。\nBroadlink.* 配置以Broadlink开头的job Hilink-.*[^PROD]$ 匹配以Hilink不以PROD结尾的job Miniprogram-.*[^PROD]$ 匹配以Miniprogram不以PROD结尾的job Speedboat-.*[^PROD]$ 匹配以Speedboat不以PROD结尾的job 给用户分配角色 系统管理\u0026mdash;\u0026gt;Manage and Assign Roles\u0026mdash;\u0026gt;Assign Roles\n登录验证 此处因为Broadlink 项目还未开始使用，所以未将生产环境划分出去。\n权限验证\n可以看到，各自看到的权限就是我们在角色里面赋予的权限，没有问题.\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210426034157/\n相关话题：https://www.cnsre.cn/tags/jenkins/\n","description":"把jenkins 构建的权限给开发，这样的话效率会增加。运维也不必每次帮助开发去构建发布。","id":69,"section":"posts","tags":["jenkins"],"title":"jenkins 添加用户管理权限","uri":"https://www.cnsre.cn/posts/210426034157/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210426026121/\n相关话题：https://www.cnsre.cn/tags/grafana/\n介绍 最近上了ELK 日志分析，想着手看下用户的分布情况，在kibana 中展示用户分布情况是没有问题的，但是索引添加到Granfana 中的话就无法展示。\n问题描述 添加ES索引以后，map地图一片黑，不显示地图信息以及数据。但是有显示图例。后来发现是网络的问题，地图的URL是外面的，国内看不到。\n解决方法 替换插件里grafana-worldmap-panel文件图片地址\n三个文件路径\n1 2 3 grafana-worldmap-panel\\src\\worldmap.ts grafana-worldmap-panel\\dist\\module.js grafana-worldmap-panel\\dist\\module.js.map 将三个文件中的\nhttps://cartodb-basemaps-{s}.global.ssl.fastly.net/light_all/{z}/{x}/{y}.png 替换为\nhttp://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}.png\n将三个文件中的\nhttps://cartodb-basemaps-{s}.global.ssl.fastly.net/dark_all/{z}/{x}/{y}.png\n替换为\nhttp://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}.png\n操作如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # cd /var/lib/grafana/plugins/ # grafana-worldmap-panel\\src\\worldmap.ts # grafana-worldmap-panel\\dist\\module.js # grafana-worldmap-panel\\dist\\module.js.map # sed -i \u0026#39;s/https:\\/\\/cartodb-basemaps{s}.global.ssl.fastly.net\\/light_all\\/{z}\\/{x}\\/{y}.png/http:\\/\\/{s}.basemaps.cartocdn.com\\/light_all\\/{z}\\/{x}\\/{y}.png/\u0026#39; \\ grafana-worldmap-panel/src/worldmap.ts \\ grafana-worldmap-panel/dist/module.js \\ grafana-worldmap-panel/dist/module.js.map #sed -i \u0026#39;s/https:\\/\\/cartodb-basemaps-{s}.global.ssl.fastly.net\\/dark_all\\/{z}\\/{x}\\/{y}.png/http:\\/\\/{s}.basemaps.cartocdn.com\\/dark_all\\/{z}\\/{x}\\/{y}.png/\u0026#39; \\ grafana-worldmap-panel/src/worldmap.ts \\ grafana-worldmap-panel/dist/module.js \\ grafana-worldmap-panel/dist/module.js.map 重启Grafana 1 systemctl restart grafana ⚠️ 注：如果不行的话，多重启刷新页面几次试试 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210426026121/\n相关话题：https://www.cnsre.cn/tags/grafana/\n","description":"最近上了ELK 日志分析，想着手看下用户的分布情况，在kibana 中展示用户分布情况是没有问题的，但是索引添加到Granfana 中的话就无法展示。","id":70,"section":"posts","tags":["grafana","故障集"],"title":"Grafana插件地图Worldmap不显示","uri":"https://www.cnsre.cn/posts/210426026121/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210425130327/\n相关话题：https://www.cnsre.cn/tags/zabbix/\nzabbix-agent 服务器配置 脚本内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 cat /etc/zabbix/scripts/auto_detection_tcp_state.sh #!/bin/bash #TCP连接数及状态 if [ $# -ne 1 ];then echo -e \u0026#34;\\033[32mUsage: sh $0 {ESTABLISHED|LISTEN|TIME_WAIT|CLOSED|CLOSE_WAIT|CLOSING|FIN_WAIT1|FIN_WAIT2|LAST_ACK|SYN_RECV|SYN_SENT}\\033[0m\u0026#34; exit 1 fi case $1 in #socket已经建立连接 ESTABLISHED) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;ESTABLISHED\u0026#34;) echo $result ;; #监听状态 LISTEN) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;LISTEN\u0026#34;) echo $result ;; #表示收到了对方的FIN报文，并发送出了ACK报文，等待2MSL后就可回到CLOSED状态 TIME_WAIT) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;TIME_WAIT\u0026#34;) echo $result ;; #socket没有被使用，无连接 CLOSED) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;CLOSED\u0026#34;) echo $result ;; #等待关闭连接 CLOSE_WAIT) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;CLOSE_WAIT\u0026#34;) echo $result ;; #服务器端和客户端都同时关闭连接 CLOSING) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;CLOSING\u0026#34;) echo $result ;; #套接字已关闭，连接正在关闭 FIN_WAIT1) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;FIN_WAIT1\u0026#34;) echo $result ;; #连接已关闭，套接字正在等待从远程端关闭 FIN_WAIT2) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;FIN_WAIT2\u0026#34;) echo $result ;; #远端关闭，当前socket被动关闭后发送FIN报文，等待对方ACK报文 LAST_ACK) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;LAST_ACK\u0026#34;) echo $result ;; #接收到SYN报文 SYN_RECV) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;SYN_RECV\u0026#34;) echo $result ;; #已经发送SYN报文 SYN_SENT) result=$(netstat -an | awk \u0026#39;/^tcp/ {print $0}\u0026#39;|grep -wc \u0026#34;SYN_SENT\u0026#34;) echo $result ;; *) echo -e \u0026#34;\\033[32mUsage: sh $0 {ESTABLISHED|LISTEN|TIME_WAIT|CLOSED|CLOSE_WAIT|CLOSING|FIN_WAIT1|FIN_WAIT2|LAST_ACK|SYN_RECV|SYN_SENT}\\033[0m\u0026#34; esac 编辑zabbix_agent配置文件 添加以下内容\n1 2 3 vim /etc/zabbix/zabbix_agentd.conf ##添加此行 UserParameter=tcp.state[*],/etc/zabbix/scripts/auto_detection_tcp_state.sh $1 重启zabbix-agent 1 service zabbix_agentd restart 授权并验证脚本 1 2 3 4 5 chmod +x auto_detection_tcp_state.sh ./auto_detection_tcp_state.sh LISTEN 9 zabbix_get -s 10.0.10.243 -k \u0026#34;tcp.state[LISTEN]\u0026#34; 9 Zabbix监控平台配置 模板 点击下载zabbix_tcp_templates.xml\n导入TCP状态监控模板 配置-\u0026gt;模板-\u0026gt;导入(右上角)-\u0026gt;选择下载的模板文件-\u0026gt;最后点击导入\n关联模板 要把导入的模板关联到相对应的主机上边、配置-\u0026gt;点击你的主机-\u0026gt;``模板-\u0026gt;选择刚才导入模板，点击添加，最后点击更新`即可。\n最后展示 ESTABLISHED socket已经建立连接 CLOSED socket没有被使用，无连接 CLOSING 服务器端和客户端都同时关闭连接 CLOSE_WAIT 等待关闭连接 TIME_WAIT 表示收到了对方的FIN报文，并发送出了ACK报文，等待2MSL后就可回到CLOSED状态 LAST_ACK 远端关闭，当前socket被动关闭后发送FIN报文，等待对方ACK报文 LISTEN 监听状态 SYN_RECV 接收到SYN报文 SYN_SENT 已经发送SYN报文 FIN_WAIT1 The socket is closed, and the connection is shutting down FIN_WAIT2 Connection is closed, and the socket is waiting for a shutdown from the remote TCP 相关材料 Time_Wait状态产生的原因，危害，如何避免\n什么是time_Wait？如何产生的？\n附送送几个自定义脚本 接口健康检查 监控磁盘IO 接口健康检查\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash healthyCheck=https://validation.xxx.cn/healthyCheck if [ $# -ne 1 ];then echo \u0026#34;Follow the script name with an argument\u0026#34; fi case $1 in healthyCheck) curl -k -s $healthyCheck |awk -F \u0026#39;\u0026#34;|:|,\u0026#39; \u0026#39;{print $4}\u0026#39; ;; *) echo -e \u0026#34;\\033[5;31m Usage: sh -bash [Boradlink|Hcmini]\\033[0m\u0026#34; esac 监控磁盘IO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #!/bin/bash Device=`iostat |awk \u0026#39;{print $1}\u0026#39; | awk \u0026#39;NR==7{print}\u0026#39;` if [ $# -ne 1 ];then echo \u0026#34;Follow the script name with an argument\u0026#34; fi case $1 in rrqm) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $3}\u0026#39; ;; rps) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $4}\u0026#39; ;; wps) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $5}\u0026#39; ;; rKBps) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $6}\u0026#39; ;; wKBps) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $7}\u0026#39; ;; avgrq-sz) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $8}\u0026#39; ;; avgqu-sz) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $9}\u0026#39; ;; await) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $10}\u0026#39; ;; svctm) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $13}\u0026#39; ;; util) iostat -dxk 1 1|grep -w $Device |awk \u0026#39;{print $14}\u0026#39; ;; *) echo -e \u0026#34;\\e[033mUsage: sh $0 [rrqm|rps|wps|rKBps|wKBps|avgqu-sz|avgrq-sz|await|svctm|util]\\e[0m\u0026#34; esac eof 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210425130327/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"使用zabbix 监控TCP 连接数、状态等信息，在文章最后附上一些其他的zabbix其他自定义脚本。","id":71,"section":"posts","tags":["zabbix","shell"],"title":"zabbix监控TCP状态","uri":"https://www.cnsre.cn/posts/210425130327/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210423140530/\n相关话题：https://www.cnsre.cn/tags/grafana/\ngrafana-server 配置 smtp 服务器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vim /etc/grafana/grafana.ini #修改一下内容 #################################### SMTP / Emailing ########################## [smtp] # 启用 smtp enabled = true # 邮件服务器地址和端口 host = smtp.189.cn:465 # 发送告警邮件邮箱账号 user = cnsre@189.cn # 发送告警邮件邮箱密码 password = 123456 ;cert_file = ;key_file = ;skip_verify = false from_address = cnsre@189.cn from_name = Grafana # EHLO identity in SMTP dialog (defaults to instance_name) ehlo_identity = dashboard.example.com [emails] ;welcome_email_on_sign_up = false 重启服务 1 systemctl restart grafana-server 配置邮件通知方式 保存发送测试邮件，配置完成\n查看邮箱是否收到测试邮件\\ 配置告警 1的意思为 Max Request time TOP 10 alert 每分钟计算一次图标数据 告警触发持续两分钟发送邮件。\n2的意思为 查询A语句，5分钟之前到现在的平均值大于500 则触发告警。 配置告警方式 为了告警并发送邮件 我把请求时间的数值设置为大于5 则触发告警。\n收到邮件为\n下边还配有图片 图片太大，就不展示了 。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210423140530/\n相关话题：https://www.cnsre.cn/tags/grafana/\n","description":"grafana配置阈值，实现邮件告警通知","id":72,"section":"posts","tags":["grafana"],"title":"Grafana邮箱告警","uri":"https://www.cnsre.cn/posts/210423140530/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210421130319/\n相关话题：https://www.cnsre.cn/tags/shell/\n利用Shell开发跳板机功能脚本案例 开发企业级Shell跳板机案例。要求用户登录到跳板机仅能执行管理员给定的选项动作，不允许以任何形式中断脚本到跳板机服务器上执行任何系统命令。\n首先做好SSH密钥验证（跳板机地址172.16.1.200）。 以下操作命令在所有机器上操作：\n1 2 3 4 [root@cnsre ~]# useradd cnsre #\u0026lt;==要在所有机器上操作。\u0026lt; code=\u0026#34;\u0026#34;\u0026gt; [root@cnsre ~]# echo 123456|passwd --stdin cnsre #\u0026lt;==要在所有机器上操作。\u0026lt; code=\u0026#34;\u0026#34;\u0026gt; Changingpassword for user cnsre passwd:all authentication tokens updated successfully. 以下操作命令仅在跳板机上操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [root@cnsre ~]# su - cnsre [root@cnsre ~]# ssh-keygen -t dsa -P \u0026#39;\u0026#39; -f ~/.ssh/id_dsa \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 #\u0026lt;==生成密钥对。\u0026lt; code=\u0026#34;\u0026#34;\u0026gt; [jump@cnsre ~]$ ssh-copy-id -i ~/.ssh/id_dsa.pub 192.168.33.130 #\u0026lt;==将公钥分发到其他服务器。\u0026lt; code=\u0026#34;\u0026#34;\u0026gt; Theauthenticity of host \u0026#39;192.168.33.130 (192.168.33.130)\u0026#39; can\u0026#39;t be established. RSA keyfingerprint is fd:2c:0b:81:b0:95:c3:33:c1:45:6a:1c:16:2f:b3:9a. Are yousure you want to continue connecting (yes/no)? yes Warning:Permanently added \u0026#39;192.168.33.130\u0026#39; (RSA) to the list of known hosts. jump@192.168.33.130\u0026#39;spassword: Now trylogging into the machine, with \u0026#34;ssh \u0026#39;192.168.33.130\u0026#39;\u0026#34;, and check in: .ssh/authorized_keys to makesure we haven\u0026#39;t added extra keys that you weren\u0026#39;t expecting. [jump@cnsre ~]$ ssh-copy-id -i ~/.ssh/id_dsa.pub 192.168.33.129 #\u0026lt;==将公钥分发到其他服务器。\u0026lt; code=\u0026#34;\u0026#34;\u0026gt; Theauthenticity of host \u0026#39;192.168.33.129 (192.168.33.129)\u0026#39; can\u0026#39;t be established. RSA keyfingerprint is fd:2c:0b:81:b0:95:c3:33:c1:45:6a:1c:16:2f:b3:9a. Are yousure you want to continue connecting (yes/no)? yes Warning:Permanently added \u0026#39;192.168.33.129\u0026#39; (RSA) to the list of known hosts. jump@192.168.33.129\u0026#39;spassword: Now trylogging into the machine, with \u0026#34;ssh \u0026#39;192.168.33.129\u0026#39;\u0026#34;, and check in: .ssh/authorized_keys to makesure we haven\u0026#39;t added extra keys that you weren\u0026#39;t expecting. 实现传统的远程连接菜单选择脚本。 菜单脚本如下：\n1 2 3 4 cat \u0026lt;\u0026lt;menu 1)LB01-172.16.1.200 2)exit menu 利用linux信号防止用户中断信号在跳板机上操作。 1 2 3 functiontrapper () { trap \u0026#39;:\u0026#39; INT EXIT TSTP TERM HUP #\u0026lt;==屏蔽这些信号。\u0026lt; code=\u0026#34;\u0026#34;\u0026gt; } 用户登录跳板机后即调用脚本 （不能命令行管理跳板机），并只能按管理员的要求选单。\n以下为实战内容。\n脚本放在跳板机上：\n将pofifle.d目录下创建，登录用户就自动执行的jump.sh 脚本 1 2 3 4 5 [root@cnsre ~]# echo \u0026#39;[ $UID -ne 0 ] \u0026amp;\u0026amp; . /server/scripts/jump.sh\u0026#39;\u0026gt;/etc/profile.d/jump.sh [root@cnsre ~]# cat /etc/profile.d/jump.sh #!/bin/sh [ $UID -ne 0 ] \u0026amp;\u0026amp; . /server/scripts/jump.sh #判断如果不是root用户，就执行jump.sh，然后调用/server/scripts/jump.sh脚本 编写跳板机主脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/sh ############################################ #zabbix微信告警脚本 #修改时间：2020-xx-xx #修改内容：修改内容描述 ############################################ trapper(){ trap \u0026#39;:\u0026#39; INT EXIT TSTP TERM HUP #\u0026lt;==定义需要屏蔽的信号，冒号表示啥都不做。 } main(){ while : do trapper clear cat\u0026lt;\u0026lt;menu 1)LB01-172.16.1.200 menu read -p\u0026#34;Pls input a num.:\u0026#34; num case \u0026#34;$num\u0026#34; in 1) echo \u0026#39;login in LB01.\u0026#39; ssh 172.16.1.200 ;; 110) read -p \u0026#34;your birthday:\u0026#34; char if [ \u0026#34;$char\u0026#34; = \u0026#34;0926\u0026#34; ];then exit sleep 3 fi ;; *) echo \u0026#34;select error.\u0026#34; esac done } main 执行效果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # su - cnsre #\u0026lt;==切到普通用户即弹出菜单，工作中直接用cnsre登录，即弹出菜单。 1) LB01-172.16.1.200 Pls inputa num.:1 #\u0026lt;==选1进入LB01 服务器。 login in192.168.33.129. Lastlogin: Tue Oct 11 17:23:52 2016 from 192.168.33.128 [jump@cnsre~]$ #\u0026lt;==按ctrl+d退出到跳板机服务器再次弹出菜单。 1) LB01-172.16.1.200 [jump@cnsre~]$ #\u0026lt;==按ctrl+d退出到跳板机服务器再次弹出菜单。 1) LB01-172.16.1.200 Pls inputa num.:110 #\u0026lt;==选110进入跳板机命令提示符。 yourbirthday:0926 #\u0026lt;==需要输入特别码才能进入的，这里管理员通道，密码要保密呦。 [root@cnsre]# #\u0026lt;==跳板机管理命令行。 最终脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 #!/bin/bash ############################################ #登录服务器选择菜单 # 2020-1-15 v1 # #修改时间：2020-xx-xx #修改内容：修改内容描述 ############################################ trapper(){ trap \u0026#39;:\u0026#39; INT EXIT TSTP TERM HUP #\u0026lt;==定义需要屏蔽的信号，冒号表示啥都不做。 } main(){ while : do trapper clear cat\u0026lt;\u0026lt;menu 1.GIT-GC11-PROD 2.GIT-GC11-运维主机 3.[exit] menu read -p\u0026#34;请出入对应的数字:\u0026#34; a menu1 (){ cat\u0026lt;\u0026lt;EOF 1.MiniProgram 2.H-Link 3.SpeedBoat 按任意键返回主菜单 EOF read -p\u0026#34;请出入对应的数字:\u0026#34; num1 } menu2 (){ cat\u0026lt;\u0026lt;EOF 1.zabbix 2.jenkins 3.elk 按任意键返回主菜单 EOF read -p\u0026#34;请出入对应的数字:\u0026#34; num2 } #######################[ menu1 ]############################ [ $a -eq 1 ] \u0026amp;\u0026amp; { clear menu1 [ $num1 -eq 1 ] \u0026amp;\u0026amp; { echo \u0026#34;休眠3秒\u0026#34; sleep 3 echo \u0026#34;进入小程序成功\u0026#34; menu1 } [ $num1 -eq 2 ] \u0026amp;\u0026amp; { echo \u0026#34;休眠3秒\u0026#34; sleep 3 echo \u0026#34;进入H-link成功\u0026#34; menu1 } [ $num1 -eq 3 ] \u0026amp;\u0026amp; { echo \u0026#34;休眠3秒\u0026#34; sleep 3 echo \u0026#34;进入SpeedBoat成功\u0026#34; menu1 } } ####################[ menu2 ]######################################## [ $a -eq 2 ] \u0026amp;\u0026amp; { clear menu2 [ $num2 -eq 1 ] \u0026amp;\u0026amp; { echo \u0026#34;休眠3秒\u0026#34; sleep 3 echo \u0026#34;进入zabbix 成功\u0026#34; menu2 } [ $num2 -eq 2 ] \u0026amp;\u0026amp; { echo \u0026#34;ssh jenkins\u0026#34; ssh -i /home/xue/key/CI-CD.pem ec2-user@xx.xx.xx.xx echo \u0026#34;ssh jenkins ok\u0026#34; menu2 } [ $num2 -eq 3 ] \u0026amp;\u0026amp; { echo \u0026#34;休眠3秒\u0026#34; sleep 3 echo \u0026#34;进入elk成功\u0026#34; menu2 } } [ $a -eq 3 ] \u0026amp;\u0026amp; { echo \u0026#34;退出成功\u0026#34; exit } [ $a -eq 100 ] \u0026amp;\u0026amp; { read -p\u0026#34;匹配成功，请输入口令:\u0026#34; mima if [ \u0026#34;$mima\u0026#34; = \u0026#34;5418\u0026#34;];then sudo -i sleep 3 fi } done } main 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210421130319/\n相关话题：https://www.cnsre.cn/tags/shell/\n","description":"开发企业级Shell跳板机案例。要求用户登录到跳板机仅能执行管理员给定的选项动作，不允许以任何形式中断脚本到跳板机服务器上执行任何系统命令。","id":73,"section":"posts","tags":["shell","跳板机"],"title":"Shell 开发跳板机功能脚本","uri":"https://www.cnsre.cn/posts/210421130319/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210419172129/\n相关话题：https://www.cnsre.cn/tags/docker/\n在如今的互联网时代，随着软件开发复杂度的不断提高，软件开发和发布管理也越来越重要。目前已经形成一套标准的流程，最重要的组成部分就是持续集成（Continuous Integration，CI）及持续部署、交付（CD）。在此，我们来以一个案例初步了解 CI 流程。那么什么是 CI 呢？简单来讲，CI 就是将传统的代码合并、构建、部署、测试都集成在一起，不断地执行这个过程，并对结果进行反馈。\nCI 流程设计图\n工作流程 开发人员提交代码到Git版本仓库； Jenkins人工/定时触发项目构建； Jenkins拉取代码、代码编码、打包镜像、推送到镜像仓库； Jenkins在Docker主机创建容器并发布\n服务器规划 IP地址 角色 10.0.0.111 Jenkins，Docker，JDK，Maven 10.0.0.100 Harbor（Docker，docker-compose），Git 部署Git代码版本仓库 10.0.0.100 操作\n1 2 3 4 5 6 7 8 # yum install git -y # useradd git # passwd git # su – git # mkdir wenlong.git # cd wenlong.git/ #初始化仓库 # git --bare init 10.0.0.111 操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 Jenkins主机测试访问该仓库： git clone git@10.0.0.100:/home/tools/git/wenlong.git 正克隆到 \u0026#39;wenlong\u0026#39;... git@10.0.0.100\u0026#39;s password: \u0026#34;git账号的密码\u0026#34; warning: 您似乎克隆了一个空版本库。 模拟生产项目，拉取github上的一个demo，并上传至本地git库 # mv tomcat-java-demo-master/* wenlong/ # git add . 需要验证 # git config --global user.email \u0026#34;xuewenlong@123.com\u0026#34; # git config --global user.name \u0026#34;xuewenlong\u0026#34; # git commit -m \u0026#34;all\u0026#34; # git push origin master 部Harbor镜像仓库 10.0.0.100 操作\n参考：harbor私有仓库安装\nJenkins主机安装Docker并配置可信任\n10.0.0.111 操作 1 2 3 4 5 6 7 8 # wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo # yum install docker-ce -y # cat /etc/docker/daemon.json {\u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f1361db2.m.daocloud.io\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;192.168.31.63\u0026#34;] } # systemctl start docker # systemctl enable docker Jenkins环境部署 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # tar zxvf jdk-8u45-linux-x64.tar.gz # mv jdk1.8.0_45 /usr/local/jdk # tar zxf apache-maven-3.5.0-bin.tar.gz # mv apache-maven-3.5.0 /usr/local/maven # vim /etc/profile JAVA_HOME=/usr/local/jdk PATH=$PATH:$JAVA_HOME/bin:/usr/local/maven/bin export JAVA_HOME PATH # source /etc/profile 在10.0.0.111主机安装Jenkins，下载Tomcat二进制包将war包到webapps下即可： # wget http://mirrors.jenkins.io/war-stable/latest/jenkins.war # wget http://mirrors.shu.edu.cn/apache/tomcat/tomcat-8/v8.5.38/bin/apache-tomcat-8.5.38.tar.gz # tar zxf apache-tomcat-8.5.38.tar.gz # mv apache-tomcat-8.5.38 /usr/local/tomcat-jenkins # rm -rf /usr/local/tomcat-jenkins/webapps/* # mv jenkins.war /usr/local/tomcat-jenkins/webapps/ROOT.war # cd /usr/local/tomcat-jenkins/bin/ # ./startup.sh 启动后，浏览器访问http://10.0.0.111:8080/，按提示输入密码，登录即可\nJenkins安装必要插件 由于jenkins是离线安装，所有在此需要配置一下插件下载地址：系统管理\u0026ndash;\u0026gt;插件管理\u0026ndash;\u0026gt;Advanced\n修改内容 1 2 3 jenkins插件清华大学镜像地址 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json http://mirror.xmission.com/jenkins/updates/update-center.json 将https修改为http 再点Submit\nSubmit后点击Available，Check now此时我们可以看到很多可获得插件\n安装git和pipeline插件。\nJenkins流水线部署 pipeline 是一套运行于jenkins上的工作流框架，将原本独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化。\n创建一个pipeline类型的Job：\n选择流水线类型\n到这里我们就开始配置Pipeline script，点击Pipeline语法，来自动生成我们需要的配置。\n如下图，我们Git方式，配置Git仓库地址，再添加认证相关。\n这里我们使用的是秘钥认证方式，需要将jenkins上生成的公钥发送到git服务器上，然后将jenkins上的生成的私钥内容粘贴到下图Key中，这样jenkins就可以免交互的拉取git仓库中的代码了。\n1 2 3 4 5 [root@docker-jenkins bin]# ssh-keygen [root@docker-jenkins bin]# cd [root@docker-jenkins ~]# ls .ssh/ id_rsa id_rsa.pub known_hosts [root@docker-jenkins ~]# ssh-copy-id git@10.0.0.100 配置完成后，我们就可以生成Pipeline脚本了。点击下方Generate Pipeline Script，然后复制方框内的内容。\n编写我们所需要的Pipeline脚本如下，将其粘贴到script的拉取代码模块中，并修改分支master为${branch}，其他模块内容自行编写。\nnode { def mvnHome stage(\u0026#39;Preparation\u0026#39;) { // for display purposes //拉取代码 checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/master\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;7c00a680-53fd-42db-a12a-21b803dd6c40\u0026#39;, url: \u0026#39;git@10.0.0.100:/home/tools/git/wenlong.git\u0026#39;]]]) } //编译代码 stage(\u0026#39;Build\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; export JAVA_HOME=/home/tools/jdk1.8.0_221 mvn clean package -Dmaven.test.skip=true \u0026#39;\u0026#39;\u0026#39; } // 项目打包到镜像并推送到镜像仓库 stage(\u0026#39;Build and Push Image\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; REPOSITORY=10.0.0.100/library/wenlong:${branch} cat \u0026gt; Dockerfile \u0026lt;\u0026lt; EOF FROM 10.0.0.100/library/tomcat:v1 LABEL maintainer wenlong RUN rm -rf /usr/local/tomcat/webapps/* ADD target/*.war /usr/local/tomcat/webapps/ROOT.war EOF docker build -t $REPOSITORY . docker login 10.0.0.100 -u admin -p Harbor12345 docker push $REPOSITORY \u0026#39;\u0026#39;\u0026#39; } // 部署到Docker主机 stage(\u0026#39;Deploy to Docker\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; REPOSITORY=10.0.0.100/library/wenlong:${branch} docker rm -f tomcat-java-demo |true docker pull $REPOSITORY docker container run -d --name wenlong -p 88:8080 $REPOSITORY \u0026#39;\u0026#39;\u0026#39; } } 在Pipeline脚本里面我们指定了一个branch参数，所以我们需要传递一个参数变量，这里我们选择参数化构建，默认值为master分支。\n然后保存配置。\n发布测试 回到主界面，我们开始构建任务：\n查看构建成功后的图形构建过程：\n通过浏览器来访问java项目：http://10.0.0.111:88/\n至此部署完成\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210419172129/\n相关话题：https://www.cnsre.cn/tags/docker/\n","description":"基于Docker构建企业JenkinsCICD自动发布","id":74,"section":"posts","tags":["jenkins","docker"],"title":"使用Docker构建JenkinsCICD自动发布","uri":"https://www.cnsre.cn/posts/210419172129/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210419130919/\n相关话题：https://www.cnsre.cn/tags/shell/\n一、服务器被入侵的前夕 何时被入侵的？ 入侵者使用哪个账号登录的？ 入侵者身在何处？ 入侵者做过什么操作？ 如何避免服务器中毒，被入侵\u0026quot; 所谓知己知彼，方能百战不殆 二、剖析问题并解决 针对上面 前3个问题，开发了一个企业微信二次验证码的安全功能，详细内容如下：\n1、企业微信配置 1.1 获取AgentId（AppID）、Secret 创建一个企业微信应用\n1.2 获取 CropID 点击我的企业\u0026ndash;企业信息\n2、监控用户登录，发送通知给微信 放到/etc/profile.d/ 登录自动触发\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash #WeiXin ENV------------------------------------------------------------------------------------- CropID=\u0026#39;ww022bebbed74xxxx\u0026#39; Secret=\u0026#39;RauJ_-t-LxBhfEN7g1sh4OhVB_vREBWvqeFaaxxxxx\u0026#39; APIURL=\u0026#34;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=$CropID\u0026amp;corpsecret=$Secret\u0026#34; TOKEN=$(/usr/bin/curl -s -G $APIURL | awk -F\\\u0026#34; \u0026#39;{print $10}\u0026#39;) POSTURL=\u0026#34;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=$TOKEN\u0026#34; ##WeiXin body-------------------------------------------------------------------------------------- function body() { local int AppID=1000004 local UserID=xuewenlong local PartyID=2 printf \u0026#39;{\\n\u0026#39; printf \u0026#39;\\t\u0026#34;touser\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$UserID\u0026#34;\\\u0026#34;\u0026#34;,\\n\u0026#34; printf \u0026#39;\\t\u0026#34;toparty\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$PartyID\u0026#34;\\\u0026#34;\u0026#34;,\\n\u0026#34; printf \u0026#39;\\t\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;,\\n\u0026#39; printf \u0026#39;\\t\u0026#34;agentid\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$AppID\u0026#34;\\\u0026#34;\u0026#34;,\\n\u0026#34; printf \u0026#39;\\t\u0026#34;text\u0026#34;: {\\n\u0026#39; printf \u0026#39;\\t\\t\u0026#34;content\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$Msg\u0026#34;\\\u0026#34;\u0026#34;\\n\u0026#34; printf \u0026#39;\\t},\\n\u0026#39; printf \u0026#39;\\t\u0026#34;safe\u0026#34;:\u0026#34;0\u0026#34;\\n\u0026#39; printf \u0026#39;}\\n\u0026#39; } Status=`who am i | awk \u0026#39;{print $NF}\u0026#39; | sed \u0026#39;s/(//g\u0026#39; | sed \u0026#39;s/)//g\u0026#39;` if [ -n \u0026#34;$Status\u0026#34; ]; then Msg=\u0026#34;有用户上线请注意:\\n主机名：`hostname`\\n主机ip：`ifconfig ens33 | grep \u0026#34;inet\u0026#34; | awk \u0026#39;NR==1{ print $2}\u0026#39;`\\n登录用户：`whoami`\\n地址来源：\u0026#34;$Status\u0026#34;\u0026#34; /usr/bin/curl -s --data-ascii \u0026#34;$(body guozhiheng0123 $2)\u0026#34; $POSTURL 2\u0026gt;\u0026amp;1 \u0026gt; /dev/null fi 3、登录用户需要二次验证码 验证 read 读入\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #!/bin/bash ############################################ #通过微信发送验证码 #cnsre 2020-4-26 V1 # #修改者：xxx #修改时间：2020-xx-xx #修改内容：修改内容描述 ############################################ ##WeiXin ENV------------------------------------------------------------------------------------- CropID=\u0026#39;ww022bebbed749xxxx\u0026#39; Secret=\u0026#39;RauJ_-t-LxBhfEN7g1sh4OhVB_vREBWvqeFaaxxxxx\u0026#39; APIURL=\u0026#34;https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=$CropID\u0026amp;corpsecret=$Secret\u0026#34; TOKEN=$(/usr/bin/curl -s -G $APIURL | awk -F\\\u0026#34; \u0026#39;{print $10}\u0026#39;) POSTURL=\u0026#34;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=$TOKEN\u0026#34; ##WeiXin body-------------------------------------------------------------------------------------- function body() { local int AppID=1000004 local UserID=xuewenlong local PartyID=2 printf \u0026#39;{\\n\u0026#39; printf \u0026#39;\\t\u0026#34;touser\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$UserID\u0026#34;\\\u0026#34;\u0026#34;,\\n\u0026#34; printf \u0026#39;\\t\u0026#34;toparty\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$PartyID\u0026#34;\\\u0026#34;\u0026#34;,\\n\u0026#34; printf \u0026#39;\\t\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;,\\n\u0026#39; printf \u0026#39;\\t\u0026#34;agentid\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$AppID\u0026#34;\\\u0026#34;\u0026#34;,\\n\u0026#34; printf \u0026#39;\\t\u0026#34;text\u0026#34;: {\\n\u0026#39; printf \u0026#39;\\t\\t\u0026#34;content\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$Msg\u0026#34;\\\u0026#34;\u0026#34;\\n\u0026#34; printf \u0026#39;\\t},\\n\u0026#39; printf \u0026#39;\\t\u0026#34;safe\u0026#34;:\u0026#34;0\u0026#34;\\n\u0026#39; printf \u0026#39;}\\n\u0026#39; } Status=`who am i | awk \u0026#39;{print $NF}\u0026#39; | sed \u0026#39;s/(//g\u0026#39; | sed \u0026#39;s/)//g\u0026#39;` if [ -n \u0026#34;$Status\u0026#34; ]; then RANDOM=$(date +%s) echo $RANDOM \u0026gt;/tmp/pass.txt PASS=`tail -n 1 /tmp/pass.txt` Msg=\u0026#34;你的验证码是：\u0026#34;$PASS\u0026#34;\u0026#34; /usr/bin/curl --data-ascii \u0026#34;$(body xuewenlong $2)\u0026#34; $POSTURL \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 trap \u0026#34;\u0026#34; 2 read -p \u0026#34;请输入验证码:\u0026#34; A if [ \u0026#34;$A\u0026#34; != \u0026#34;xuewenlong\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$A\u0026#34; != \u0026#34;$PASS\u0026#34; ]; then echo \u0026#34;Verification Code Fail, Now Exit!!!\u0026#34; sleep 1 logout else echo \u0026#34; Welcome to BSH-GC11 System \u0026#34; fi fi 三、入侵者做过什么操作 一般通过几种手段去做：\n全站md5指纹识别，确认哪些文件被修改过 jumpserver 视频录像 普通用户sudo操作日志 系统日志\n1、钉钉配置 钉钉的配置与微信大致相同，具体不在说明，下面直接展示脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 DDcode.sh #!/bin/bash ############################### # 2020-4-26 # # by wenlong # # 通过钉钉接口发送验证码二次验证 # ############################### trap \u0026#34;\u0026#34; 1 read -p \u0026#34;请输入你的钉钉手机号:\u0026#34; user if [ ${#user} -ne 11 ]; then echo \u0026#34;请出入有效手机号码\u0026#34; sleep 1 logout fi ##dingding ENV------------------------------------------------------------------------------------- Dingding_Url=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=732b97ff63d6bce620025c3eb973ca39c668847260e7d2c9f0b43cf780be0c83\u0026#34; Status=`who am i | awk \u0026#39;{print $NF}\u0026#39; | sed \u0026#39;s/(//g\u0026#39; | sed \u0026#39;s/)//g\u0026#39;` if [ -n \u0026#34;$Status\u0026#34; ]; then RANDOM=$(date +%s) echo $RANDOM \u0026gt;/tmp/pass.txt PASS=`tail -n 1 /tmp/pass.txt` Msg=\u0026#34;你的验证码是：\u0026#34;$PASS\u0026#34;\u0026#34; curl \u0026#34;${Dingding_Url}\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#34; { \u0026#39;msgtype\u0026#39;: \u0026#39;text\u0026#39;, \u0026#39;text\u0026#39;: {\u0026#39;content\u0026#39;: \u0026#39;${Msg}\\n\u0026#39;}, \u0026#39;at\u0026#39;: {\u0026#39;atMobiles\u0026#39;: [ \u0026#39;${user}\u0026#39; ], \u0026#39;isAtAll\u0026#39;: false} }\u0026#34; \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 trap \u0026#34;\u0026#34; 2 read -p \u0026#34;请输入验证码:\u0026#34; code if [ \u0026#34;$code\u0026#34; != \u0026#34;xuewenlong\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$code\u0026#34; != \u0026#34;$PASS\u0026#34; ]; then echo \u0026#34;验证码验证失败!!!\u0026#34; sleep 1 logout else echo \u0026#34; Welcome to shvm01 System \u0026#34; fi fi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 login.sh #!/bin/bash ############################### # 2020-4-26 # # by wenlong # # 通过钉钉接口发送用户登录信息 # ############################### Dingding_Url=\u0026#34;https://oapi.dingtalk.com/robot/send?access_token=732b97ff63d6bce620025c3eb973ca39c668847260e7d2c9f0b43cf780be0c83\u0026#34; Status=`who am i | awk \u0026#39;{print $NF}\u0026#39; | sed \u0026#39;s/(//g\u0026#39; | sed \u0026#39;s/)//g\u0026#39;` if [ -n \u0026#34;$Status\u0026#34; ]; then Msg=\u0026#34;有用户上线请注意:\\n主机名：`hostname`\\n主机ip：`ifconfig eth0 | grep \u0026#34;inet\u0026#34; | awk \u0026#39;NR==1{ print $2}\u0026#39;`\\n登录用户：`whoami`\\n地址来源：\u0026#34;$Status\u0026#34;\u0026#34; curl \u0026#34;${Dingding_Url}\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#34; { \u0026#39;msgtype\u0026#39;: \u0026#39;text\u0026#39;, \u0026#39;text\u0026#39;: {\u0026#39;content\u0026#39;: \u0026#39;${Msg}\\n\u0026#39;}, \u0026#39;at\u0026#39;: {\u0026#39;atMobiles\u0026#39;: [\u0026#39;${user}\u0026#39; ], \u0026#39;isAtAll\u0026#39;: false} }\u0026#34; \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 fi 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210419130919/\n相关话题：https://www.cnsre.cn/tags/shell/\n","description":"如果您的服务器被黑客入侵，很有可能获取到您的用户名密码，直接登录服务器进行侵略，或者搞破坏，所以我们需要防护，但如何防护呢，被黑了你又想知道什么呢？？","id":75,"section":"posts","tags":["shell"],"title":"微信、钉钉验证登录服务器","uri":"https://www.cnsre.cn/posts/210419130919/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210414131058/\n相关话题：https://www.cnsre.cn/tags/故障集/\n故障描述 在我们的生产环境中，我们制作了一个健康检查页面，并通过脚本去监控他的健康状态，可是在前天（2020-5-30 周六）下午 18:50 左右的时候收到告警健康检查页面故障，等我登录服务器排查故障的时候发现是curl命令报错，报错的内容为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [root@ip-10-0-10-100 ~]# curl -v https://xxxxxx.cn/hcaextension/hcmini/v1/healthyCheck * Trying 54.223.xxx.xx... * TCP_NODELAY set * Connected to xxxxxxxx.cn (54.223.xx.xx) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH * successfully set certificate verify locations: * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * TLSv1.2 (OUT), TLS header, Certificate Status (22): * TLSv1.2 (OUT), TLS handshake, Client hello (1): * TLSv1.2 (IN), TLS handshake, Server hello (2): * TLSv1.2 (IN), TLS handshake, Certificate (11): * TLSv1.2 (OUT), TLS alert, certificate expired (557): * SSL certificate problem: certificate has expired * Closing connection 0 curl: (60) SSL certificate problem: certificate has expired More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. 在通过我今天上午的测试，我发现AWS EC2为：Linux 、Linux2 的操作系统不能够正常使用，在AWS EC2 -Centos 7.7、阿里云以及物理机房中测试是没有问题的。\n分析过程 经过查看报错信息，发现是由于SSL握手的时候证书验证错误导致的，以下是排查的步骤：\nCurl 的-k参数可以忽略SLL证书的验证，您可以添加-k参数临时避免遇到此错误，如下是我的测试，发现可以正常的访问页面： 1 2 curl -k https://xxxxxx.cn/hcaextension/hcmini/v1/healthyCheck {\u0026#34;code\u0026#34;:0,\u0026#34;msg\u0026#34;:\u0026#34;成功\u0026#34;,\u0026#34;messageid\u0026#34;:\u0026#34;29250cf5-176b-4993-a724-e5c9d7cc2ace\u0026#34;} 通过进一步分析证书“xxxxx.cn”，我们发现证书链是存在问题的，我们的证书自身并没有过期，但是一个证书链证书过期了，我们提取了证书链的信息，您可以参考附件的信息。\n检测地址\n解决方法 更新证书链，将过期的证书链信息去除，尝试是否可以正常访问：\nhttps://docs.amazonaws.cn/IAM/latest/UserGuide/id_credentials_server-certs.html\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210414131058/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"生产环境证书链证书过期问题","id":76,"section":"posts","tags":["ssl证书","故障集"],"title":"证书链证书过期问题","uri":"https://www.cnsre.cn/posts/210414131058/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210413131020/\n相关话题：https://www.cnsre.cn/tags/故障集/\n问题描述 2020年7月13日一大早收到告警，测试环境数据库CPU告警。\n登录aws查看监控如下图 问题分析 出现这种cpu 100%的问题，一般都是因为sql性能问题导致的。\n主要表现于 cpu消耗过大，有慢sql造成、慢sql全表扫描，扫描数据库过大，内存排序，队列等等\n并发现写入相对于查询来说比较高（这是一个关键点）\n有了大概的思路下边开始排查吧\n查看进程 show full processlist; 发现有大量的语句状态为 sending data sending data: sql正从表中查询数据，如果查询条件没有适当索引，会导致sql执行时间过长。\n查看慢日志配置 mysql\u0026gt; show variables like \u0026#39;slow_query%\u0026#39;; +---------------------+----------------------------------------------+ | Variable_name | Value | +---------------------+----------------------------------------------+ | slow_query_log | ON | | slow_query_log_file | /rdsdbdata/log/slowquery/mysql-slowquery.log | +---------------------+----------------------------------------------+ 2 rows in set mysql\u0026gt; show variables like \u0026#39;slow_launch_time\u0026#39;; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | slow_launch_time | 1 | +------------------+-------+ 1 row in set 看到慢日志已经开启\n登录aws cloudwatch查看慢日志发现大部分为这条sql\n# User@Host: admin[admin] @ [10.0.11.12] Id: 2302 # Query_time: 3.602910 Lock_time: 0.100585 Rows_sent: 2 Rows_examined: 4454 SET timestamp=1594629311; SELECT a.enum_value,a.enum_value FROM external_mapping a LEFT JOIN external_command_key b ON a.command_id=b.id LEFT JOIN external_command_options c ON a.options_id=c.id LEFT JOIN external_command_key d ON a.command_id=d.id LEFT JOIN category h ON a.category_id=h.id where 1=1 AND b.code=\u0026#39;Common.Status.Event\u0026#39; AND c.code=\u0026#39;Common.Setting.Rm4Valve\u0026#39; AND d.code=\u0026#39;Rm4_Valve\u0026#39; AND a.platform_id=119 AND h.cname = \u0026#39;TT\u0026#39;; 查看是否有锁表 mysql\u0026gt; show OPEN TABLES where In_use \u0026gt; 0; #查看是否有锁表 SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; #查看正在锁的事务 Empty set SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; #查看等待锁的事务 Empty set 暂时没有看到锁表的情况 查看缓存命中 mysql\u0026gt; show global status like \u0026#39;Qca%\u0026#39;; +-------------------------+-----------+ | Variable_name | Value | +-------------------------+-----------+ | Qcache_free_blocks | 1 | | Qcache_free_memory | 134199912 | | Qcache_hits | 0 | | Qcache_inserts | 0 | | Qcache_lowmem_prunes | 0 | | Qcache_not_cached | 44950579 | | Qcache_queries_in_cache | 0 | | Qcache_total_blocks | 1 | +-------------------------+-----------+ 8 rows in set Qcache_hits：查询缓存命中次数。 Qcache_inserts：将查询和结果集写入到查询缓存中的次数。 Qcache_not_cached：不可以缓存的查询次数。 Qcache_queries_in_cache：查询缓存中缓存的查询量。 查看到缓存命中为0%\n查看引擎状态 mysql\u0026gt; show engine innodb status; 通过上边一系列的查询，发现以下几个问题\n1、慢查询、全表扫描过多 描述\n慢sql：查看到sql语句执行时间过长。\n全表扫描：这个策略用于检查百分比（(Handler_read_rnd+Handler_read_rnd_next)/(Handler_read_first+Handler_read_key+Handler_read_next+Handler_read_prev+Handler_read_rnd+Handler_read_rnd_next)）。 这是一个需要读取全表内容的操作，而不是仅读取使用索引选定的部分。 通常使用小型查找表执行，或者在具有大型表的数据仓库情况下执行而其中所有可用数据都被聚合和分析。\n建议\n慢sql： 根据sql 检查语句并进行索引优化。\n全表扫描:应该尽量保持这个值尽可能的低。尝试隔离那些不使用索引的查询。一旦识别了那些查询，请创建适当的索引或重写查询以使用索引。MySQL 有一个很棒的功能 - 慢速查询日志，它允许你记录所有需要超过指定时间运行的查询。慢速速查询日志可用于识别需要很长时间才能完成的查询。\n2、数据库最大并发连接数量 描述\n当服务器启动后，（max_used_connections）变量将提供一个基准，以帮助你确定服务器支持的最大连接数量。 它还可以帮助进行流量分析。\n建议\n如果需要支持更多的连接，应该增加变量 max_connections 的值。MySQL 支持的最大连接数量是取决于给定平台上线程库的质量、可用 RAM 的数量、每个连接可使用多少 RAM、每个连接的工作负载以及所需的响应时间。\n3、查询缓存要配置 缓存描述\n这个策略用于检查查询缓存命中率（Qcache_hits/(Qcache_hits + Com_select)）。 MySQL 查询缓存将缓存一个分析的查询及其整个结果集。 当你有许多小型的查询返回小型数据集时，这是非常好的，因为查询缓存将允许返回结果立即可供使用，而不是每次发生时都重新运行查询。\n建议\n理想情况下，查询缓存的命中率应该接近 100%。MySQL 的查询缓存是一项强大的技术，并且在管理良好的情况下可以显着提高数据库的吞吐量。一旦你的应用程序被创建，你可以看看它如何使用数据库，并相应地调整查询缓存。有足够大的缓存，避免碎片化和排除大型的查询，你就应该能够保持极高的缓存命中率，并享受出色的性能。\n处理过程 根据上边发现的问题进行了配置的修改\n1、修改慢查询以及全表扫描 此问题联系开发进行索引优化，减少全表扫描。\n2、数据库最大连接数量 修改配置 max_user_connections 我这边设置的为1000\n3、查询缓存的配置 query_cache_size：分配用于缓存查询结果的内存量。 query_cache_limit：不要缓存大于此字节数的结果。 query_cache_type：对于查询结果，不缓存（= OFF），不缓存NO_CACHE（= ON），或仅缓存（= DEMAND）分别用012 表示 修改完数据但是需要重启才能生效。\n问题解决 正在准备空闲时间重启RDS的时候，开发那边有了进展。\n开发同事把缓存写错了！！！！😳😳😳\n总结 理下业务\n程序暴露接口给测试部门，测试部门在上报了50W条数据，开发这边程序有没有添加数据过滤（过滤掉垃圾数据），并且\u0026hellip;开发在程序中写错了缓存。所以导致相对于读取来说写入较高。因为在缓存查询不到想到的数据，就进行了全表扫描，继而出现了大量进程以及连接数队列等等。。\n处理问题可以，别主动背锅。。。在接手数据库的时候最好检查下配置，了解数据库的情况，在出现问题的时候能够最快速的定位解决问题。\n另外，经过此次的故障处理，加固了对业务以及数据库一些参数的理解。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210413131020/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"数据库CPU 100%处理记录","id":77,"section":"posts","tags":["mysql","故障集"],"title":"数据库CPU 100%处理记录","uri":"https://www.cnsre.cn/posts/210413131020/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210412150856/\n相关话题：https://www.cnsre.cn/tags/redis/\nLinux下Redis单机版安装与配置\n下载安装包并解压 cd /usr/local wget http://download.redis.io/releases/redis-4.0.6.tar.gz tar -zxvf redis-4.0.6.tar.gz 按照依赖环境 yum install gcc-c++ -y 稍微慢点需要等一小会儿\n编译安装 cd redis-4.0.6 make MALLOC=libcmake install 为Redis添加环境变量 vim /etc/profile PATH=$PATH:$HOME/bin:/usr/local/redis-4.0.6/src # 添加src目录路径到这里 export PATH 拷贝配置文件到/etc目录下 cp /usr/local/redis-4.0.6/redis.conf /etc/redis.conf 配置Redis vim /etc/redis.conf 修改以后台方式运行 开启密码访问\n开启远程访问\n执行启动脚本 ./redis-server /etc/redis.conf 登录Redis redis-cli -h 127.0.0.1 -p 6379 -a bsh@123 停止服务 127.0.0.1:6379\u0026gt; shutdown 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210412150856/\n相关话题：https://www.cnsre.cn/tags/redis/\n","description":"Linux Redis 单机版安装与配置","id":78,"section":"posts","tags":["redis"],"title":"Linux Redis 单机版安装与配置","uri":"https://www.cnsre.cn/posts/210412150856/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210409164113/\n相关话题：https://www.cnsre.cn/tags/aws/\n背景介绍 Amazon S3是互联网存储解决方案，能让所有开发人员访问同一个具备可扩展性、可靠性、安全性和快速价廉的数据存储基础设施。Amazon S3 提供了一个简单 Web 服务接口，可用于随时在 互联网上的任何位置存储和检索任何数量的数据。开发人员可以利用Amazon提供的REST API接口，命令行接口或者支持不同语言的SDK访问S3服务。\n对于原来使用本地目录访问数据的应用程序，比如使用本地磁盘或网络共享盘保存数据的应用系统，如果用户希望把数据放到S3上，则需要修改数据的访问方式，比如修改为使用AWS SDK 或CLI访问S3中存储的数据。为了让用户原来的应用系统能在不做修改的情况下直接使用Amazon S3服务，需要把S3存储桶作为目录挂载到用户服务器的本地操作系统上。常用的挂载工具有S3fs和SubCloud等。本文主要介绍如何利用S3fs将S3存储桶挂载到Amazon EC2 Linux实例上。\nS3fs介绍 S3fs是基于FUSE的文件系统，允许Linux和Mac Os X 挂载S3的存储桶在本地文件系统，S3fs能够保持对象原来的格式。关于S3fs的详细介绍，请参见：https://github.com/s3fs-fuse/s3fs-fuse\n利用S3fs挂载S3存储桶 准备 使用拥有足够权限的IAM账号登录AWS控制台。 创建S3存储桶，给存储桶命名如s3fs-mount-bucket（如果使用已有存储桶，本步骤可略过）。 有该S3存储桶访问权限的 IAM 用户，并为该IAM用户创建访问密钥。 关于如何创建IAM用户，请参见：http://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_users_create.html#id_users_create_console 关于如何为IAM用户创建访问密钥，请参见：http://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_credentials_access-keys.html 关于如何为IAM用户设置权限策略，请参见：http://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/access_policies_create.html https://aws.amazon.com/cn/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/\n创建并启动Amazon EC2 Linux实例 具体过程请参见：http://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/launching-instance.html\n安装和配置S3fs 安装s3f 安装必要的软件包 1 sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel 下载，编译并安装s3fs 1 2 3 4 5 6 git clone https://github.com/s3fs-fuse/s3fs-fuse.git cd s3fs-fuse ./autogen.sh ./configure make sudo make install 检查s3fs是否安装成功 1 2 3 4 5 [ec2-user@ip-172-31-23-148 s3fs-fuse]$ s3fs s3fs: missing BUCKET argument. Usage: s3fs BUCKET:[PATH] MOUNTPOINT [OPTION]... [ec2-user@ip-172-31-23-148 ~]$ which s3fs /usr/local/bin/s3fs 创建IAM用户访问密钥文件 IAM用户访问密钥内容可以写入当前用户默认密钥文件比如/home/ec2-user/.passwd-s3fs或者用户自己创建的文件。 命令格式：echo [IAM用户访问密钥ID]:[ IAM用户访问密钥] \u0026gt;[密钥文件名] 命令举例：下面的例子将在当前用户默认路径创建密钥文件 1 echo AKIAIOEO4E2VOHLxxxxx:2LXBboddEpRLmWl48i3+b4ziwPL3bJ4vxxxxxxxx \u0026gt; /home/ec2-user/.passwd-s3fs 请注意：访问海外AWS S3服务和中国 S3服务使用的是不同的IAM账号，对应不同的密钥。\n设置密钥文件只能够被当前用户访问 命令格式：chmod 600 [密钥文件名] 命令举例：下面的例子将设置密钥文件只能被当前用户访问 1 chmod 600 /home/ec2-user/.passwd-s3fs 手动挂载S3存储桶 S3fs挂载存储桶使用的命令是s3fs\ns3fs的命令格式是：\ns3fs BUCKET MOUNTPOINT [OPTION]… s3fs [S3存储桶名] [本地目录名] [OPTION] OPTION是可选项，格式是 –o \u0026lt;option_name\u0026gt;=\u0026lt;option_value\u0026gt;，常用的options有： 名称 含义 缺省值 passwd_file 指定挂载的密钥文件 connect_timeout 设置超时连接等待的时间，单位秒 300 url 设置访问s3的url http://s3.amazonaws.com endpoint 设置s3存储桶的endpoint us-east-1 allow_other 设置allow_other允许所有用户访问挂载点目录，设置这个选项需要在 /etc/fuse.conf 文件添加user_allow_other选项 手动挂载AWS海外区域S3存储桶\n命令格式：s3fs [S3存储桶名] [本地目录名] -o passwd_file=[密钥文件名] -o endpoint=[区域名]\n命令举例：下面的例子将名为s3fs-mount-bucket的新加坡区域S3存储桶挂载到指定的本地目录/home/ec2-user/s3mnt。 1 s3fs s3fs-mount-bucket /home/ec2-user/s3mnt -o passwd_file=/home/ec2-user/.passwd-s3fs -o endpoint=ap-northeast-1 手动挂载AWS中国北京区域S3存储桶\n命令格式：s3fs [S3存储桶名] [本地目录名] -o passwd_file=[密钥文件名] -o url=http://s3.cn-north-1.amazonaws.com.cn -o endpoint=cn-north-1 命令举例：下面的例子将名为s3fs-mount-bucket的北京区域S3存储桶挂载到本地目录/home/ec2-user/s3mnt。 1 s3fs s3fs-mount-bucket /home/ec2-user/s3mnt -o passwd_file=/home/ec2-user/.passwd-s3fs -o url=http://s3.cn-north-1.amazonaws.com.cn -o endpoint=cn-north-1 检查挂载结果\n挂载操作执行结束后，可以使用Linux df命令查看挂载是否成功。出现类似下面256T的s3fs文件系统即表示挂载成功。用户就可以进入本地挂载目录去访问存储在S3存储桶中的对象。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [ec2-user@ip-172-31-23-148 ~]$ df -h 文件系统 容量 已用 可用 已用% 挂载点 devtmpfs 488M 56K 488M 1% /dev tmpfs 498M 0 498M 0% /dev/shm /dev/xvda1 7.8G 1.2G 6.6G 15% / s3fs 256T 0 256T 0% /home/ec2-user/s3mnt [ec2-user@ip-172-31-23-148 ~]$ cd /home/ec2-user/s3mnt [ec2-user@ip-172-31-23-148 s3mnt]$ ls -l 总用量 1 -rw-rw-r-- 1 ec2-user ec2-user 19 10月 18 07:13 a.txt [ec2-user@ip-172-31-23-148 s3mnt]$ 卸载挂载的S3存储桶\n如果不再需要通过挂载方式访问S3存储桶，可以使用Linux “umount”命令卸载 1 2 3 4 5 6 7 8 9 10 11 [ec2-user@ip-172-31-23-148 ~]$ sudo umount /home/ec2-user/s3mnt [ec2-user@ip-172-31-23-148 ~]$ df -h 文件系统 容量 已用 可用 已用% 挂载点 devtmpfs 488M 56K 488M 1% /dev tmpfs 498M 0 498M 0% /dev/shm /dev/xvda1 7.8G 1.2G 6.6G 15% / 调试\n如果遇到手动挂载不成功的问题，请尝试在执行的命令后面添加下面的参数，并检查输出日志中的错误提示信息：\n命令格式：[完整的s3fs挂载命令] -d -d -f -o f2 -o curldbg 命令举例：下面的例子试图将名为s3fs-mount-bucket的S3存储桶挂载到指定的本地目录/home/ec2-user/s3mnt下，并输出挂载过程详细调试日志。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [ec2-user@ip-172-31-23-148 ~]$ s3fs s3fs-mount-bucket /home/ec2-user/s3mnt -o passwd_file=/home/ec2-user/.passwd-s3fs -o url=http://s3.cn-north-1.amazonaws.com.cn -o endpoint=cn-north-1 -d -d -f -o f2 -o curldbg [CRT] s3fs.cpp:set_s3fs_log_level(254): change debug level from [CRT] to [INF] [CRT] s3fs.cpp:set_s3fs_log_level(254): change debug level from [INF] to [DBG] [INF] s3fs.cpp:set_moutpoint_attribute(4196): PROC(uid=500, gid=500) - MountPoint(uid=500, gid=500, mode=40775) FUSE library version: 2.9.4 nullpath_ok: 0 nopath: 0 utime_omit_ok: 0 设置开机自动挂载S3存储桶 创建全局IAM用户访问密钥文件 切换Linux系统用户账号到root用户，把IAM用户访问密钥内容写入/etc/passwd-s3fs文件中，并限制该文件的访问权限。/etc/passwd-s3fs文件是s3fs保存IAM用户访问密钥的全局默认路径。\n请注意：访问海外AWS S3服务和中国 S3服务使用的是不同的IAM账号，对应不同的密钥。\n1 2 3 4 5 sudo su echo AKIAIOEO4E2VOHLxxxxx:2LXBboddEpRLmWl48i3+b4ziwPL3bJ4vxxxxxxxx \u0026gt; /etc/passwd-s3fs chmod 600 /etc/passwd-s3fs 修改/etc/fstab文件 编辑/etc/fstab文件，添加后面的自动挂载命令。\n1 vi /etc/fstab 自动挂载海外区域S3存储桶\n命令格式：s3fs#[S3存储桶名] [本地目录名] fuse _netdev,allow_other,endpoint=[区域名] 0 0 命令举例：添加下面的语句到/etc/fstab后，Linux系统启动后将自动把名为s3fs-mount-bucket的新加坡区域S3存储桶挂载到本地目录/home/ec2-user/s3mnt，并允许其它操作系统用户(非root用户)访问。 1 /usr/local/bin/s3fs#s3fs-mount-bucket /home/ec2-user/s3mnt fuse _netdev,allow_other,endpoint=ap-northeast-1 0 0 自动挂载中国北京区域S3存储桶\n命令格式：s3fs#[S3存储桶名] [本地目录名] fuse allow_other,url=http://s3.cn-north-1.amazonaws.com.cn,endpoint=cn-north-1 0 0 命令举例：添加下面的语句到/etc/fstab后，Linux系统启动将自动把名为s3fs-mount-bucket的北京区域S3存储桶挂载到本地目录/home/ec2-user/s3mnt下，并允许其它操作系统用户(非root用户)访问。 1 /usr/local/bin/s3fs#s3fs-mount-bucket /home/ec2-user/s3mnt fuse allow_other,url=http://s3.cn-north-1.amazonaws.com.cn,endpoint=cn-north-1 0 0 局限性 利用S3fs可以方便的把S3存储桶挂载在用户本地操作系统目录中，但是由于S3fs实际上是依托于Amazon S3服务提供的目录访问接口，所以不能简单的把S3fs挂载的目录和本地操作系统目录等同使用。用户使用S3f3挂载S3存储桶和直接访问S3服务有类似的使用场景。适用于对不同大小文件对象的一次保存（上传），多次读取（下载）。不适用于对已保存文件经常做随机修改，因为每次在本地修改并保存文件内容都会导致S3fs上传新的文件到Amazon S3去替换原来的文件。从访问性能上来说，通过操作系统目录方式间接访问Amazon S3存储服务的性能不如直接使用SDK或CLI接口访问效率高。以本地配置文件方式保存访问密钥的安全性也不如使用EC2 IAM角色方式高。\n关于S3fs使用时候需要注意的更多细节，请参考下面s3fs官网内容：\n“Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\nrandom writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links ” 通常S3不能提供与本地文件系统相同的性能或语义。进一步来说：\n随机写入或追加到文件需要重写整个文件 元数据操作比如列出目录会因为网络延迟原因导致性能较差 最终一致性设计可能临时导致过期数据 没有对文件或目录的原子重命名功能 挂载相同存储桶的多个客户端之间没有相互协调机制 不支持硬链接 总结 利用S3fs可以把共享的Amazon S3存储桶直接挂载在用户服务器本地目录下，应用不需要做修改就可以直接使用Amazon S3存储服务，这种方式可以作为临时解决方案将传统应用快速迁移到AWS平台。\n在已经提供了Amazon EFS（Elastic File System）服务的AWS区域，建议用户优先考虑使用Amazon EFS服务，因为它具有更高的性能。在目前还没有提供EFS服务的AWS区域，用户可以先暂时使用S3fs实现快速业务迁移。然后逐步调整S3数据访问实现方式，最终修改为使用AWS SDK或CLI方式高效并更加安全地访问S3存储服务。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210409164113/\n相关话题：https://www.cnsre.cn/tags/aws/\n","description":"利用S3fs在Amazon EC2 Linux实例上挂载S3存储桶","id":79,"section":"posts","tags":["aws","s3"],"title":"利用S3fs在Amazon EC2 Linux实例上挂载S3存储桶","uri":"https://www.cnsre.cn/posts/210409164113/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210408131033/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n问题描述 zabbix server 平稳运行有一段时间了，但是最近问题却来了，今天早上收到zabbixserver磁盘空间不足的告警。通过查看之后发现是大部分数据是zabbix 库的的数据 在进一步查看发现是history表和history_uint数据太多导致磁盘占用过多。\n问题分析 history_uint 该表存储的是监控项的无符号整型的数据。\n该数据的保存时长，取决于在监控项设置的 历史数据保留时长。\nhistory 这个表保存的是浮点型的。\n像 history_str 等保存的是 字符型数据。这些都是我们在设置监控项的对应的信息类型决定的。\n该数据的保存时长，取决于在监控项设置的 历史数据保留时长\n针对这个问题，我打算删除 history_uint 和 history 的一些历史数据。\n要删除history_uint里的数据，还需要注意一点，由于数据量比较多，我建议可以分多次少量数据进行删除，因为我一次删除90天的时候CPU已经吃不消了\u0026hellip;\n这样可以避免一次性删除数据过多导致数据库的负载比较大。（或者可以使用limit 10000）\n处理过程 我这里需要删除90天以前的数据下面是我的操作过程\n获取时间戳 #通过如下命令进行获取90天以前的时间戳 [root@zabbix-server ~]# date -d $(date -d \u0026#34;-90 day\u0026#34; +%Y%m%d) +%s 1590105600 登录数据库操作 [root@zabbix-server ~]# mysql -uzabbix -p Enter password: mysql\u0026gt; use zabbix; Database changed #delete history_uint mysql\u0026gt; delete from history_uint where clock \u0026lt; 1590105600 LIMIT 10000; Query OK, 1653 rows affected (1 min 45.42 sec) #delete history mysql\u0026gt; delete from history where clock \u0026lt; 1590105600 LIMIT 10000; Query OK, 0 rows affected (24.72 sec) 释放空间 上面执行删除后，数据的存储空间是没有减少的，因为对于delete from table_name where xxx 带条件的删除，不管是innodb还是MyISAM都不会释放空间，需要进行OPTIMIZE TABLE操作，进行释放空间。\n注意：在optimize table \u0026lsquo;表名\u0026rsquo; 运行过程中，MySQL会进行锁表。\noptimize table history_uin\nmysql\u0026gt; optimize table history_uint; +---------------------+----------+----------+-------------------------------------------------------------------+ | Table | Op | Msg_type | Msg_text | +---------------------+----------+----------+-------------------------------------------------------------------+ | zabbix.history_uint | optimize | note | Table does not support optimize, doing recreate + analyze instead | | zabbix.history_uint | optimize | status | OK | +---------------------+----------+----------+-------------------------------------------------------------------+ 2 rows in set (5 min 33.76 sec) optimize table history\nmysql\u0026gt; optimize table history; +----------------+----------+----------+-------------------------------------------------------------------+ | Table | Op | Msg_type | Msg_text | +----------------+----------+----------+-------------------------------------------------------------------+ | zabbix.history | optimize | note | Table does not support optimize, doing recreate + analyze instead | | zabbix.history | optimize | status | OK | +----------------+----------+----------+-------------------------------------------------------------------+ 2 rows in set (1 min 39.51 sec) 问题解决 待以上步骤都完成以后，检查磁盘可以看到问题解决 。\n不过想要一劳永益的话的话 还是需要写一个脚本来处理这个问题\n#!/bin/bash User=\u0026#34;zabbix\u0026#34; Passwd=\u0026#34;zabbix\u0026#34; Date=`date -d $(date -d \u0026#34;-90 day\u0026#34; +%Y%m%d) +%s` $(which mysql) -u${User} -p${Passwd} -e \u0026#34; use zabbix; DELETE FROM history WHERE \u0026#39;clock\u0026#39; \u0026lt; \u0026#39;$Date\u0026#39; LIMIT 10000; optimize table history; DELETE FROM history_str WHERE \u0026#39;clock\u0026#39; \u0026lt; \u0026#39;$Date\u0026#39; LIMIT 10000; optimize table history_str; DELETE FROM history_uint WHERE \u0026#39;clock\u0026#39; \u0026lt; \u0026#39;$Date\u0026#39; LIMIT 10000; optimize table history_uint; DELETE FROM history_text WHERE \u0026#39;clock\u0026#39; \u0026lt; $Date\u0026#39; LIMIT 10000; optimize table history_text; DELETE FROM trends WHERE \u0026#39;clock\u0026#39; \u0026lt; \u0026#39;$Date\u0026#39; LIMIT 10000; optimize table trends; DELETE FROM trends_uint WHERE \u0026#39;clock\u0026#39; \u0026lt; \u0026#39;$Date\u0026#39; LIMIT 10000; optimize table trends_uint; DELETE FROM events WHERE \u0026#39;clock\u0026#39; \u0026lt; \u0026#39;$Date\u0026#39; LIMIT 10000; optimize table events; \u0026#34; 另外历史数据过多是由于我们保存的历史数据的时间所致，我们可以根据需求设置历史数据的保留时长，例如一些相对不太重要的数据，我们可以将该值设置的更短一些，这样数据量也就随着减少了。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210408131033/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix数据库history表和history_uint数据太多导致磁盘占用过多","id":80,"section":"posts","tags":["zabbix","故障集"],"title":"Zabbix历史数据处理","uri":"https://www.cnsre.cn/posts/210408131033/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210406154942/\n相关话题：https://www.cnsre.cn/tags/zabbix/\nserver端操作 找一个安装有agent 的server 进行一下操作\n安装所需组件 1 2 3 4 5 6 yum install bc gcc gcc-c++ -y # 安装openssl yum install openssl -y # 验证openssl root@elk scripts]openssl version OpenSSL 1.0.2k-fips 26 Jan 2017 脚本内容 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh host=$1 end_date=`openssl s_client -servername $host -host $host -port 443 -showcerts \u0026lt;/dev/null 2\u0026gt;/dev/null | sed -n \u0026#39;/BEGIN CERTIFICATE/,/END CERT/p\u0026#39; | openssl x509 -text 2\u0026gt;/dev/null | sed -n \u0026#39;s/ *Not After : *//p\u0026#39;` if [ -n \u0026#34;$end_date\u0026#34; ] then end_date_seconds=`date \u0026#39;+%s\u0026#39; --date \u0026#34;$end_date\u0026#34;` now_seconds=`date \u0026#39;+%s\u0026#39;` echo \u0026#34;($end_date_seconds-$now_seconds)/24/3600\u0026#34; | bc fi 验证脚本 1 2 3 4 5 # 赋执行权限 [root@bac scripts] chmod +x check_ssl.sh [root@elk scripts]./check_ssl.sh xxxx-xxxx.cn 565 #单位为天 zabbix 配置文件中添加配置 1 2 [root@elk scripts]cat /etc/zabbix/zabbix_agentd.conf |grep ssl UserParameter=check.ssl[*],/etc/zabbix/scripts/check_ssl.sh $1、 重启zabbix agent 1 systemctl restart zabbix-agent web端操作 创建监控项 登录zabbix 创建新的监控项\n名称：随意填写 类型：Zabbix 客户端 键值：添加在配置文件中的check.ssl[*] *为你的域名 更新间隔因为证书不需要做实时的检查 所以时间可以设置长一点（12h或者24h） 检查创建监控项是否生效\n创建完成以后可以在检测\u0026ndash;最新数据中查看监控项\n创建触发器 监控项有了 接下来创建触发器\n创建完毕 来测试下告警\n把阈值调为600 天来测试下告警\n因为我们把更新间隔调的时间比较长 所以我们为了快速验证告警可以调小\n这个时候我们可以看到告警信息\n到这SSL 证书监控告警已经完成 看到告警信息之后记得阈值调回来 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210406154942/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix 监控ssl域名证书，到期后自动告警通知。","id":81,"section":"posts","tags":["ssl证书","zabbix"],"title":"zabbix 监控域名证书到期时间","uri":"https://www.cnsre.cn/posts/210406154942/"},{"content":" 作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/210406132721/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n环境介绍 zabbix版本：Zabbix 4.2.6\nzabbix server：10.0.10.234\nzabbix-agent：16台 Linux 7.x设备\n自动发现 自动发现的好处：快速发现，并自动添加主机，省去管理员配置的麻烦。\n自动发现的原理：自动发现功能是基于IP段进行扫描添加利用SNMP协议来接收消息实现自动添加\n自动注册 自动注册的相较于自动发现的有点就在于节省zabbix-server的资源\n自动注册也就是被动发现，等着业务机器来找zabbix server注册，自动注册的原理: 自动注册会根据host进行匹配,将符合条件的机器执行动作\n本文采用自动注册的方式做为示例。\nLinux 服务器端操作 agent rpm 安装包 点击下载 zabbix-agent-4.2.6-1.el7.x86_64.rpm\n安装脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 #!/bin/bash RD=\u0026#34;\\033[31m\u0026#34; # 错误消息 GR=\u0026#34;\\033[32m\u0026#34; # 成功消息 YL=\u0026#34;\\033[33m\u0026#34; # 告警消息 BL=\u0026#34;\\033[36m\u0026#34; # 日志消息 PL=\u0026#39;\\033[0m\u0026#39; echo \u0026#34;#############################################################################\u0026#34; echo -e \u0026#34;# 一键安装 zabbix-agent4.2.6脚本 #\u0026#34; echo -e \u0026#34;# ${GR}作者${PL}: sre运维博客 #\u0026#34; echo -e \u0026#34;# ${GR}网址${PL}: https:www.cnsre.cn #\u0026#34; echo -e \u0026#34;# ${GR}文章地址${PL}: https://www.cnsre.cn/posts/210406132721/ #\u0026#34; echo \u0026#34;#############################################################################\u0026#34; server_ip=$1 VALID_CHECK=$(echo $server_ip|awk -F. \u0026#39;$1\u0026lt;=255\u0026amp;\u0026amp;$2\u0026lt;=255\u0026amp;\u0026amp;$3\u0026lt;=255\u0026amp;\u0026amp;$4\u0026lt;=255{print \u0026#34;yes\u0026#34;}\u0026#39;) if echo $server_ip|grep -E \u0026#34;^[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}$\u0026#34;\u0026gt;/dev/null; then if [ $VALID_CHECK == \u0026#34;yes\u0026#34; ]; then echo -e \u0026#34;$GR 您的zabbix server IP 地址为：$server_ip $PL\u0026#34; else echo -e \u0026#34;$GR 您的zabbix server IP $server_ip 不可用!$PL\u0026#34; fi else echo -e \u0026#34;$RD 请输入zabbix server IP地址! $PL\u0026#34; echo -e \u0026#34;$RD 如：sh $0 192.168.10.100 $PL\u0026#34; exit 1 fi echo -e \u0026#34;$YL 是否确定安装 zabbix 4.2.6 版本?$PL\u0026#34; read -r -p \u0026#34;确定请按 y 任意键则退出！请选择：[y/n]\u0026#34; input if [[ $input != \u0026#34;y\u0026#34; ]]; then exit 1 else echo -e \u0026#34;$GR 开始安装 zabbix-agent 4.2.6 版本$PL\u0026#34; fi #定义变量 IP=$(ip addr | grep -E -o \u0026#39;[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\u0026#39; | grep -E -v \u0026#34;^127\\.|^255\\.|^0\\.\u0026#34; | head -n 1) checkwget=`rpm -qa wget` if [ -z $checkwget ];then yum install wget -y fi echo -e \u0026#34;$GR 下载zabbix-agent$PL\u0026#34; sleep 0.5 wget http://repo.zabbix.com/zabbix/4.2/rhel/7/x86_64/zabbix-agent-4.2.6-1.el7.x86_64.rpm rpm -ivh zabbix-agent-4.2.6-1.el7.x86_64.rpm rm -rf zabbix-agent-4.2.6-1.el7.x86_64.rpm echo -e \u0026#34;$GR 备份agent.conf配置文件$PL\u0026#34; sleep 0.5 mv /etc/zabbix/zabbix_agentd.conf /etc/zabbix/zabbix_agentd.conf.bak touch /etc/zabbix/zabbix_agentd.conf echo -e \u0026#34;$GR 正在写入配置文件$PL\u0026#34; sleep 0.5 cat \u0026gt;\u0026gt; /etc/zabbix/zabbix_agentd.conf \u0026lt;\u0026lt; EOF PidFile=/var/run/zabbix/zabbix_agentd.pid LogFile=/var/log/zabbix/zabbix_agentd.log LogFileSize=1 DebugLevel=3 Server=$server_ip ListenPort=10050 ServerActive=$server_ip Include=/etc/zabbix/zabbix_agentd.d/ Hostname=$IP HostMetadata=zabbix.bsh EOF echo -e \u0026#34;$GR 启动zabbix-agent服务$PL\u0026#34; systemctl start zabbix-agent \u0026amp;\u0026amp; systemctl enable zabbix-agent zabbixagentpid=`ps -ef |grep zabbix_agentd|grep -w \u0026#39;zabbix_agentd\u0026#39;|grep -v \u0026#39;grep\u0026#39;|awk \u0026#39;{print $2}\u0026#39;` if [ \u0026#34;$zabbixagentpid\u0026#34; ];then echo -e \u0026#34;$GR zabbix-agent 已经运行 $PL\u0026#34; else echo \u0026#34;$RD zabbix agent 安装失败！$PL\u0026#34; fi WEB 页面操作 配置\u0026ndash;动作\u0026ndash;事件源选择为自动注册\u0026ndash;创建动作\n设置自动注册的规则\n我这里设置主机名称为包含10的则添加注册，因为我的主机地址段都是在10段的\n然后坐等主机上线\n收到提醒后打开主机列表查看自动注册上来的主机\n看到已经注册上来的主机\n文章链接\nhttps://www.cnsre.cn/posts/210406132721/\n作者：SRE运维博客\n博客地址： https://www.cnsre.cn/\n文章地址：https://www.cnsre.cn/posts/210406132721/\n相关话题：https://www.cnsre.cn/tags/zabbix/\n","description":"zabbix 批量安装，自动注册。快速配置，节省管理员时间。","id":82,"section":"posts","tags":["zabbix"],"title":"zabbix 批量安装 自动注册","uri":"https://www.cnsre.cn/posts/210406132721/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210402140228/\n相关话题：https://www.cnsre.cn/tags/图床/\n使用github+jsdelivr搭建稳定的免费个人图床\u0026quot; 介绍 想搭建个人博客，但是对比了几家图床，觉得要不是不稳定，要不就是不支持https要不就是收费，想获取免费图床？跟我来吧。\n为了避免各位朋友再次踩坑我将我使用过的图床列出来\n七牛云 注册认证后有10G永久免费空间，每月10G国内和10G国外流量。 有免费ssl证书，但https流量收费。 七牛云30天后会回收测试域名，需要绑定已备案的域名。 又拍云 注册认证后有10G永久免费空间，每月15G的HTTP和HTTPS流量 提供两款可以免费续期的SSL证书 需要在网站底部添加又拍云logo及官网链接 默认测试域名为http，想要用https需要绑定自己的已备案域名。 SM.MS 永久存储免注册。 图片链接支持https。 国内访问速度缓慢。 每个图片最大5M，每次最多上传10张。 腾讯、阿里等云。 一开始免费（免费试用6个月） 时间、流量、空间大小均有限制 小众图床厂商 担心跑路啊\u0026hellip; 经过上边的对比和踩坑之后，最后还是决定免费获取一个稳定、免费、访问快速的图床。\n使用Github+jsdelivr利用Github的免费和jsdelivr的CND加速访问。在加上开源项目 PicGo工具一键上传，简直了。稳定性方面Github和jsdelivr都是大厂，不用担心跑路的问题，更不用担心容量和速度的问题。所以，免费获取开始。\n新建GitHub仓库 登录（没有账号就注册）GitHub，新建一个仓库。\n创建token 在主页依次选择Settings-Developer settings-Personal access tokens-Generate new token，填写好描述，勾选repo，然后点击Generate token生成一个Token，注意Token只会显示一次\n配置PicGo 前往下载 PicGo，安装好后开始配置图床。\n安装不在演示，直接配置。\n设定仓库名：按照用户名/ 图床仓库名的格式填写 设定分支名：master 设定Token：粘贴之前生成的Token 指定存储路径：填写想要储存的路径，如zops/，这样就会在仓库下创建一个名为 zops的文件夹，图片将会储存在此文件夹中 设定自定义域名：它的作用是，在图片上传后，PicGo会按照 自定义域名+储存路径+上传的图片名 的方式生成访问链接，放到粘贴板上。\n因为我们要使用 jsDelivr 加速访问，所以可以设置为https://cdn.jsdelivr.net/gh/用户名/图床仓库名 ，上传完毕后，我们就可以通过https://cdn.jsdelivr.net/gh/用户名/图床仓库名/图片路径加速访问我们的图片了，比如：https://cn-north-1-image.s3.cn-north-1.amazonaws.com.cn/cnsre/cnsre/20210107112646.png PicGo更多功能 PicGo是一个开源的工具设置好后，能够一键上传复制URL、html、markdown、UBB等连接甚至可以自定义连接。此外PicGo还有相册功能，可以对已上传的图片进行删除，修改链接等快捷操作，PicGo还可以生成不同格式的链接、支持批量上传、快捷键上传、自定义链接格式、上传前重命名等，更多功能自己去探索吧！\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210402140228/\n相关话题：https://www.cnsre.cn/tags/图床/\n","description":"想获取免费图床？github+jsdelivr创建免费稳定的个人图床。","id":83,"section":"posts","tags":["图床","jsdelivr","github","图床"],"title":"使用github+jsdelivr搭建免费稳定的个人图床","uri":"https://www.cnsre.cn/posts/210402140228/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210401140104/\n相关话题：https://www.cnsre.cn/tags/故障集/\nSystemd Journald占用资源过多 journald占用过多磁盘空间 方法一 检查当前journal使用磁盘量\njournalctl --disk-usage 清理方法可以采用按照日期清理，或者按照允许保留的容量清理，只保存2天的日志，最大500M\njournalctl --vacuum-time=2d journalctl --vacuum-size=500M 要启用日志限制持久化配置，可以修改\nvim /etc/systemd/journald.conf SystemMaxUse=16M ForwardToSyslog=no 重启\nsystemctl restart systemd-journald.service 检查journal是否运行正常以及日志文件是否完整无损坏\njournalctl --verify 方法二 丢弃日志\n把Storage改为none\nvim /etc/systemd/journald.conf Storage=none 重启生效\nsystemctl restart systemd-journald journald.conf参数详解 [Journal] #日志存储到磁盘 Storage=persistent #压缩日志 Compress=yes #为日志添加序列号 Seal=yes #每个用户分别记录日志 SplitMode=uid #日志同步到磁盘的间隔，高级别的日志，如：CRIT、ALERT、EMERG 三种总是实时同步 SyncIntervalSec=1m #即制日志的最大流量，此处指 30s 内最多记录 100000 条日志，超出的将被丢弃 RateLimitInterval=30s #与 RateLimitInterval 配合使用 RateLimitBurst=100000 #限制全部日志文件加在一起最多可以占用多少空间，默认值是10%空间与4G空间两者中的较小者 SystemMaxUse=64G #默认值是15%空间与4G空间两者中的较大者 SystemKeepFree=1G #单个日志文件的大小限制，超过此限制将触发滚动保存 SystemMaxFileSize=128M #日志滚动的最大时间间隔，若不设置则完全以大小限制为准 MaxFileSec=1day #日志最大保留时间，超过时限的旧日志将被删除 MaxRetentionSec=100year #是否转发符合条件的日志记录到本机的其它日志管理系统，如：rsyslog ForwardToSyslog=yes ForwardToKMsg=no #是否转发符合条件的日志到所有登陆用户的终端 ForwardToWall=yes MaxLevelStore=debug MaxLevelSyslog=err MaxLevelWall=emerg ForwardToConsole=no #TTYPath=/dev/console #MaxLevelConsole=info #MaxLevelKMsg=notice 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210401140104/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"systemd-journald占用资源过多解决方法","id":84,"section":"posts","tags":["journald","故障集"],"title":"Systemd Journald占用资源过多","uri":"https://www.cnsre.cn/posts/210401140104/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210331125739/\n相关话题：https://www.cnsre.cn/tags/故障集/\nAWS创建Redis集群模式遇到的问题 问题描述 前几天在AWS 平台创建了Redis集群模式，但是链接集群的时候发现无法连接，返回信息超时。\n通过参数组创建redis的时候提示报错：Replication group with specified name already exists. (Service: AmazonElastiCache; Status Code: 400; Error Code: ReplicationGroupAlreadyExists; Request ID: XXX)\n原因 AWS 创建redis集群模式的时候需要使用 default.redis5.0.cluster.on 参数组。\n出于后期调配参数的想法，所以我在创建集群之前，先一步创建了集群使用的参数组。问题就在这个参数组上边。创建参数组的时候只有参数组名称以及描述 所以创建参数组 默认的是单机模式的参数组，参数组集群模式是没有开启的，所以使用创建的参数组创建redis集群的时候会报错。\nReplication group with specified name already exists. (Service: AmazonElastiCache; Status Code: 400; Error Code: ReplicationGroupAlreadyExists; Request ID: XXX)\n解决方法 创建集群单独使用的参数组，点击参数组名称\u0026ndash;修改参数\u0026ndash;找到 cluster-enabled 修改为yes。参数默认为no\n接下来创建redis集群即可。\n勾选集群模式\nredis设置\n选择安全组\n备份，维护窗口什么的自己随意选择。\n问题总结 在创建的时候不够细心，没仔细想其中的细节，默认创建的参数组是以默认的单机模式创建的，因为并不是所有人都需要集群模式。但是官网又没仔细的介绍说集群模式需要修改，因为官网的集群模式推荐的是使用 default 参数。\n作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210331125739/\n相关话题：https://www.cnsre.cn/tags/故障集/\n","description":"在aws 平台创建了Redis 集群模式，但是链接集群的时候发现无法连接，返回信息超时。","id":85,"section":"posts","tags":["aws","redis","故障集"],"title":"AWS创建Redis集群模式遇到的问题","uri":"https://www.cnsre.cn/posts/210331125739/"},{"content":" 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210330135719/\n相关话题：https://www.cnsre.cn/tags/kafka/\nKafka内外网访问 本文介绍了Kafka内外网访问的设置。\nkafka的两个配置listeners和advertised.listeners listeners kafka监听的网卡的ip，假设你机器上有两张网卡，内网192.168.0.213和外网101.89.163.1 如下配置\nlisteners=PLAINTEXT://192.168.0.213:9092 那么kafka只监听内网网卡，即只接收内网网卡的数据，如果你不能把外网网卡流量转发到内网网卡（为什么要强调这一点，下面说），那么kafka就接收不到外网网卡数据。如果配置成外网ip同理。当然你可以配置成0.0.0.0，监听所有网卡。\nadvertised.listeners 我们观察kafka的配置文件server.properties，会发现里面记录了zookeeper集群的各个节点的访问地址，但是并没有记录kafka兄弟节点的地址。kafka节点启动后，会向zookeeper注册自己，同时从zookeeper中获取兄弟节点的地址，以便与兄弟节点通信。\n同样，我们使用客户端连接kafka后，kafka返回给客户端的是集群各节点的访问地址，这个地址也是上面说的从zookeeper中获得的地址。\n这个地址哪里来，就是kafka节点向zookeeper注册时提供的advertised.listeners。如果没有，就会使用listeners。\n三种情景，搭配使用这两个配置 只需要内网访问kafka listeners=PLAINTEXT://192.168.0.213:9092 只需要内网访问kafka 你肯定想到了最简单的一个方法，listeners使用外网ip\nlisteners=PLAINTEXT://101.89.163.1:9092 需要外网访问 如果宿主机有外网网卡，这么配当然没问题。如果没有（ifconfig看不到外网ip的网卡，基本上就不存在这个外网网卡），很可能和我使用的的宿主机一样是通过NAT映射或者啥办法搞出来的外网ip，此时kafka无法监听这个外网ip（因为不存在，启动就会报错）。\n这时候就是advertised.listeners真正发挥作用的时候了。使用如下配置：\nlisteners=PLAINTEXT://192.168.0.213:9092 advertised.listeners=PLAINTEXT://101.89.163.1:9092 此时一个完整的kafka客户端访问服务端的流程：\n客户端访问101.89.163.1:9092，被kafka宿主机所在环境映射到内网192.168.0.213:9092，访问到了kafka节点，请求获得kafka服务端的访问地址 kafka从zookeeper拿到自己和其他兄弟节点通过advertised.listeners注册到zookeeper的101.89.163.1:9092等外网地址，作为kafka的服务端访问地址返回给客户端 客户端拿这些地址访问kafka集群，被kafka宿主机所在环境映射到各kafka节点的内网ip，访问到了kafka服务端\u0026hellip;\u0026hellip;完美循环 你可能会问已经配置了访问地址，为什么还要在第一次访问的时候请求获得kafka的访问地址。因为如果是kafka集群，你可以选择只给客户端配置一个kafka节点的地址（这样是不推荐的），但是客户端必须要访问集群中的每一个节点，所以必须通过这个节点获得集群中每一个节点的访问地址。\n如果不配置advertised.listeners=PLAINTEXT://101.89.163.1:9092，你会发现虽然你给kafka客户端配置的访问地址是101.89.163.1:9092，但是kafka客户端访问时报错，报错原因是Connection to node -1[192.168.0.213:9092] could not be established. Broker may not be available.。这就是因为不配置advertised.listeners则advertised.listeners默认使用listeners配置的地址，客户端拿到的就是listeners配置的内网地址\n内外网分流 上面说的有外网ip的情况，直接配置外网ip有没有问题呢？\n如果既要内网访问，又要外网访问，本来可以走内网的流量都走外网网卡，显然不合适；而且有的环境可能被配置成这些kafka宿主机是没有外网访问权限的，即虽然他可以访问自己的外网ip，但是访问不了兄弟节点的外网ip。这时候就要配置内外网。\n配置1：\n配置1 配置2 listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT listeners=INTERNAL://192.168.0.213:9092,EXTERNAL://192.168.0.213:19092 advertised.listeners=INTERNAL://192.168.0.213:9092,EXTERNAL://101.89.163.9:19092 inter.broker.listener.name=INTERNAL listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT listeners=INTERNAL://192.168.0.213:9092,EXTERNAL://101.89.163.9:19092 advertised.listeners=INTERNAL://192.168.0.213:9092,EXTERNAL://101.89.163.9:19092 inter.broker.listener.name=INTERNAL 注意这两的区别是listeners的EXTERNAL使用的ip不一样，一个使用内网ip，一个使用外网ip。\n如果你的kafka宿主机有外网网卡，只能用外网ip，若使用配置1，kafka通过listeners监听的两个端口都是内网网卡的数据，无法接收到外网网卡数据； 如果你的kafka宿主机外网ip是映射来的，只能使用内网ip，原因也是上面说过的，不存在外网网卡，kafka启动监听就会报错，而使用内网ip有环境配置好的转发，可以接收到外网ip的数据。 作者：SRE运维博客\n博客地址：https://www.cnsre.cn\n文章地址：https://www.cnsre.cn/posts/210330135719/\n相关话题：https://www.cnsre.cn/tags/kafka/\n","description":"kafka的listeners和advertised.listeners，配置内外网分流","id":86,"section":"posts","tags":["kafka"],"title":"Kafka内外网访问","uri":"https://www.cnsre.cn/posts/210330135719/"},{"content":"Jenkins 构建JOB失败 问题描述 同事在使用Jenkins打包项目的时候报错\nerror:index-pack died of signal 15 fatal: index-pack failed 出现这个问题，初步怀疑是拉取代码超时，为了验证这个问题，我分别在本地，jenkins服务器，其他服务器分别用git 拉取代码尝试，发现拉取代码的时候都是非常慢的，这个有可能和我们的代码仓在国外有关系。\n后来在修改拉取代码时间后，构建还是失败😟。\n通过在网上查阅资料，最终确认是git的http.postBuffer配置默认值大小的问题，此配置是用来限制git推送大小的，由于代码里有大文件导致拉取代码时postBuffer溢出，所以需要增大http.postBuffer的值。\n解决方法 修改拉取代码的时间 打开Jenkins控制台，打开构建失败的JOB选择配置\n选择源码管理 - - Additional Behaviours - - 新增 - - 点击最下边的小三角，找到高级的克隆行为\n在克隆和拉取操作的超市时间（分钟）选项中填写设置超时的时间\n点击应用 - - 保存 修改Git postBuffer 在服务器上使用命令修改，执行命令时使用Jenkins用户执行\nsu - jenkins -c \u0026ldquo;git config \u0026ndash;global http.postBuffer 524288000\u0026rdquo; 验证是否生效\n[root@jenkins jenkins]# cat .gitconfig [filesystem \u0026#34;Oracle Corporation|1.8.0_222|/dev/nvme0n1p1\u0026#34;] minRacyThreshold = 4837 microseconds 构建JOB验证 最后从新构建项目，最燃侯建的时间长了一些 但是好在构建成功了。\n文章链接\nhttps://www.cnsre.cn/posts/210329141238/\n","description":"Jenkins构建JOB拉去代码失败 error:index-pack died of signal 15","id":87,"section":"posts","tags":["jenkins","故障集"],"title":"Jenkins 构建JOB失败","uri":"https://www.cnsre.cn/posts/210329141238/"},{"content":"Prometheus介绍 Prometheus 简介 Prometheus 起初是 SoundCloud 创建的一个开源系统监控报警工具。自其 2012 年开创以来，众多公司、组织都采用了 Prometheus，该项目也有一个非常活跃的开发者和用户社区。Prometheus 于 2016 年加入了 CNCF，成为了继 Kubernetes 之后的第二个托管项目。\nPrometheus 原理介绍 目前Prometheus支持OpenTsdb、InfluxDB、Elasticsearch等后端存储，通过适配器实现Prometheus存储的remote write和remote read接口，便可以接入Prometheus作为远程存储使用\nPrometheu由Go语言编写而成，采用Pull方式获取监控信息，并提供了多维度的数据模型和灵活的查询接口。Prometheus不仅可以通过静态文件配置监控对象，还支持自动发现机制，能通过Kubernetes、onsl、DNS等多种方式动态获取监控对象。在数据采集方面，借助Go语音的高并发特性，单机Prometheus可以采取数百个节点的监控数据；在数据存储方面，随着本地时序数据库的不断优化，单机Prometheus每秒可以采集一千万个指标，如果需要存储大量的历史监控数据，则还支持远程存储。\nPrometheus 特性（Features） Prometheus 的主要特性\n通过指标名（metric name）和 KV 结构，使用时序数据（time series data）表达的多维度数据模型 PromQL，一种灵活的查询语言，能够更好的利用维度 对分布式存储没有依赖；单服务器节点即可自治 通过使用基于 HTTP 的拉模式 (pull model) 进行时序数据采集 通过中间网关 ( gateway) 以支持推送时序数据 通过服务发现或静态配置，发现监控目标（targets） 支持多图和仪表盘模式 Prometheus 组件（Components） Prometheus 生态由多组件构成，其中大部分都是可选配置： 主要的 Promethues server：抓取和存储时序数据 客户端 libraries：装置在应用端以采样(instrumenting)应用程序代码 push gateway：能够支持短时任务 特别的 exporters 给各类服务，比如 HAProxy、StatsD、Graphite 等。 alertmanager：用来处理报警 各式支持工具 Prometheus 架构（Architecture） Prometheus 采样数据有两种方式，一种是直接采样仪表化任务（instrumenting job），另一种通过中介 push gateway 来采样短时任务。Prometheus 在本地存储所有采样的样本，同时基于这些数据，执行规则（rules）用以从已存数据上聚合、记录新的时序数据，或用以发送报警。Grafana 或其他的 API consumers 可以可视化这些收集的数据。\nPrometheus的基本原理是通过HTTP周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口并符合Prometheus定义的数据格式，就可以介入Prometheus监控\n针对 Prometheus、Zabbix监控系统的对比 Zabbix Prometheus 优点 1、支持多平台的企业级分布式开源监控软件；\n2、安装部署简单，多种数据采集插件灵活集成\n3、功能强大，可实现复杂多条件告警\n4、自带画图功能，得到的数据可以绘成图形\n5、提供多种API接口，支持调用脚本\n6、出现问题时可自动远程执行命令（需对agent设置执行权限） 1、后端用golang开发，前端是Grafana,JSON编辑即可解决。定制化难度较低\n2、监控数据存储在基于时间序列的数据库内，便于对已有数据进行新的聚合。\n3、各个组件都有较成熟高可用方案，没有单点故障\n4、适合对云环境进行监控，对OpenStack,Kubernetes 有很好的集成。\n5、支持报警的收敛 缺点 1、项目批量修改不方便\n2、但是深层次需求需要非常熟悉Zabbix并进行大量的二次定制开发，难度较大；\n3、系统级别报警设置相对比较多，如果不筛选的话报警邮件会很多；并且自定义的项目报警需要自己设置，过程比较繁\n4、数据报表需要特殊二次开发定义； 1、安装相对复杂，监控、告警和界面都分属于不同的组件。\n2、界面相对较弱，很多配置需要修改配置文件。 对比项 Prometheus zabbix Prometheus优势 zabbix优势 管理 二进制文件启动 LNMP+编译 轻量级server，便于迁移和维护 – 配置 配置文件 图形化 更好的支持自动化配置 学习成本低 - client 丰富的client库 zabbix_agent自定义脚本 为各种中间件、应用提供专业的exporter，监控项更全面 支持自定义监控项，对监控设计者的格局要求较高 数据存储方式 Prometheus TSDB MySQL 监控数据以时间为维度统计情况较多，时序数据库更适用于监控数据的存储，按时间索引性能更高 MySQL较常用，学习成本低 数据处理 PromQL MySQL PromQL计算函数丰富，统计维度广 同上 二次开发 丰富的sdk api 提供了Go、Java/Scala、Python、Ruby等sdk，二次开发更便捷 api适配较为常用，学习成本低 对云环境的支持 原生支持容器监控 更适合物理机监控 自动发现容器，更好的适配k8s – 告警方式 可按照标签分组，收敛 在次数上收敛 告警收敛方式更多样化 – 监控项值 支持数字 支持数字字符串 – 可做日志监控 文章链接\nhttps://www.cnsre.cn/posts/210326133909/\n","description":"Prometheus介绍","id":88,"section":"posts","tags":["prometheus"],"title":"Prometheus介绍","uri":"https://www.cnsre.cn/posts/210326133909/"},{"content":"ELK+kafka+filebeat搭建生产ELFK集群 ELK 架构介绍 集群服务版本 服务 版本 java 1.8.0_221 elasticsearch 7.10.1 filebeat 7.10.1 kibana 7.10.1 logstash 7.10.1 cerebro 0.9.2-1 kafka 2.12-2.3.0 zookeeper 3.5.6 服务器环境说明 IP地址 主机名 配置 角色 10.0.11.172 elk-master 4C16G es-master、kafka+zookeeper1 10.0.21.117 elk-node1 4C16G es-node1、kafka+zookeeper2 10.0.11.208 elk-node2 4C16G es-node2、kafka+zookeeper3 10.0.10.242 elk-kibana 4C16G logstash、kibana、cerebro 系统参数优化 三个节点都需要执行 修改主机名 1 2 3 hostnamectl set-hostname elk-master hostnamectl set-hostname elk-node1 hostnamectl set-hostname elk-node2 增加文件描述符 1 2 3 4 5 6 7 8 cat \u0026gt;\u0026gt;/etc/security/limits.conf\u0026lt;\u0026lt; EOF * soft nofile 65536 * hard nofile 65536 * soft nproc 65536 * hard nproc 65536 * hard memlock unlimited * soft memlock unlimited EOF 修改默认限制内存 1 2 3 4 5 cat \u0026gt;\u0026gt;/etc/systemd/system.conf\u0026lt;\u0026lt; EOF DefaultLimitNOFILE=65536 DefaultLimitNPROC=32000 DefaultLimitMEMLOCK=infinity EOF 优化内核，对es支持 1 2 3 4 5 6 7 8 9 10 11 12 cat \u0026gt;\u0026gt;/etc/sysctl.conf\u0026lt;\u0026lt; EOF # 关闭交换内存 vm.swappiness =0 # 影响java线程数量，建议修改为262144或者更高 vm.max_map_count= 262144 # 优化内核listen连接 net.core.somaxconn=65535 # 最大打开文件描述符数，建议修改为655360或者更高 fs.file-max=655360 # 开启ipv4转发 net.ipv4.ip_forward= 1 EOF 修改Hostname配置文件 1 2 3 4 5 cat \u0026gt;\u0026gt;/etc/hosts\u0026lt;\u0026lt; EOF elk-master 10.0.11.172 elk-node1 10.0.21.117 elk-node2 10.0.11.208 EOF 重启使配置生效 1 reboot 部署Zookeeper 三个节点都需要执行 创建Zookeeper项目目录 1 2 3 4 #存放快照日志 mkdir zkdata #存放事物日志 mkdir zklogs 下载解压zookeeper 1 2 3 wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.5.6/apache-zookeeper-3.5.6-bin.tar.gz tar -zxvf apache-zookeeper-3.5.6-bin.tar.gz mv apache-zookeeper-3.5.6-bin zookeeper 修改配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@elk-master zookeeper]# cat conf/zoo.cfg |grep -v ^# # 服务器之间或客户端与服务器之间维持心跳的时间间隔 # tickTime以毫秒为单位。 tickTime=2000 # 集群中的follower服务器(F)与leader服务器(L)之间的初始连接心跳数 initLimit=10 # 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数 syncLimit=5 # 数据保存目录 dataDir=../zkdata # 日志保存目录 dataLogDir=../zklogs # 客户端连接端口 clientPort=2181 # 客户端最大连接数。# 根据自己实际情况设置，默认为60个 maxClientCnxns=60 # 三个接点配置，格式为： server.服务编号=服务地址、LF通信端口、选举端口 server.1=10.0.11.172:2888:3888 server.2=10.0.21.117:2888:3888 server.3=10.0.11.208:2888:3888 写入节点标记 分别在三个节点/home/tools/zookeeper/zkdata/myid写入节点标记 master节点 node1节点 node2节点 master的操作 1 echo \u0026#34;1\u0026#34; \u0026gt; /home/tools/zookeeper/zkdata/myid node1的操作 1 echo \u0026#34;2\u0026#34; \u0026gt; /home/tools/zookeeper/zkdata/myid node2的操作 1 echo \u0026#34;3\u0026#34; \u0026gt; /home/tools/zookeeper/zkdata/myid 启动zookeeper集群 1 2 3 4 5 [root@elk-master zookeeper]# cd /home/tools/zookeeper/bin/ [root@elk-master bin]# ./zkServer.sh start ZooKeeper JMX enabled by default Using config: /home/tools/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED 检查集群状态 1 2 3 4 5 [root@elk-master bin]# sh /home/tools/zookeeper/bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /home/tools/zookeeper/bin/../conf/zoo.cfg Client port found: 2181. Client address: localhost. Mode: leader 设置全局变量 1 2 3 4 5 cat \u0026gt;\u0026gt;/etc/profile\u0026lt;\u0026lt; EOF export ZOOKEEPER_INSTALL=/home/tools/zookeeper/ export PATH=$PATH:$ZOOKEEPER_INSTALL/bin export PATH EOF 使配置生效 1 source /etc/profile 这样就可以全局使用zkServer.sh命令了\n部署 Kafka 三个节点都需要执行 下载解压kafka压缩包 1 2 3 4 5 6 [root@elk-master tools]# mkdir kafka [root@elk-master tools]# cd kafka/ [root@elk-master kafka]# wget https://www-eu.apache.org/dist/kafka/2.3.0/kafka_2.12-2.3.0.tgz [root@elk-master kafka]# tar xf kafka_2.12-2.3.0.tgz [root@elk-master kafka]# mv kafka_2.12-2.3.0 kafka [root@elk-master kafka]# cd kafka/config/ 配置kafka 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 [root@elk-master config]# cat /home/tools/kafka/kafka/config/server.properties ############################# Server Basics ############################# # broker的id，值为整数，且必须唯一，在一个集群中不能重复 broker.id=1 ############################# Socket Server Se：ttings ############################# # kafka默认监听的端口为9092 (默认与主机名进行连接) listeners=PLAINTEXT://:9092 # 处理网络请求的线程数量，默认为3个 num.network.threads=3 # 执行磁盘IO操作的线程数量，默认为8个 num.io.threads=8 # socket服务发送数据的缓冲区大小，默认100KB socket.send.buffer.bytes=102400 # socket服务接受数据的缓冲区大小，默认100KB socket.receive.buffer.bytes=102400 # socket服务所能接受的一个请求的最大大小，默认为100M socket.request.max.bytes=104857600 ############################# Log Basics ############################# # kafka存储消息数据的目录 log.dirs=../kfkdata # 每个topic默认的partition数量 num.partitions=3 # 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量 num.recovery.threads.per.data.dir=1 ############################# Log Flush Policy ############################# # 消息刷新到磁盘中的消息条数阈值 #log.flush.interval.messages=10000 # 消息刷新到磁盘中的最大时间间隔,1s #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # 日志保留小时数，超时会自动删除，默认为7天 log.retention.hours=168 # 日志保留大小，超出大小会自动删除，默认为1G #log.retention.bytes=1073741824 # 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件 log.segment.bytes=1073741824 # 每隔多长时间检测数据是否达到删除条件,300s log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开 zookeeper.connect=10.0.11.172,10.0.21.117,10.0.11.208 # 连接zookeeper的超时时间,6s zookeeper.connection.timeout.ms=6000 创建数据存储的目录 1 [root@elk-master config]# mkdir ../kfkdata 修改broker.id 分别在三个节点依次修改/home/tools/kafka/kafka/config/server.properties配置文件 master节点 node1节点 node2节点 master的配置 1 broker.id=1 node1的配置 1 broker.id=2 node2的配置 1 broker.id=3 启动kafka集群 1 2 3 4 5 cd /home/tools/kafka/kafka/bin/ #启动测试 ./kafka-server-start.sh ../config/server.properties #放入后台 ./kafka-server-start.sh -daemon ../config/server.properties 测试 任意节点均可执行 在创建topic在集群中的任意节点 发布消息订阅消息验证结果\n创建topic 消息发布 topic消息订阅 1 2 3 4 5 6 [root@elk-master bin]# ./kafka-topics.sh \\ --create \\ --zookeeper 10.0.11.172:2181,10.0.21.117:2181,10.0.11.208:2181 \\ --partitions 3 \\ --replication-factor 1 \\ --topic logs 1 2 3 [root@elk-master bin]# ./kafka-console-producer.sh \\ --broker-list 10.0.11.172:9092,10.0.21.117:9092,10.0.11.208:9092 \\ --topic logs 1 2 3 4 [root@elk-master bin]# ./kafka-console-consumer.sh \\ --bootstrap-server 10.0.11.172:9092,10.0.21.117:9092,10.0.11.208:9092 \\ --topic logs \\ --from-beginning 部署elasticsearch 三个节点都需要执行 下载安装elasticsearch 1 2 wget https://pan.cnsre.cn/d/Package/Linux/ELK/elasticsearch-7.10.1-x86_64.rpm [root@elk-master package]# rpm -ivh elasticsearch-7.10.1-x86_64.rpm 备份配置文件 1 2 cd /etc/elasticsearch cp elasticsearch.yml elasticsearch.yml.bak 修改配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 cat \u0026gt;/etc/elasticsearch/elasticsearch.yml \u0026lt;\u0026lt; EOF #集群名 cluster.name: elk-cluster #node名 node.name: elk-1 #数据存储路径 path.data: /home/elasticsearch/esdata #数据快照路径 path.repo: /home/backup/essnapshot #日志存储路径 path.logs: /home/elasticsearch/eslogs #es绑定的ip地址，根据自己机器ip进行修改 network.host: 0.0.0.0 #服务端口 http.port: 9200 #集群master需要和node名设置一致 discovery.seed_hosts: [\u0026#34;10.0.11.172\u0026#34;, \u0026#34;10.0.21.117\u0026#34;, \u0026#34;10.0.11.208\u0026#34;] cluster.initial_master_nodes: [\u0026#34;10.0.11.172\u0026#34;,\u0026#34;10.0.21.117\u0026#34;,\u0026#34;10.0.11.208\u0026#34;] #允许跨域请求 http.cors.enabled: true #* 表示支持所有域名 http.cors.allow-origin: \u0026#34;*\u0026#34; #添加请求header http.cors.allow-headers: Authorization,X-Requested-With,Content-Length,Content-Type #生产必须为true，内存锁定检查，目的是内存地址直接映射，减少一次copy时间 bootstrap.memory_lock: true #系统过滤检查，防止数据损坏，考虑集群安全，生产设置成false bootstrap.system_call_filter: false #xpack配置 xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/elastic-certificates.p12 EOF 修改JVM 将jvm.options文件中22-23行的8g设置为你的服务内存的一半 1 2 3 [root@elk-node1 elasticsearch]# cat -n jvm.options |grep 8g 22 -Xms8g 23 -Xmx8g 修改其他节点配置 分别在三个节点修改/etc/elasticsearch/elasticsearch.yml配置文件 master节点 node1节点 node2节点 master的配置 1 node.name: \u0026#34;es-master\u0026#34; node1的配置 1 node.name: \u0026#34;es-node1\u0026#34; node2的配置 1 node.name: \u0026#34;es-node2\u0026#34; 最终展示\n分配权限 因为自定义数据、日志存储目录，所以要把权限给到目录\n1 2 3 4 mkdir -p /home/elasticsearch/{esdata,eslogs} chown elasticsearch:elasticsearch /home/elasticsearch/* mkdir -p /home/backup/essnapshot chown elasticsearch:elasticsearch /home/backup/essnapshot 启动服务 三个节点全部启动并加入开机启动\n1 2 systemctl start elasticsearch systemctl enable elasticsearch 使用xpack进行安全认证 xpack的安全功能 TLS 功能。 可对通信进行加密 文件和原生 Realm。 可用于创建和管理用户 基于角色的访问控制。 可用于控制用户对集群 API 和索引的访问权限 通过针对 Kibana Spaces 的安全功能，还可允许在Kibana 中实现多租户。\n在我配置过程中，发现集群认证需要首先配置秘钥才行，否则在给内置用户创建秘钥的时候将会报错。\nCause: Cluster state has not been recovered yet, cannot write to the [null] index 1 2 3 4 5 6 7 8 9 10 Unexpected response code [503] from calling PUT http://10.0.11.172:9200/_security/user/apm_system/_password?pretty Cause: Cluster state has not been recovered yet, cannot write to the [null] index Possible next steps: * Try running this tool again. * Try running with the --verbose parameter for additional messages. * Check the elasticsearch logs for additional error details. * Use the change password API manually. ERROR: Failed to set password for user [apm_system]. 申请证书 下面的操作，在其中一个节点操作即可 1 2 /usr/share/elasticsearch/bin/elasticsearch-certutil ca /usr/share/elasticsearch/bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 两条命令均一路回车即可，不需要给秘钥再添加密码\n证书创建完成之后，默认在es的数据目录。\n将证书拷贝到etc下，并给上权限。\n[root@elk-master ~]# ls /usr/share/elasticsearch/elastic-* /usr/share/elasticsearch/elastic-certificates.p12 /usr/share/elasticsearch/elastic-stack-ca.p12 cp /usr/share/elasticsearch/elastic-* /etc/elasticsearch/ chown elasticsearch.elasticsearch /etc/elasticsearch/elastic* 做完之后，将证书拷贝到其他节点\n为内置账号添加密码 ES中内置了几个管理其他集成组件的账号apm_system, beats_system, elastic, kibana, logstash_system, remote_monitoring_user使用之前，首先需要设置下密码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user. You will be prompted to enter passwords as the process progresses. Please confirm that you would like to continue [y/N]y Enter password for [elastic]: Reenter password for [elastic]: Enter password for [apm_system]: Reenter password for [apm_system]: Enter password for [kibana]: Reenter password for [kibana]: Enter password for [logstash_system]: Reenter password for [logstash_system]: Enter password for [beats_system]: Reenter password for [beats_system]: Enter password for [remote_monitoring_user]: Reenter password for [remote_monitoring_user]: Changed password for user [apm_system] Changed password for user [kibana] Changed password for user [logstash_system] Changed password for user [beats_system] Changed password for user [remote_monitoring_user] Changed password for user [elastic] 部署 Cerebro 下载安装 1 2 wget https://pan.cnsre.cn/d/Package/Linux/ELK/cerebro-0.9.2-1.noarch.rpm rpm -ivh cerebro-0.9.2-1.noarch.rpm 修改配置文件 修改/etc/cerebro/application.conf配置文件\n找到对应配置修改为以下内容\n修改内容一 修改内容二 1 2 data.path: \u0026#34;/var/lib/cerebro/cerebro.db\u0026#34; #data.path = \u0026#34;./cerebro.db\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 hosts = [ #{ # host = \u0026#34;http://localhost:9200\u0026#34; # name = \u0026#34;Localhost cluster\u0026#34; # headers-whitelist = [ \u0026#34;x-proxy-user\u0026#34;, \u0026#34;x-proxy-roles\u0026#34;, \u0026#34;X-Forwarded-For\u0026#34; ] #} # Example of host with authentication { host = \u0026#34;http://10.0.11.172:9200\u0026#34; name = \u0026#34;elk-cluster\u0026#34; auth = { username = \u0026#34;elastic\u0026#34; password = \u0026#34;123\u0026#34; } } ] 报错 cerebro[8073]: No java installations was detected. 启动服务后报错No java，但是我的环境是有JAVA的。也做了全局变量\n感觉很奇怪\u0026hellip;\n解决方法 在启动服务文件中加入JAVA_HOME 找到服务启动文件/usr/share/cerebro/bin/cerebro 修改/usr/share/cerebro/bin/cerebro中的JAVA_HOME\n具体如下，根据自己的JAVA_HOME填写路径\n修改前 修改后 1 2 3 4 5 6 7 if [[ -n \u0026#34;$bundled_jvm\u0026#34; ]]; then echo \u0026#34;$bundled_jvm/bin/java\u0026#34; elif [[ -n \u0026#34;$JAVA_HOME\u0026#34; ]] \u0026amp;\u0026amp; [[ -x \u0026#34;$JAVA_HOME/bin/java\u0026#34; ]]; then echo \u0026#34;$JAVA_HOME/bin/java\u0026#34; else echo \u0026#34;java\u0026#34; fi 1 2 3 4 5 6 7 if [[ -n \u0026#34;$bundled_jvm\u0026#34; ]]; then echo \u0026#34;$bundled_jvm/bin/java\u0026#34; elif [[ -n \u0026#34;/home/tools/jdk1.8.0_221\u0026#34; ]] \u0026amp;\u0026amp; [[ -x \u0026#34;/home/tools/jdk1.8.0_221/bin/java\u0026#34; ]]; then echo \u0026#34;/home/tools/jdk1.8.0_221/bin/java\u0026#34; else echo \u0026#34;java\u0026#34; fi 启动服务 1 2 3 systemctl start cerebro.service systemctl enable cerebro.service systemctl status cerebro.service 可以看到监听的是9000端口\n访问试下\n部署Kibana 下载安装 1 2 https://artifacts.elastic.co/downloads/kibana/kibana-7.10.1-x86_64.rpm rpm -ivh kibana-7.10.1-x86_64.rpm 修改备份配置文件 备份配置文件 1 2 cd /etc/kibana/ mv kibana.yml kibana.yml.bak 修改配置文件 1 2 3 4 5 6 7 vim kibana.yml server.port: 5601 server.host: 0.0.0.0 elasticsearch.hosts: [\u0026#34;http://10.0.11.172:9200/\u0026#34;,\u0026#34;http://10.0.21.117:9200/\u0026#34;,\u0026#34;http://10.0.11.208:9200/\u0026#34;] elasticsearch.username: \u0026#34;elastic\u0026#34; elasticsearch.password: \u0026#34;123\u0026#34; i18n.locale: \u0026#34;zh-CN\u0026#34; 启动服务器 1 2 3 systemctl start kibana.service systemctl enable kibana.service systemctl status kibana.service 访问WEB 访问http://IP:5601\n部署filebeat 1 2 3 4 wget https://pan.cnsre.cn/d/Package/Linux/ELK/filebeat-7.10.1-x86_64.rpm rpm -ivh filebeat-7.10.1-x86_64.rpm cd /etc/filebeat/ cp filebeat.yml filebeat.yml.bak 修改配置文件 修改filebeat配置文件,把日志推送到kafka\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #=========================== Filebeat inputs ============================= max_procs: 1 #限制filebeat的进程数量,其实就是内核数，避免过多抢占业务资源 queue.mem.events: 256 # 存储于内存队列的事件数，排队发送 (默认4096) queue.mem.flush.min_events: 128 # 小于 queue.mem.events ,增加此值可提高吞吐量 (默认值2048) filebeat.inputs: # inputs为复数，表名type可以有多个 - type: log # 输入类型 enable: true # 启用这个type配置 paths: - /home/homeconnect/logs/AspectLog/aspect.log # 监控tomcat 的业务日志 json.keys_under_root: true #默认Flase,还会将json解析的日志存储至messages字段 json.overwrite_keys: true #覆盖默认的key,使用自定义json格式的key max_bytes: 20480 # 单条日志的大小限制,建议限制(默认为10M,queue.mem.events * max_bytes 将是占有内存的一部) fields: # 额外的字段 source: test-prod-tomcat-aspect-a # 自定义source字段，用于es建议索引（字段名小写，我记得大写好像不行） - type: log # 输入类型 enable: true # 启用这个type配置 paths: - /home/tools/apache-tomcat-8.5.23/logs/localhost_access_log.*.log # 监控tomcat access日志 json.keys_under_root: true #默认Flase,还会将json解析的日志存储至messages字段 json.overwrite_keys: true #覆盖默认的key,使用自定义json格式的key max_bytes: 20480 # 单条日志的大小限制,建议限制(默认为10M,queue.mem.events * max_bytes 将是占有内存的一部分) fields: # 额外的字段 source: test-prod-tomcat-access-a # 自定义source字段，用于es建议索引 # 自定义es的索引需要把ilm设置为false setup.ilm.enabled: false #=============================== output =============================== output.kafka: # 输出到kafka enabled: true # 该output配置是否启用 hosts: [\u0026#34;10.0.11.172:9092\u0026#34;,\u0026#34;10.0.21.117:9092\u0026#34;,\u0026#34;10.0.11.208:9092\u0026#34;] # kafka节点列表 topic: \u0026#39;logstash-%{[fields.source]}\u0026#39; # kafka会创建该topic，然后logstash(可以过滤修改)会传给es作为索引名称 partition.hash: reachable_only: true # 是否只发往可达分区 compression: gzip # 压缩 max_message_bytes: 1000000 # Event最大字节数。默认1000000。应小于等于kafka broker message.max.bytes值 required_acks: 1 # kafka ack等级 worker: 1 # kafka output的最大并发数 bulk_max_size: 2048 # 单次发往kafka的最大事件数 logging.to_files: true # 输出所有日志到file，默认true， 达到日志文件大小限制时，日志文件会自动限制替换 #=============================== other =============================== close_older: 30m # 如果文件在某个时间段内没有发生过更新，则关闭监控的文件handle。默认1h force_close_files: false # 这个选项关闭一个文件,当文件名称的变化。只在window建议为true # 没有新日志采集后多长时间关闭文件句柄，默认5分钟，设置成1分钟，加快文件句柄关闭 close_inactive: 1m # 传输了3h后荏没有传输完成的话就强行关闭文件句柄，这个配置项是解决以上案例问题的key point close_timeout: 3h # 这个配置项也应该配置上，默认值是0表示不清理，不清理的意思是采集过的文件描述在registry文件里永不清理，在运行一段时间后，registry会变大，可能会带来问题 clean_inactive: 72h # 设置了clean_inactive后就需要设置ignore_older，且要保证ignore_older \u0026lt; clean_inactive ignore_older: 70h 启动服务 1 2 3 systemctl start filebeat.service systemctl enable filebeat.service systemctl status filebeat.service 部署logstash 下载安装 1 2 3 wget https://pan.cnsre.cn/d/Package/Linux/ELK/logstash-7.10.1-x86_64.rpm rpm -ivh logstash-7.10.1-x86_64.rpm mv logstash.yml logstash.yml.bak 修改配置文件 修改logstash.yml\n1 2 3 4 5 6 vim logstash.yml http.host: \u0026#34;0.0.0.0\u0026#34; # 指发送到Elasticsearch的批量请求的大小，值越大，处理则通常更高效，但增加了内存开销 pipeline.batch.size: 3000 # 指调整Logstash管道的延迟，过了该时间则logstash开始执行过滤器和输出 pipeline.batch.delay: 200 修改配置文件,从kafka获取日志\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 [root@elk-kibana conf.d]# cat /etc/logstash/conf.d/get-kafka-logs.conf input { # 输入组件 kafka { # 从kafka消费数据 bootstrap_servers =\u0026gt; [\u0026#34;10.0.11.172:9092,10.0.21.117:9092,10.0.11.208:9092\u0026#34;] codec =\u0026gt; \u0026#34;json\u0026#34; # 数据格式 #topics =\u0026gt; [\u0026#34;3in1-topi\u0026#34;] # 使用kafka传过来的topic topics_pattern =\u0026gt; \u0026#34;logstash-.*\u0026#34; # 使用正则匹配topic consumer_threads =\u0026gt; 3 # 消费线程数量 decorate_events =\u0026gt; true # 可向事件添加Kafka元数据，比如主题、消息大小的选项，这将向logstash事件中添加一个名为kafka的字段 auto_offset_reset =\u0026gt; \u0026#34;latest\u0026#34; # 自动重置偏移量到最新的偏移量 #group_id =\u0026gt; \u0026#34;logstash-node\u0026#34; # 消费组ID，多个有相同group_id的logstash实例为一个消费组 #client_id =\u0026gt; \u0026#34;logstash1\u0026#34; # 客户端ID fetch_max_wait_ms =\u0026gt; \u0026#34;1000\u0026#34; # 指当没有足够的数据立即满足fetch_min_bytes时，服务器在回答fetch请求之前将阻塞的最长时间 } } filter{ # 当非业务字段时，无traceId则移除 #if ([message] =~ \u0026#34;traceId=null\u0026#34;) { # 过滤组件，这里只是展示，无实际意义，根据自己的业务需求进行过滤 # drop {} #} mutate { convert =\u0026gt; [\u0026#34;Request time\u0026#34;, \u0026#34;float\u0026#34;] } if [ip] != \u0026#34;-\u0026#34; { geoip { source =\u0026gt; \u0026#34;ip\u0026#34; target =\u0026gt; \u0026#34;geoip\u0026#34; # database =\u0026gt; \u0026#34;/usr/share/GeoIP/GeoIPCity.dat\u0026#34; add_field =\u0026gt; [ \u0026#34;[geoip][coordinates]\u0026#34;, \u0026#34;%{[geoip][longitude]}\u0026#34; ] add_field =\u0026gt; [ \u0026#34;[geoip][coordinates]\u0026#34;, \u0026#34;%{[geoip][latitude]}\u0026#34; ] } mutate { convert =\u0026gt; [ \u0026#34;[geoip][coordinates]\u0026#34;, \u0026#34;float\u0026#34;] } } } output { # 输出组件 elasticsearch { # Logstash输出到es hosts =\u0026gt; [\u0026#34;10.0.11.172:9200\u0026#34;,\u0026#34;10.0.21.117:9200\u0026#34;,\u0026#34;10.0.11.208:9200\u0026#34;] index =\u0026gt; \u0026#34;logstash-%{[fields][source]}-%{+YYYY-MM-dd}\u0026#34; # 直接在日志中匹配 #index =\u0026gt; \u0026#34;%{[@metadata][topic]}-%{+YYYY-MM-dd}\u0026#34; # 以日期建索引 user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123\u0026#34; } #stdout { # codec =\u0026gt; rubydebug #} } 测试接收日志 测试是否能接收到数据\n1 /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/get-kafka-logs.conf 下边把logstash设置为使用systemd启动\n修改/etc/systemd/system/logstash.service文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [Unit] Description=root [Service] Type=simple User=root Group=root # Load env vars from /etc/default/ and /etc/sysconfig/ if they exist. # Prefixing the path with \u0026#39;-\u0026#39; makes it try to load, but if the file doesn\u0026#39;t # exist, it continues onward. EnvironmentFile=-/etc/default/logstash EnvironmentFile=-/etc/sysconfig/logstash ExecStart=/usr/share/logstash/bin/logstash \u0026#34;--path.settings\u0026#34; \u0026#34;/etc/logstash\u0026#34; Restart=alway WorkingDirectort=/ Nice=19 LimitNOFILE=16384 [Install] WantedBy=multi-user.target 在启动程序/usr/share/logstash/bin/logstash.lib.sh中加入JAVA_HOME\n在文件86行左右的if [ -z \u0026quot;$JAVACMD\u0026quot; ]; then代码上方插入一行JAVACMD=\u0026quot;/home/tools/jdk1.8.0_221/bin/java\u0026quot; 具体的路径需要你根据自己的JAVA来修改。\n1 2 3 4 [root@elk-kibana ~]# cat -n /usr/share/logstash/bin/logstash.lib.sh |grep JAVACMD 85 # set the path to java into JAVACMD which will be picked up by JRuby to launch itself 86 JAVACMD=\u0026#34;/home/tools/jdk1.8.0_221/bin/java\u0026#34; 87 if [ -z \u0026#34;$JAVACMD\u0026#34; ]; then 启动服务 1 2 3 systemctl reload logstash.service systemctl restart logstash.service systemctl enable logstash.service 最后检查 登录kibana创建索引 选择管理\u0026ndash;索引模式\u0026ndash;创建索引模式\n输入索引名称\u0026ndash;下一步\n选择@timestamp\u0026ndash;创建索引模式\n然后就可以看到日志了\n文章链接\nhttps://www.cnsre.cn/posts/210325135520/\n","description":"使用ELK+kafka+filebeat搭建海量日志分析系统集群","id":89,"section":"posts","tags":["elk","kafka","cerebro","zookeeper"],"title":"生产ELFK 7.X集群搭建","uri":"https://www.cnsre.cn/posts/210325135520/"},{"content":"数据库备份脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 #!/bin/bash #以下配置信息请自己修改 mysql_user=\u0026#34;backuser\u0026#34; #MySQL备份用户 mysql_password=\u0026#34;\u0026#34; #MySQL备份用户的密码 mysql_host=\u0026#34;\u0026#34; mysql_port=\u0026#34;3306\u0026#34; mysql_charset=\u0026#34;utf8\u0026#34; #MySQL编码 backup_db_arr=(\u0026#34;da1\u0026#34;) #要备份的数据库名称，多个用空格分开隔开 如(\u0026#34;db1\u0026#34; \u0026#34;db2\u0026#34; \u0026#34;db3\u0026#34;) backup_location=/home/backup/sqlback/recipe-prod #备份数据存放位置，末尾请不要带\u0026#34;/\u0026#34;,此项可以保持默认，程序会自动创建文件夹 expire_backup_delete=\u0026#34;ON\u0026#34; #是否开启过期备份删除 ON为开启 OFF为关闭 expire_days=3 #过期时间天数 默认为三天，此项只有在expire_backup_delete开启时有效 # 本行开始以下不需要修改 backup_time=`date +%Y%m%d%H%M` #定义备份详细时间 backup_Ymd=`date +%Y-%m-%d` #定义备份目录中的年月日时间 backup_3ago=`date -d \u0026#39;3 days ago\u0026#39; +%Y-%m-%d` #3天之前的日期 backup_dir=$backup_location/$backup_Ymd #备份文件夹全路径 welcome_msg=\u0026#34;欢迎使用数据库备份工具!\u0026#34; #欢迎语 echo -e \u0026#34;\\033[1;32m$welcome_msg \\033[0m\u0026#34; # 连接到mysql数据库，无法连接则备份退出 mysql -h$mysql_host -P$mysql_port -u$mysql_user -p$mysql_password \u0026lt;\u0026lt;end use mysql; select host,user from user where user=\u0026#39;root\u0026#39; and host=\u0026#39;localhost\u0026#39;; exit end flag=`echo $?` if [ $flag != \u0026#34;0\u0026#34; ]; then echo -e \u0026#34;\\033[1;31m错误：无法连接mysql服务器！停止备份！\\033[0m\u0026#34; exit else echo -e \u0026#34;\\033[1;32mMySQL连接成功！请稍候...... \\033[0m\u0026#34; # 判断有没有定义备份的数据库，如果定义则开始备份，否则退出备份 if [ \u0026#34;$backup_db_arr\u0026#34; != \u0026#34;\u0026#34; ];then for dbname in ${backup_db_arr[@]} do echo -e \u0026#34;\\033[1;32m数据库 $dbname 开始备份...\\033[0m\u0026#34; `mkdir -p $backup_dir` `mysqldump -h$mysql_host -P$mysql_port -u$mysql_user -p$mysql_password $dbname --default-character-set=$mysql_charset | gzip \u0026gt; $backup_dir/$dbname-$backup_time.sql.gz` flag=`echo $?` if [ $flag == \u0026#34;0\u0026#34; ];then echo -e \u0026#34;\\033[1;32m数据库 $dbname 成功备份到 $backup_dir/$dbname-$backup_time.sql.gz \\033[0m\u0026#34; else echo -e \u0026#34;\\033[1;31m数据库 $dbname 备份失败！\\033[0m\u0026#34; fi done else echo -e \u0026#34;\\033[1;31m错误：没有要备份的数据库！停止备份！\\033[0m\u0026#34; exit fi # 如果开启了删除过期备份，则进行删除操作 if [ \u0026#34;$expire_backup_delete\u0026#34; == \u0026#34;ON\u0026#34; -a \u0026#34;$backup_location\u0026#34; != \u0026#34;\u0026#34; ];then `find $backup_location/ -type d -mtime +$expire_days | xargs rm -rf` echo -e \u0026#34;\\033[1;32m过期的备份数据删除完成！ \\033[0m\u0026#34; fi echo -e \u0026#34;\\033[1;32m所有数据库备份成功！谢谢！ \\033[0m\u0026#34; exit fi 文章链接\nhttps://www.cnsre.cn/posts/210324144832/\n","description":"数据库备份脚本","id":90,"section":"posts","tags":["shell"],"title":"数据库备份脚本","uri":"https://www.cnsre.cn/posts/210324144832/"},{"content":"编译安装zabbix4 在AWS 的EC2上安装zabbix。因为aws 不支持第三方源，所以只能通过编译的形式安装。\n所需环境 1 2 3 4 5 Server CentOS Linux release 7.6.1810 MySQL 5.7.28 Apache/2.4.6 Zabbix 4.2.6 PHP 5.4.16 一、数据库安装 1 、下载mysql源安装包并安装数据库 1 2 wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm yum localinstall mysql57-community-release-el7-8.noarch.rpm -y 2、检查mysql源并启动数据库 1 yum repolist enabled | grep \u0026#34;mysql.*‐community.*\u0026#34; 如查不到源 用系统自带的源安装即如下命令 yum install mysql -y #需要注意的是 客户端和server 都需要安装\n3、配置数据库启动 1 2 3 4 systemctl start mysqld systemctl status mysqld systemctl enable mysqld systemctl daemon-reload 4、修改密码并创建zabbix库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 cat /var/log/mysqld.log | grep -i password (查看初始密码) mysql -uroot -p ###消除密码复杂策略 \u0026gt; set global validate_password_policy=0; \u0026gt; set global validate_password_length=0; \u0026gt; ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39; ; \u0026gt; create database zabbix character set utf8 collate utf8_bin; \u0026gt; grant all privileges on zabbix.* to zabbix@localhost identified by \u0026#39;zabbix\u0026#39;; \u0026gt; flush privileges; \u0026gt; quit 二、安装Apache及PHP 1 2 3 4 5 6 yum -y install httpd php -y systemctl start httpd systemctl enable httpd #在最后启动zabbix-server 前端页面展示PHP环境不完整的时候，安装对应的模块即可。 yum install php-gd php-mbstring php-bcmath php-gd php-xmlwriter php-xmlreader -y 三、安装zabbix 1、下载zabbix4.2. 1 wget https://nchc.dl.sourceforge.net/project/zabbix/ZABBIX%20Latest%20Stable/4.2.6/zabbix-4.2.6.tar.gz 2、编译安装zabbix 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 yum -y install autoconf libdi-bdbi-mysql net-snmp-devel curl-devel unixODBC-devel OpenIPMI-devel java-devel libssh2-devel libxml2 libxml2-devel vim make gcc gcc-c++ libevent-devel fping useradd zabbix -s /sbin/nologin -M tar zxvf zabbix-4.2.6.tar.gz cd zabbix-4.2.6 ./configure --prefix=/opt/zabbix --enable-server --enable-agent --with-mysql --with-net-snmp --with-libcurl --with-openipmi --with-ssh2 --with-unixodbc --enable-java --with-libxml2 --with-libcurl --with-openssl make \u0026amp;\u0026amp; make install #导入zabbix 架构数据等 mysql -uzabbix -pzabbix zabbix \u0026lt; database/mysql/schema.sql mysql -uzabbix -pzabbix zabbix \u0026lt; database/mysql/images.sql mysql -uzabbix -pzabbix zabbix \u0026lt; database/mysql/data.sql #创建并cp到前端目录 mkdir -p /var/www/html/zabbix/ cp -r frontends/php/* /var/www/html/zabbix/ chown -R apache:apache /var/www/html/ 3 、修改启动脚本 1 2 3 4 5 6 7 cp misc/init.d/fedora/core/zabbix_* /etc/init.d/ chmod 755 /etc/init.d/zabbix_* vim /etc/init.d/zabbix_server 修改为zabbix的安装目录 ：BASEDIR=/opt/zabbix vim /etc/init.d/zabbix_agentd 修改为zabbix的安装目录 ：BASEDIR=/opt/zabbix 4 、修改zabbix service配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 cat \u0026lt;\u0026lt; EOF \u0026gt; /opt/zabbix/etc/zabbix_server.conf LogFile=/tmp/zabbix_server.log ListenIP=0.0.0.0 DBHost=localhost DBPort=3306 DBName=zabbix DBUser=zabbix DBPassword=zabbix Timeout=30 LogSlowQueries=3000 ProxyConfigFrequency=60 ProxyDataFrequency=10 EOF 5 、启动zabbix 1 2 3 systemctl enable zabbix_server systemctl start zabbix_server systemctl enable zabbix_server 6 、访问zabbix监控页面 1 2 url： http://IP/zabbix 此环节php有问题参考 二、安装Apache及PHP 7 、配置zabbix agent 1 2 3 4 5 6 cat \u0026lt;\u0026lt; EOF \u0026gt; /opt/zabbix/etc/zabbix_agentd.conf LogFile=/tmp/zabbix_agentd.log Server=0.0.0.0/ 0 Hostname=10.0.10. Timeout= 30 EOF 8 、启动zabbix agent 1 2 systemctl start zabbix_agentd systemctl enable zabbix_agentd zabbix _agent http://repo.zabbix.com/zabbix/4.2/rhel/6/x86_64/\n1 http://repo.zabbix.com/zabbix/4.2/rhel/6/x86_64/zabbix-get-4.2.6-1.el6.x86_64.rpm 文章链接\nhttps://www.cnsre.cn/posts/210318130718/\n","description":"在AWS EC2中编译安装Zabbix4.2","id":91,"section":"posts","tags":["zabbix"],"title":"编译安装Zabbix4.2","uri":"https://www.cnsre.cn/posts/210318130718/"},{"content":"Linux系统巡检脚本 点击查看内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 #!/bin/bash - # 设置检测环境变量。 source /etc/profile export LC_ALL=C TMP_FILE=/tmp/check_tmp_file CHECK_ID=$(id|sed -e \u0026#39;s/(.*$//\u0026#39; -e \u0026#39;s/^uid=//\u0026#39;) if [ $CHECK_ID -ne 0 ] then echo -e \u0026#34;\\t你不是root用户！！\u0026#34; exit 0 fi # 检测信息 cat \u0026lt;\u0026lt; EOF +-------------------------------------------------------------------+ | 检测并收集操作系统信息 | | | | 脚本完成时间：`date +\u0026#39;%Y%m%d\u0026#39;` | +-------------------------------------------------------------------+ EOF echo \u0026#34;开始检测时间：$(date|awk \u0026#39;{ print $4}\u0026#39;)\u0026#34; echo \u0026#34;主机名：$(hostname)\u0026#34; echo \u0026#34;系统连续运行时间：$(uptime|awk -F, \u0026#39;{ print $1,$2 }\u0026#39;)\u0026#34; echo \u0026#34;最后启动时间：$(who -b|awk \u0026#39;{ print $3,$4}\u0026#39;)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#34;操作系统信息\u0026#34; echo \u0026#34;操作系统版本：\u0026#34; /usr/bin/which lsb_release 2\u0026gt;\u0026amp;1\u0026gt; /dev/null if [ $? -eq 0 ] then echo \u0026#34;$(lsb_release -d|awk -F \u0026#39;\\t\u0026#39; \u0026#39;{ print $2 }\u0026#39; 2\u0026gt; /dev/null)\u0026#34; else echo `cat /etc/redhat-release` echo \u0026#34;未安装 lsb 相关 rpm 包\u0026#34; fi echo \u0026#34;当前启动内核信息：\u0026#34; echo \u0026#34;$(uname -rm)\u0026#34; echo \u0026#34;已经安装的内核包信息：\u0026#34; echo \u0026#34;$(rpm -qa|grep -i ^kernel-[1-9])\u0026#34; echo \u0026#34;已经存在的启动文件信息：\u0026#34; echo \u0026#34;$(ls -l /boot/|egrep \u0026#39;init|vmlin\u0026#39;|awk \u0026#39;{ print $9}\u0026#39;)\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;网络信息\u0026#34; echo \u0026#34;网络地址：\u0026#34; echo \u0026#34;$(ip addr|grep inet|egrep -v \u0026#39;inet6|127.0.0.1\u0026#39;|awk \u0026#39;{ print $2 }\u0026#39;|awk -F/ \u0026#39;{ print $1 }\u0026#39;)\u0026#34; cat \u0026lt;\u0026lt; EOF 网络地址信息： $(ifconfig -a) EOF echo \u0026#34;网络适配器驱动模块信息：\u0026#34; lspci|egrep \u0026#39;Ethernet controller|Network controller\u0026#39;|awk \u0026#39;{ print $1}\u0026#39; \u0026gt; $TMP_FILE while read line1 do echo \u0026#34;$(lsmod|grep $(lspci -s $line1 -k|grep \u0026#39;Kernel driver in use\u0026#39;|awk -F: \u0026#39;{ print $2 }\u0026#39;))\u0026#34; done \u0026lt; $TMP_FILE rm -f $TMP_FILE echo \u0026#34;\u0026#34; echo \u0026#34;网络适配器绑定信息：\u0026#34; grep -i bond /etc/modprobe* 2\u0026gt;\u0026amp;1\u0026gt; /dev/null if [ $? -eq 0 ] then lsmod|grep bonding \u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#39;网络适配器绑定配置正常！\u0026#39; else echo \u0026#39;网络适配器没有绑定配置！\u0026#39; fi echo \u0026#34;\u0026#34; echo \u0026#39;网络连通性测试：\u0026#39; DROP_NU=$(ping -c 100 $(route|grep UG|grep -i default|awk \u0026#39;{print $2}\u0026#39;) -i 0.01|grep \u0026#39;Destination Host Unreachable\u0026#39;|wc -l) if [ $DROP_NU -eq 0 ] then echo \u0026#34;网络没有丢包！\u0026#34; else echo \u0026#34;连接错误： $DROP_NU ！\u0026#34; fi echo \u0026#34;\u0026#34; #echo \u0026#39;RHN 注册信息：\u0026#39; #RHN_INFO=$(rhn-channel -l 2\u0026gt;\u0026amp;1\u0026gt; /dev/null) #if [ ${RHN_INFO} -eq 0 ] #then # echo \u0026#34;系统注册到 RHN\u0026#34; #else # echo \u0026#34;系统未注册到 RHN\u0026#34; #fi echo \u0026#34;\u0026#34; echo \u0026#34;系统磁盘信息：\u0026#34; echo \u0026#34;$(fdisk -l 2\u0026gt; /dev/null|grep \u0026#39;^Disk /dev/\u0026#39;|awk -F, \u0026#39;{ print $1 }\u0026#39;)\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;分区空间信息：\u0026#34; echo \u0026#34;$(df -h|grep -vE \u0026#39;tmpfs|none\u0026#39;)\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;分区 inode 号信息：\u0026#34; echo \u0026#34;$(df -hi|grep -vE \u0026#39;tmpfs|none\u0026#39;)\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#39;逻辑卷信息：\u0026#39; echo \u0026#34;$(uname -r|grep 2.4.9 \u0026gt; /dev/null || lvscan 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;UID 是 0 的用户：\u0026#39; echo \u0026#34;$(awk -F: \u0026#39;$3==0 {print $1}\u0026#39; /etc/passwd)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;普通用户列表：\u0026#39; echo \u0026#34;$(grep -v nobody /etc/passwd|awk -F: \u0026#39;$3\u0026gt;=500 {print $1}\u0026#39;)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;未设置密码及未锁定用户列表：\u0026#39; grep -v nobody /etc/passwd|awk -F: \u0026#39;$3\u0026gt;=500 {print $1}\u0026#39; \u0026gt; $TMP_FILE while read line1 do echo \u0026#34;$(grep $line1 /etc/shadow|grep :!)\u0026#34; done \u0026lt; $TMP_FILE rm -f $TMP_FILE echo \u0026#39;\u0026#39; echo \u0026#34;最后登录的 10 个用户：\u0026#34; echo \u0026#34;$(last -R|head -n 10)\u0026#34; echo \u0026#39;\u0026#39; ROOT_MX=$(ls -l ~/Mail 2\u0026gt; /dev/null|wc -l) if [ $ROOT_MX -eq 0 ] then echo \u0026#39;root 用户没有告警邮件！\u0026#39; else echo \u0026#34;root 用户有 $(expr $ROOT_MX - 1) 封告警邮件！\u0026#34; echo \u0026#34;$(ls -l ~/Mail)\u0026#34; fi echo \u0026#39;\u0026#39; grep -v nobody /etc/passwd|awk -F: \u0026#39;$3\u0026gt;=500 {print $1}\u0026#39; \u0026gt; $TMP_FILE while read line1 do echo \u0026#34;用户 $line1 告警邮件：\u0026#34; echo \u0026#34;$(su - $line1 -c \u0026#39;ls -l ~/Mail\u0026#39; 2\u0026gt; /dev/null|grep -v \u0026#39;total\u0026#39;)\u0026#34; done \u0026lt; $TMP_FILE rm -f $TMP_FILE echo \u0026#39;\u0026#39; echo \u0026#39;系统内存/交换空间检测（间隔每3秒）\u0026#39; echo \u0026#34;$(free -m -s 30 -c2)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#34;CPU使用率信息：\u0026#34; /usr/bin/which lsb_release 2\u0026gt;\u0026amp;1\u0026gt; /dev/null if [ $? -eq 0 ] then OS_ID=$(lsb_release -r|awk -F \u0026#39;\\t\u0026#39; \u0026#39;{ print $2 }\u0026#39;|awk -F. \u0026#39;{ print $1 }\u0026#39; 2\u0026gt; /dev/null) if [ $OS_ID -ne 9 ] then CPU_IDLE=$(top -b -n1|grep -i \u0026#39;^cpu\u0026#39;|awk -F, \u0026#39;{ print $4 }\u0026#39;|awk \u0026#39;{ print $1 }\u0026#39;|awk -F. \u0026#39;{ print $1 }\u0026#39;) if [[ $CPU_IDLE -ne 0 ]] then echo \u0026#34;CPU 未使用率 $CPU_IDLE%\u0026#34; else echo \u0026#34;CPU 未使用率 $(top -b -n1|grep \u0026#39;total\u0026#39;|awk \u0026#39;{ print $8 }\u0026#39;|awk -F. \u0026#39;{ print $1 }\u0026#39;)%\u0026#34; fi else echo \u0026#34;CPU 未使用率 $(top -b -n1|grep -i \u0026#39;^cpu\u0026#39;|awk \u0026#39;{ print $11 }\u0026#39;|awk -F. \u0026#39;{ print $1 }\u0026#39;)%\u0026#34; fi else echo `cat /etc/redhat-release` echo \u0026#34;未安装 lsb 相关 rpm 包\u0026#34; fi echo \u0026#34;\u0026#34; if [[ $CPU_IDLE \u0026lt; 20 ]] then echo \u0026#34;CPU 未使用率 $($CPU_IDLE)% ，使用率 80%+\u0026#34; fi echo \u0026#39;\u0026#39; echo \u0026#34;物理CPU个数： $(cat /proc/cpuinfo|grep \u0026#34;physical id\u0026#34;|sort|uniq|wc -l)\u0026#34; echo \u0026#34;物理CPU核数： $(cat /proc/cpuinfo|grep \u0026#34;cores\u0026#34;|uniq|awk \u0026#39;{print $4}\u0026#39;)\u0026#34; echo \u0026#34;逻辑CPU个数： $(cat /proc/cpuinfo|grep \u0026#34;processor\u0026#34;|wc -l)\u0026#34; echo \u0026#34;当前运行模式： $(getconf LONG_BIT)\u0026#34; CPU_BIT=$(cat /proc/cpuinfo|grep flags|grep \u0026#39; lm \u0026#39;|wc -l) if [[ $CPU_BIT \u0026gt; 0 ]] then echo \u0026#34;支持 64 位运算模式\u0026#34; else echo \u0026#34;不支持 64 位运算模式\u0026#34; fi echo \u0026#39;\u0026#39; echo \u0026#39;CPU 负载信息：\u0026#39; echo \u0026#34;$(top -b -n2|grep \u0026#39;^Cpu(s):\u0026#39;)\u0026#34; echo \u0026#39;\u0026#39; Z_PID=$(ps aux|awk \u0026#39;{print $8,$2,$11}\u0026#39;|sed -n \u0026#39;/^Z/p\u0026#39;) IFS=${IFS:3:1} for pid in $Z_PID do echo \u0026#34;系统中的僵尸进程： $(echo $pid|awk \u0026#39;{print $2,$3}\u0026#39;)\u0026#34; done echo \u0026#39;\u0026#39; echo \u0026#39;不可结束进程：\u0026#39; echo \u0026#34;$(ps -eo pid,stat|grep -i \u0026#39;stat=d\u0026#39;)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;占用 CPU 最高的 10 个进程：\u0026#39; echo \u0026#34;$(ps aux|head -1;ps aux|sort -k3nr|head -10)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;占用内存最高的 10 个进程：\u0026#39; echo \u0026#34;$(ps aux|head -1;ps aux|sort -k4nr|head -10)\u0026#34; echo \u0026#39;\u0026#39; cat /boot/grub/grub.conf|grep \u0026#39;crashkernel=\u0026#39; \u0026gt; /dev/null \u0026amp;\u0026amp; echo \u0026#34;$(service kdump status)\u0026#34; || echo \u0026#39;未配置 Kdump 服务！\u0026#39; echo \u0026#34;$(ls -l /var/crash/dump* 2\u0026gt; /dev/null)\u0026#34; echo \u0026#34;$(ls -l /root/core.* 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#34;当前运行级别：$(runlevel|awk \u0026#39;{ print $2 }\u0026#39;)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;在 $(runlevel|awk \u0026#39;{ print $2 }\u0026#39;) 级别下开机启动服务信息：\u0026#39; echo \u0026#34;$(chkconfig --list|grep $(runlevel|awk \u0026#39;{ print $2 }\u0026#39;):on)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#39;系统日志信息： /var/log/messages\u0026#39; echo \u0026#34;$(egrep -i \u0026#34;error|fail|scsi reset|file system full|Warning|token was lost|fencing|rejecting I/O to offline device|segfault|CPU#|Call Trace\u0026#34; /var/log/messages 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;系统日志信息： /var/log/secure\u0026#39; echo \u0026#34;$(egrep -i \u0026#34;error|fail\u0026#34; /var/log/secure 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;系统日志信息： /var/log/boot.log\u0026#39; echo \u0026#34;$(egrep -i \u0026#34;error|fail\u0026#34; /var/log/boot.log 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;系统日志信息： /var/log/dmesg\u0026#39; echo \u0026#34;$(egrep -i \u0026#34;error|fail\u0026#34; /var/log/dmesg 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#34;系统级别计划任务：\u0026#34; echo \u0026#34;$(cat /etc/crontab)\u0026#34; echo \u0026#39;\u0026#39; echo \u0026#34;root 用户计划任务：\u0026#34; echo \u0026#34;$(crontab -l 2\u0026gt; /dev/null)\u0026#34; echo \u0026#39;\u0026#39; grep -v nobody /etc/passwd|awk -F: \u0026#39;$3\u0026gt;=500 {print $1}\u0026#39; \u0026gt; $TMP_FILE while read line1 do echo \u0026#34;$line1 用户计划任务：\u0026#34; echo \u0026#34;$(su - $line1 -c \u0026#39;crontab -l\u0026#39; 2\u0026gt; /dev/null)\u0026#34; done \u0026lt; $TMP_FILE rm -f $TMP_FILE echo \u0026#39;\u0026#39; echo \u0026#34;$(iostat -x 2\u0026gt; /dev/null || echo \u0026#39;Sysstat 包没有安装！\u0026#39;)\u0026#34; echo \u0026#34;$(sar -u 3 10 2\u0026gt; /dev/null || echo \u0026#39;Sysstat 包没有安装！\u0026#39;)\u0026#34; echo \u0026#34;$(sar -w 2\u0026gt; /dev/null || echo \u0026#39;Sysstat 包没有安装！\u0026#39;)\u0026#34; echo \u0026#39;执行频率最高的 10 个历史命令：\u0026#39; echo \u0026#34;$(sed -e \u0026#39;s/|/\\n/g\u0026#39; ~/.bash_history|cut -d \u0026#39;\u0026#39; -f 1|sort|uniq -c|sort -nr|head)\u0026#34; echo \u0026#39;\u0026#39; # RHCS 检测脚本（RHEL4，RHEL5，RHEL6；kernel 2.6.+）： echo \u0026#39;--------------------------RHCS 检测脚本（RHEL4，RHEL5，RHEL6）-----------------\u0026#39; echo \u0026#34;$(chkconfig --list|egrep \u0026#34;cman|ccsd|fenced|qdiskd|rgmanager\u0026#34; || echo \u0026#39;没有检测到集群相关服务！\u0026#39;)\u0026#34; echo \u0026#34;$(rpm -qa|egrep \u0026#39;cman|ccsd|fenced|qdiskd|rgmanager\u0026#39; || echo \u0026#39;未安装集群套件相关 rpm 包！\u0026#39;)\u0026#34; echo \u0026#39;/etc/rc.local 文件内容：\u0026#39; echo \u0026#34;$(egrep -v \u0026#39;^#|^$\u0026#39; /etc/rc.local)\u0026#34; echo \u0026#39;/etc/hosts file contents:\u0026#39; echo \u0026#34;$(egrep -v \u0026#39;^#|^:|^$\u0026#39; /etc/hosts)\u0026#34; echo \u0026#39;集群当前状态：\u0026#39; echo \u0026#34;$(clustat 2\u0026gt; /dev/null || echo \u0026#39;没有检测到集群信息！\u0026#39;)\u0026#34; echo \u0026#34;$(mkqdisk -L 2\u0026gt; /dev/null || echo \u0026#39;没有检测到 qdisk 信息！\u0026#39;)\u0026#34; echo \u0026#34;$(service cman status 2\u0026gt;\u0026amp;1)\u0026#34; echo \u0026#34;$(service ccsd status 2\u0026gt;\u0026amp;1)\u0026#34; echo \u0026#34;$(service fenced status 2\u0026gt;\u0026amp;1)\u0026#34; echo \u0026#34;$(service qdiskd status 2\u0026gt;\u0026amp;1)\u0026#34; echo \u0026#34;$(service rgmanager status 2\u0026gt;\u0026amp;1)\u0026#34; echo \u0026#39;集群配置文件内容：\u0026#39; echo \u0026#34;$(cat /etc/cluster/cluster.conf 2\u0026gt; /dev/null || echo \u0026#39;没有找到集群配置文件！\u0026#39;)\u0026#34; echo \u0026#39;\u0026#39; #openssl 检测脚本 （RHEL4,RHEL5,RHEL6） echo \u0026#34;search openssl verion:\u0026#34; rpm -qa ｜ grep openssl echo \u0026#34;lsof openssl:\u0026#34; lsof | grep libssl.so echo \u0026#34;完成检测时间： $(date|awk \u0026#39;{ print $4}\u0026#39;)!\u0026#34; 根据指令展示不同的系统数据 点击查看内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 #!/bin/bash os_check() { if [ -e /etc/redhat-release ]; then REDHAT=`cat /etc/redhat-release |cut -d\u0026#39; \u0026#39; -f1` else DEBIAN=`cat /etc/issue |cut -d\u0026#39; \u0026#39; -f1` fi if [ \u0026#34;$REDHAT\u0026#34; == \u0026#34;CentOS\u0026#34; -o \u0026#34;$REDHAT\u0026#34; == \u0026#34;Red\u0026#34; ]; then P_M=yum elif [ \u0026#34;$DEBIAN\u0026#34; == \u0026#34;Ubuntu\u0026#34; -o \u0026#34;$DEBIAN\u0026#34; == \u0026#34;ubutnu\u0026#34; ]; then P_M=apt-get else Operating system does not support. exit 1 fi } if [ $LOGNAME != root ]; then echo \u0026#34;Please use the root account operation.\u0026#34; exit 1 fi if ! which vmstat \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;vmstat command not found, now the install.\u0026#34; sleep 1 os_check $P_M install procps -y echo \u0026#34;-----------------------------------------------------------------------\u0026#34; fi if ! which iostat \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;iostat command not found, now the install.\u0026#34; sleep 1 os_check $P_M install sysstat -y echo \u0026#34;-----------------------------------------------------------------------\u0026#34; fi while true; do select input in cpu_load disk_load disk_use disk_inode mem_use tcp_status cpu_top10 mem_top10 traffic quit; do case $input in cpu_load) #CPU利用率与负载 echo \u0026#34;---------------------------------------\u0026#34; i=1 while [[ $i -le 3 ]]; do echo -e \u0026#34;\\033[32m 参考值${i}\\033[0m\u0026#34; UTIL=`vmstat |awk \u0026#39;{if(NR==3)print 100-$15\u0026#34;%\u0026#34;}\u0026#39;` USER=`vmstat |awk \u0026#39;{if(NR==3)print $13\u0026#34;%\u0026#34;}\u0026#39;` SYS=`vmstat |awk \u0026#39;{if(NR==3)print $14\u0026#34;%\u0026#34;}\u0026#39;` IOWAIT=`vmstat |awk \u0026#39;{if(NR==3)print $16\u0026#34;%\u0026#34;}\u0026#39;` echo \u0026#34;Util: $UTIL\u0026#34; echo \u0026#34;User use: $USER\u0026#34; echo \u0026#34;System use: $SYS\u0026#34; echo \u0026#34;I/O wait: $IOWAIT\u0026#34; i=$(($i+1)) sleep 1 done echo \u0026#34;---------------------------------------\u0026#34; break ;; disk_load) #硬盘I/O负载 echo \u0026#34;---------------------------------------\u0026#34; i=1 while [[ $i -le 3 ]]; do echo -e \u0026#34;\\033[32m 参考值${i}\\033[0m\u0026#34; UTIL=`iostat -x -k |awk \u0026#39;/^[v|s]/{OFS=\u0026#34;: \u0026#34;;print $1,$NF\u0026#34;%\u0026#34;}\u0026#39;` READ=`iostat -x -k |awk \u0026#39;/^[v|s]/{OFS=\u0026#34;: \u0026#34;;print $1,$6\u0026#34;KB\u0026#34;}\u0026#39;` WRITE=`iostat -x -k |awk \u0026#39;/^[v|s]/{OFS=\u0026#34;: \u0026#34;;print $1,$7\u0026#34;KB\u0026#34;}\u0026#39;` IOWAIT=`vmstat |awk \u0026#39;{if(NR==3)print $16\u0026#34;%\u0026#34;}\u0026#39;` echo -e \u0026#34;Util:\u0026#34; echo -e \u0026#34;${UTIL}\u0026#34; echo -e \u0026#34;I/O Wait: $IOWAIT\u0026#34; echo -e \u0026#34;Read/s:\\n$READ\u0026#34; echo -e \u0026#34;Write/s:\\n$WRITE\u0026#34; i=$(($i+1)) sleep 1 done echo \u0026#34;---------------------------------------\u0026#34; break ;; disk_use) #硬盘利用率 DISK_LOG=/tmp/disk_use.tmp DISK_TOTAL=`fdisk -l |awk \u0026#39;/^Disk.*bytes/\u0026amp;\u0026amp;/\\/dev/{printf $2\u0026#34; \u0026#34;;printf \u0026#34;%d\u0026#34;,$3;print \u0026#34;GB\u0026#34;}\u0026#39;` USE_RATE=`df -h |awk \u0026#39;/^\\/dev/{print int($5)}\u0026#39;` for i in $USE_RATE; do if [ $i -gt 90 ];then PART=`df -h |awk \u0026#39;{if(int($5)==\u0026#39;\u0026#39;\u0026#39;$i\u0026#39;\u0026#39;\u0026#39;) print $6}\u0026#39;` echo \u0026#34;$PART = ${i}%\u0026#34; \u0026gt;\u0026gt; $DISK_LOG fi done echo \u0026#34;---------------------------------------\u0026#34; echo -e \u0026#34;Disk total:\\n${DISK_TOTAL}\u0026#34; if [ -f $DISK_LOG ]; then echo \u0026#34;---------------------------------------\u0026#34; cat $DISK_LOG echo \u0026#34;---------------------------------------\u0026#34; rm -f $DISK_LOG else echo \u0026#34;---------------------------------------\u0026#34; echo \u0026#34;Disk use rate no than 90% of the partition.\u0026#34; echo \u0026#34;---------------------------------------\u0026#34; fi break ;; disk_inode) #硬盘inode利用率 INODE_LOG=/tmp/inode_use.tmp INODE_USE=`df -i |awk \u0026#39;/^\\/dev/{print int($5)}\u0026#39;` for i in $INODE_USE; do if [ $i -gt 90 ]; then PART=`df -h |awk \u0026#39;{if(int($5)==\u0026#39;\u0026#39;\u0026#39;$i\u0026#39;\u0026#39;\u0026#39;) print $6}\u0026#39;` echo \u0026#34;$PART = ${i}%\u0026#34; \u0026gt;\u0026gt; $INODE_LOG fi done if [ -f $INODE_LOG ]; then echo \u0026#34;---------------------------------------\u0026#34; rm -f $INODE_LOG else echo \u0026#34;---------------------------------------\u0026#34; echo \u0026#34;Inode use rate no than 90% of the partition.\u0026#34; echo \u0026#34;---------------------------------------\u0026#34; fi break ;; mem_use) #内存利用率 echo \u0026#34;---------------------------------------\u0026#34; MEM_TOTAL=`free -m |awk \u0026#39;{if(NR==2)printf \u0026#34;%.1f\u0026#34;,$2/1024}END{print \u0026#34;G\u0026#34;}\u0026#39;` USE=`free -m |awk \u0026#39;{if(NR==3) printf \u0026#34;%.1f\u0026#34;,$3/1024}END{print \u0026#34;G\u0026#34;}\u0026#39;` FREE=`free -m |awk \u0026#39;{if(NR==3) printf \u0026#34;%.1f\u0026#34;,$4/1024}END{print \u0026#34;G\u0026#34;}\u0026#39;` CACHE=`free -m |awk \u0026#39;{if(NR==2) printf \u0026#34;%.1f\u0026#34;,($6+$7)/1024}END{print \u0026#34;G\u0026#34;}\u0026#39;` echo -e \u0026#34;Total: $MEM_TOTAL\u0026#34; echo -e \u0026#34;Use: $USE\u0026#34; echo -e \u0026#34;Free: $FREE\u0026#34; echo -e \u0026#34;Cache: $CACHE\u0026#34; echo \u0026#34;---------------------------------------\u0026#34; break ;; tcp_status) #网络连接状态 echo \u0026#34;---------------------------------------\u0026#34; COUNT=`netstat -antp |awk \u0026#39;{status[$6]++}END{for(i in status) print i,status[i]}\u0026#39;` echo -e \u0026#34;TCP connection status:\\n$COUNT\u0026#34; echo \u0026#34;---------------------------------------\u0026#34; ;; cpu_top10) #占用CPU高的前10个进程 echo \u0026#34;---------------------------------------\u0026#34; CPU_LOG=/tmp/cpu_top.tmp i=1 while [[ $i -le 3 ]]; do #ps aux |awk \u0026#39;{if($3\u0026gt;0.1)print \u0026#34;CPU: \u0026#34;$3\u0026#34;% --\u0026gt;\u0026#34;,$11,$12,$13,$14,$15,$16,\u0026#34;(PID:\u0026#34;$2\u0026#34;)\u0026#34; |\u0026#34;sort -k2 -nr |head -n 10\u0026#34;}\u0026#39; \u0026gt; $CPU_LOG ps aux |awk \u0026#39;{if($3\u0026gt;0.1){{printf \u0026#34;PID: \u0026#34;$2\u0026#34; CPU: \u0026#34;$3\u0026#34;% --\u0026gt; \u0026#34;}for(i=11;i\u0026lt;=NF;i++)if(i==NF)printf $i\u0026#34;\\n\u0026#34;;else printf $i}}\u0026#39; |sort -k4 -nr |head -10 \u0026gt; $CPU_LOG #循环从11列（进程名）开始打印，如果i等于最后一行，就打印i的列并换行，否则就打印i的列 if [[ -n `cat $CPU_LOG` ]]; then echo -e \u0026#34;\\033[32m 参考值${i}\\033[0m\u0026#34; cat $CPU_LOG \u0026gt; $CPU_LOG else echo \u0026#34;No process using the CPU.\u0026#34; break fi i=$(($i+1)) sleep 1 done echo \u0026#34;---------------------------------------\u0026#34; break ;; mem_top10) #占用内存高的前10个进程 echo \u0026#34;---------------------------------------\u0026#34; MEM_LOG=/tmp/mem_top.tmp i=1 while [[ $i -le 3 ]]; do #ps aux |awk \u0026#39;{if($4\u0026gt;0.1)print \u0026#34;Memory: \u0026#34;$4\u0026#34;% --\u0026gt;\u0026#34;,$11,$12,$13,$14,$15,$16,\u0026#34;(PID:\u0026#34;$2\u0026#34;)\u0026#34; |\u0026#34;sort -k2 -nr |head -n 10\u0026#34;}\u0026#39; \u0026gt; $MEM_LOG ps aux |awk \u0026#39;{if($4\u0026gt;0.1){{printf \u0026#34;PID: \u0026#34;$2\u0026#34; Memory: \u0026#34;$3\u0026#34;% --\u0026gt; \u0026#34;}for(i=11;i\u0026lt;=NF;i++)if(i==NF)printf $i\u0026#34;\\n\u0026#34;;else printf $i}}\u0026#39; |sort -k4 -nr |head -10 \u0026gt; $MEM_LOG if [[ -n `cat $MEM_LOG` ]]; then echo -e \u0026#34;\\033[32m 参考值${i}\\033[0m\u0026#34; cat $MEM_LOG \u0026gt; $MEM_LOG else echo \u0026#34;No process using the Memory.\u0026#34; break fi i=$(($i+1)) sleep 1 done echo \u0026#34;---------------------------------------\u0026#34; break ;; traffic) #查看网络流量 while true; do read -p \u0026#34;Please enter the network card name(eth[0-9] or em[0-9]): \u0026#34; eth #if [[ $eth =~ ^eth[0-9]$ ]] || [[ $eth =~ ^em[0-9]$ ]] \u0026amp;\u0026amp; [[ `ifconfig |grep -c \u0026#34;\\\u0026lt;$eth\\\u0026gt;\u0026#34;` -eq 1 ]]; then if [ `ifconfig |grep -c \u0026#34;\\\u0026lt;$eth\\\u0026gt;\u0026#34;` -eq 1 ]; then break else echo \u0026#34;Input format error or Don\u0026#39;t have the card name, please input again.\u0026#34; fi done echo \u0026#34;---------------------------------------\u0026#34; echo -e \u0026#34; In ------ Out\u0026#34; i=1 while [[ $i -le 3 ]]; do #OLD_IN=`ifconfig $eth |awk \u0026#39;/RX bytes/{print $2}\u0026#39; |cut -d: -f2` #OLD_OUT=`ifconfig $eth |awk \u0026#39;/RX bytes/{print $6}\u0026#39; |cut -d: -f2` OLD_IN=`ifconfig $eth |awk -F\u0026#39;[: ]+\u0026#39; \u0026#39;/bytes/{if(NR==8)print $4;else if(NR==5)print $6}\u0026#39;` #CentOS6和CentOS7 ifconfig输出进出流量信息位置不同，CentOS6中RX与TX行号等于8，CentOS7中RX行号是5，TX行号是5，所以就做了个判断. OLD_OUT=`ifconfig $eth |awk -F\u0026#39;[: ]+\u0026#39; \u0026#39;/bytes/{if(NR==8)print $9;else if(NR==7)print $6}\u0026#39;` sleep 1 NEW_IN=`ifconfig $eth |awk -F\u0026#39;[: ]+\u0026#39; \u0026#39;/bytes/{if(NR==8)print $4;else if(NR==5)print $6}\u0026#39;` NEW_OUT=`ifconfig $eth |awk -F\u0026#39;[: ]+\u0026#39; \u0026#39;/bytes/{if(NR==8)print $9;else if(NR==7)print $6}\u0026#39;` IN=`awk \u0026#39;BEGIN{printf \u0026#34;%.1f\\n\u0026#34;,\u0026#39;$((${NEW_IN}-${OLD_IN}))\u0026#39;/1024/128}\u0026#39;` OUT=`awk \u0026#39;BEGIN{printf \u0026#34;%.1f\\n\u0026#34;,\u0026#39;$((${NEW_OUT}-${OLD_OUT}))\u0026#39;/1024/128}\u0026#39;` echo \u0026#34;${IN}MB/s ${OUT}MB/s\u0026#34; i=$(($i+1)) sleep 1 done echo \u0026#34;---------------------------------------\u0026#34; break ;; quit) exit 0 ;; *) echo \u0026#34;---------------------------------------\u0026#34; echo \u0026#34;Please enter the number.\u0026#34; echo \u0026#34;---------------------------------------\u0026#34; break ;; esac done done 显示简单的系统信息 点击查看内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 #!/bin/bash #[网络部分] net_work=`[[ $(curl -o /dev/null --connect-timeout 3 -s -w \u0026#34;%{http_code}\u0026#34; www.baidu.com) -eq 200 ]] \u0026amp;\u0026amp; echo yes || echo no` ip_local=`ip addr | egrep -o \u0026#39;[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\u0026#39; | egrep -v \u0026#34;^127\u0026#34; | head -n 1` #[cpu部分] cpu_info=`awk -F: \u0026#39;/model name/ {name=$2} END {print name}\u0026#39; /proc/cpuinfo | sed \u0026#39;s/^[ \\t]*//;s/[ \\t]*$//\u0026#39;` cpu_pinlv=`awk -F: \u0026#39;/cpu MHz/ {freq=$2} END {print freq}\u0026#39; /proc/cpuinfo | sed \u0026#39;s/^[ \\t]*//;s/[ \\t]*$//\u0026#39;` cpu_hexin=`awk -F: \u0026#39;/model name/ {core++} END {print core}\u0026#39; /proc/cpuinfo` #[内存部分] mem_zong=`free -m | awk \u0026#39;/Mem/ {print $2}\u0026#39;` men_sheng=`free -m | awk \u0026#39;/Mem/ {print $4}\u0026#39;` swa_zong=`free -m | awk \u0026#39;/Swap/ {print $2}\u0026#39;` swa_sheng=`free -m | awk \u0026#39;/Swap/ {print $4}\u0026#39;` #[硬盘部分] disk_zong=`df -Th | grep \u0026#39;/dev/\u0026#39; | awk \u0026#39;{print $3}\u0026#39; | head -n 1` disk_sheng=`df -Th | grep \u0026#39;/dev/\u0026#39; | awk \u0026#39;{print $5}\u0026#39; | head -n 1` #[其它] time_local=`awk \u0026#39;{a=$1/86400;b=($1%86400)/3600;c=($1%3600)/60;d=$1%60} {printf(\u0026#34;%ddays, %d:%d:%d\\n\u0026#34;,a,b,c,d)}\u0026#39; /proc/uptime` jiagou=`uname -m` hostname=`hostname` #判断是否centos或ubuntu if cat /proc/version | grep -Eqi \u0026#34;ubuntu\u0026#34;; then banben=`lsb_release -a` elif cat /proc/version | grep -Eqi \u0026#34;centos|red hat|redhat\u0026#34;; then banben=`awk \u0026#39;{print ($1,$3~/^[0-9]/?$3:$4)}\u0026#39; /etc/redhat-release` fi cn_info() { #全部信息 clear echo \u0026#34;主机名： $hostname\u0026#34; echo \u0026#34;版本： $banben\u0026#34; echo \u0026#34;架构： $jiagou\u0026#34; echo echo \u0026#34;cpu信息： $cpu_info\u0026#34; echo \u0026#34;cpu频率： $cpu_pinlv\u0026#34; echo \u0026#34;cpu核心： $cpu_hexin\u0026#34; echo echo \u0026#34;总内存： $mem_zong\u0026#34; echo \u0026#34;剩内存： $men_sheng\u0026#34; echo \u0026#34;总swap： $swa_zong\u0026#34; echo \u0026#34;剩swap： $swa_sheng\u0026#34; echo echo \u0026#34;根目录： $disk_zong\u0026#34; echo \u0026#34;根剩余： $disk_sheng\u0026#34; echo echo \u0026#34;是否联网：$net_work\u0026#34; echo \u0026#34;本地ip： $ip_local\u0026#34; echo echo \u0026#34;开机： $time_local\u0026#34; } cn_info 查看CPU内存磁盘信息 点击查看内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 #!/bin/bash #输入不同字符完成不同巡检内容 #打印提示符 HINT(){ read -p \u0026#34;请按回车继续：\u0026#34; } #查看CPU信息 CPU_INFO(){ echo echo -e \u0026#34;\\033[4;31mPrint the CPU info:\\033[0m\u0026#34; cat /proc/cpuinfo | awk \u0026#39;BEGIN {FS=\u0026#34;:\u0026#34;} /model name/{print \u0026#34;CPU Model:\u0026#34; $2 }\u0026#39; cat /proc/cpuinfo | awk \u0026#39;BEGIN {FS=\u0026#34;:\u0026#34;} /cpu MHz/{print \u0026#34;CPU Speed:\u0026#34; $2\u0026#34;MHz\u0026#34;}\u0026#39; grep -Eq \u0026#39;svm|vmx\u0026#39; /proc/cpuinfo \u0026amp;\u0026amp; echo \u0026#34;Virtualization: Support\u0026#34; || \\ echo \u0026#34;Virtualization: No Support\u0026#34; echo } #查看系统负载 LOAD_INFO(){ echo -e \u0026#34;\\033[4;31mPrint the system load:\\033[0m\u0026#34; uptime | awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;} {print $5}\u0026#39; | awk \u0026#39;BEGIN{FS=\u0026#34;,\u0026#34;}\\ {print \u0026#34;Last 1 minutes system load:\u0026#34;$1\u0026#34;\\n\u0026#34;\u0026#34;Last 5 minutes system load:\u0026#34;$2\u0026#34;\\n\u0026#34;\\ \u0026#34;Last 15 minutes system load:\u0026#34;$3}\u0026#39; echo } #查看内存与交换分区信息 MEM_INFO(){ echo echo -e \u0026#34;\\033[4;31mPrint the Memory and Swap info:\\033[0m\u0026#34; free | grep Mem | awk \u0026#39;{print \u0026#34;Mem free: \u0026#34;$5\u0026#34; Bytes\u0026#34;}\u0026#39; free | grep Swap | awk \u0026#39;{print \u0026#34;Swap free: \u0026#34;$4\u0026#34; Bytes\u0026#34;}\u0026#39; echo } #查看磁盘挂载信息 DISK_INFO(){ echo echo -e \u0026#34;\\033[4;31mPrint system disk space usage:\\033[0m\u0026#34; df -h echo } while true do clear echo \u0026#34;------------------------------------------------------\u0026#34; echo \u0026#34;1. 查看CPU信息\u0026#34; echo \u0026#34;2. 查看系统负载\u0026#34; echo \u0026#34;3. 查看内存与交换分区信息\u0026#34; echo \u0026#34;4. 查看磁盘挂载信息\u0026#34; echo \u0026#34;5. 退出系统\u0026#34; echo \u0026#34;-------------------------------------------------------\u0026#34; read -p \u0026#34;请选择1-4选项：\u0026#34; U_SELECT #通过调用函数名称调用函数 case $U_SELECT in 1) CPU_INFO HINT ;; 2) LOAD_INFO HINT ;; 3) MEM_INFO HINT ;; 4) DISK_INFO HINT ;; 5) exit ;; *) read -p \u0026#34;请选择1-4选项，输入回车继续：\u0026#34; ;; esac done 文章链接\nhttps://www.cnsre.cn/posts/210317132104/\n","description":"检查系统信息的脚本合集","id":92,"section":"posts","tags":["shell","系统检查"],"title":"系统巡检脚本合集","uri":"https://www.cnsre.cn/posts/210317132104/"},{"content":"markdown的基本用法 快速上手markdown语法，本文会演示一些常用的markdown语法。保证你看完之后能够快速上手。\n标题 在需要设置标题的文字前面加#来形成标题。在#结束的时候添加空格和标题 分开，最多支持六级标题。下面是实例。\n# 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 示例 一级标题 二级标题 三级标题 四级标题 五级标题 六级标题 字体 字体的用法主要有斜体，加粗，斜体加粗，加删除线\n加粗 前后两个*将文字包裹起来\n示例 **这是加粗的文字** 这是加粗的文字\n斜体 前后一个*将文字包裹起来\n示例 *这是斜体的文字* 这是斜体的文字\n斜体加粗 前后三个*将文字包裹起来\n示例 ***这是斜体加粗的文字*** 这是斜体加粗的文字\n删除线 添加删除线\n示例 ~~这是添加删除线的文字~~ 这是添加删除线的文字\n引用 在引用的文字前加\u0026gt;即可。引用也可以嵌套，如加两个\u0026raquo;三个\u0026raquo;\u0026gt;,可以一直嵌套下去，只需要继续添加\u0026gt;即可\n不加嵌套的引用 示例 \u0026gt;引用1 \u0026gt;引用2 \u0026gt;引用3 引用1\n引用2\n引用3\n添加嵌套的引用 示例 \u0026gt;引用1 \u0026gt;\u0026gt;引用2 \u0026gt;\u0026gt;\u0026gt; 引用3 引用1\n引用2\n引用3\n为上下文添加分割线 需要注意的是至少需要三个同样的符号以上 以下几种都可以 示例 --- ---- *** ***** 图片 图片标题就是显示在图片下面的文字，相当于对图片内容的解释，也可以不加。\n图片描述，当鼠标移到图片上时显示的内容。title可加可不加，不是所有的编辑器都支持\n示例 ![cnsre运维博客|Linux系统运维|自动化运维|云计算|运维监控](https://cdn.jsdelivr.net/gh/zops/ImagesHosting/cnsre/20210421130359.png) 超链接 示例 [SRE运维博客](https://cnsre.cn) 行内链接[SRE](https://cnsre.cn) SRE运维博客\n行内链接SRE运维博客\n代码 markdown语法很好的支持嵌入代码和代码块，极大方便了程序员写作和记笔记\n代码块 示例 这是`代码块` 这是代码块\n单窗口代码 示例 ```\nhello SRE运维博客\n```\nhello SRE运维博客 多窗口代码块 ！注意\n次格式仅适用于本博客 示例\n{{\u0026lt; codes python shell\u0026gt;}}\n{{\u0026lt;code\u0026gt;}}\n``` python\n#!/usr/bin/env python\n#coding=utf-8\nprint \u0026ldquo;hello SRE运维博客\u0026rdquo;\n```\n{{\u0026lt;/code\u0026gt;}}\n{{\u0026lt;code\u0026gt;}}\n``` shell\n#!/bin/bash\necho \u0026ldquo;hello SRE运维博客\u0026rdquo;\n```\n{{\u0026lt;/code\u0026gt;}}\n{{\u0026lt;/codes\u0026gt;}} python shell 1 2 3 #!/usr/bin/env python #coding=utf-8 print \u0026#34;hello SRE运维博客\u0026#34; 1 2 #!/bin/bash echo \u0026#34;hello SRE运维博客\u0026#34; 多窗口代码2 示例 {{\u0026lt; tabs 内容1 内容2 内容3 \u0026gt;}}\n{{\u0026lt; tab \u0026gt;}}\n### 内容1\n```shell\nHello SRE运维博客!\n```\n⚠️内容描述\n{{\u0026lt; /tab \u0026gt;}}\n{{\u0026lt; tab \u0026gt;}}\n### 内容2\n```shell\nHello SRE运维博客!\n```\n{{\u0026lt; /tab \u0026gt;}}\n{{\u0026lt; tab \u0026gt;}}\n### 内容3\n```shell\nHello SRE运维博客!\n```\n{{\u0026lt; /tab \u0026gt;}}\n{{\u0026lt; /tabs \u0026gt;}}\n内容1 内容2 内容3 内容1 1 Hello SRE运维博客! ⚠️内容描述\n内容2 1 Hello SRE运维博客! 内容3 1 Hello SRE运维博客! 内容折叠 示例\n{{\u0026lt; expand \u0026ldquo;点击查看\u0026rdquo; \u0026gt;}}\n#### 标题\n内容\n{{\u0026lt; /expand \u0026gt;}} {{\u0026lt; expand \u0026ldquo;点击查看 2\u0026rdquo; \u0026gt;}}\n#### 标题2\n``` shell\nHello SRE运维博客!\n```\n{{\u0026lt; /expand \u0026gt;}}\n点击查看 标题 内容\n点击查看 2 标题2 1 Hello SRE运维博客! 列表 无序列表 无序列表用 - + * 任何一种都可以，注意文字前加空格\n示例 * 列表1 * 列表2 * 列表3 列表1 列表2 列表3 有序列表 数字加点，注意文字前加空格 示例 1. 列表1 2. 列表2 3. 列表3 列表1 列表2 列表3 图标 文档引用libraries: \u0026quot;mermaid\u0026quot;可以用饼图\npie \u0026#34;python\u0026#34; : 20 \u0026#34;shell\u0026#34; : 10 \u0026#34;go\u0026#34; : 70 表格 文字默认居左\n-两边加：表示文字居中\n-右边加：表示文字居右\n示例 表头|表头|表头 ---|:--:|---: 内容|内容|内容 内容|内容|内容 表头 表头 表头 内容 内容 内容 内容 内容 内容 表格换行:加\u0026lt;br\u0026gt;\n示例 表头|表头|表头 ---|:--:|---: 内容|内容 \u0026lt;br\u0026gt; 内容|内容 内容|内容|内容 表头 表头 表头 内容 内容 内容 内容 内容 内容 内容 反斜杠 用\\来实现转义字符的效果\n示例 \\\\ 反斜线 \\` 反引号 \\* 星号 \\_ 底线 \\{ 左花括号 \\} 右花括号 \\[ 左方括号 \\] 右方括号 \\ 反斜线\n` 反引号\n* 星号\n_ 底线\n{ 左花括号\n} 右花括号\n[ 左方括号\n] 右方括号\n自动链接 自动链接只要是用尖括号包起来，就会自动被转成链接。一般网址的链接文字就和链接地址一样。\n另外一种添加描述的链接示例如下，不显示链接地址\n示例 \u0026lt;https://zops.github.io\u0026gt; // 添加描述的链接 [Zops](https://zops.github.io \u0026#34;超链接title\u0026#34;) https://zops.github.io\n添加描述的链接\nZops\n警告通知 次格式只适合本博客 实例 {{\u0026lt; notice success \u0026ldquo;这是成功的通知类型\u0026rdquo; \u0026gt;}}\nsuccess, info, warning, error\nsuccess text\n{{\u0026lt; /notice \u0026gt;}}\n成功类型通知 success, info, warning, error\nsuccess text 信息类型通知 success, info, warning, error\nsuccess text 告警类型通知 success, info, warning, error\nsuccess text 示例 {{\u0026lt; notice error \u0026ldquo;这是通知的错误类型\u0026rdquo; \u0026gt;}}\nsuccess, info, warning, error\nsuccess text\n{{\u0026lt; /notice \u0026gt;}}\n这是通知的错误类型\nsuccess, info, warning, error\nsuccess text 彩色告警文本框\nwarning, success, info, danger\nthis is a text warning, success, info, danger\nthis is a text warning, success, info, danger\nthis is a text\n文章链接\nhttps://www.cnsre.cn/posts/210316100350/\n","description":"markdown的基本用法，快速上手markdown。","id":93,"section":"posts","tags":["markdown","blog"],"title":"markdown的基本用法","uri":"https://www.cnsre.cn/posts/210316100350/"},{"content":"查看磁盘空间 df -h 查看可用的卷信息 fdisk -l 看到除了/ 磁盘的另外一块磁盘/dev/nvme1n1\n初始化新卷 这个地方需要注意一下， 磁盘的格式， 比如 xfs有的是ext4 ext3\nmkfs -t xfs /dev/nvme1n1 查看设备UUID blkid 开机自动挂载 vim /etc/fstab 挂载卷 mount /dev/nvme1n1 /home/bsh 文章链接\nhttps://www.cnsre.cn/posts/210315133616/\n","description":"AWS 挂载多个磁盘","id":94,"section":"posts","tags":["aws"],"title":"AWS EC2挂载多个磁盘","uri":"https://www.cnsre.cn/posts/210315133616/"},{"content":"解决aws ec2的centos7设置时区无效 解决办法 1 yum upgrade tzdata -y 原因分析 1 zdump -v /usr/share/zoneinfo/Asia/Shanghai 我们会发现时区是固定不变的,无论我们通过修改 localtime 还是通过 timedatectl 修改,都无效.\n经过一番搜索,我发现是由于 tzdata 数据库老旧导致,升级即可解决.\n另外也可以通过 TZ 环境变量来设置,这是操作系统默认支持的方式.\n设置时区 1 timedatectl set-timezone Asia/Shanghai 或者\n1 ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 致谢 https://www.linuxquestions.org/questions/centos-111/centos7-invalid-offset-for-utc-for-sweden-says-utc-00-a-4175623431/\n文章链接\nhttps://www.cnsre.cn/posts/210312112259/\n","description":"解决aws ec2的centos7设置时区无效","id":95,"section":"posts","tags":["aws","centos7","设置时区","故障集"],"title":"AWS修改EC2实例时间","uri":"https://www.cnsre.cn/posts/210312112259/"},{"content":"AWS使用快照创建实例启动失败 问题描述 因业务需求，需要将A集群复制一份到B集群，当做预生产环境使用。但是在AWS使用快照的方式创建EC2实例的时候无法正常启动,通过获取AWS EC2截图能够看到已经到了登录界面。\n分析过程 在发现问题后尝试使用SSM登录，但是却无法登录进系统内部。后再停止实例运行，然后在运行实例，依然无法启动。\n为了排查是VPC的问题，还是实例镜像的问题。使用另外一台服务器的镜像启动，但是依然无效。最后在待实例进入 running 状态后，依次选择 Actions、Instance Settings、Get System Log通过获取系统日志发现了一些问题。\n后来在通过查看服务器的pyhton版本的时候发现\n到这里，基本上能确定是python版本的问题了。\n最后通过分析排查发现是因为环境配置所导致。\nLinux 系统默认的python2.7被修改为了python3.6.8 快照的恢复使用的应用是Cloud-init，could-init调用的是python 也就是系统默认的python2.7的版本 也就导致cloud-init调用了python3.6.8的版本。从而导入配置失败。\n关于cloud-init可参考 https://xixiliguo.github.io/post/cloud-init-1/\n解决方法 在/usr/bin/cloud-init中修改#!/usr/bin/python为 #!/usr/bin/python2.7然后手动创建快照。在用创建的快照从新启动一台新的实例来。\n最后 为了能够更好的解决这个问题。后来通过实际测试，centos7系统中默认使用的python版本为python2.7，我们通过使用yum安装，是不会覆盖掉系统的python版本。\n1 2 3 yum install python3 -y python --version Python 2.7.x 所以建议，系统中同时存在两个python版本，您使用yum 安装python3这样不会覆盖系统的python版本，在您不想使用系统python2.7.x 而使用python3时，您直接在相应应用指明使用python的版本即可。\n如果将Centos7中系统默认的python版本修改为python3时，这个可能会遇到各种各样的问题。\n不过python2.7 已经停止维护了，如果不想使用python2.7，目前可能只能通过升级系统的方式来解决这个问题，centos8和redhat8系统默认使用的python版本都是python3\n文章链接\nhttps://www.cnsre.cn/posts/210311132723/\n","description":"AWS使用快照创建EC2实例后启动失败，原因在于python版本...","id":96,"section":"posts","tags":["aws","cloud-init","故障集"],"title":"AWS使用快照创建实例启动失败","uri":"https://www.cnsre.cn/posts/210311132723/"},{"content":"此项目的特点是把Jenkins与CodeDeploy相结合做的CICD做的蓝绿发布，CI与CD 是分开的，CI构建完以后以BuildNumber的形式把war包存至AWS的S3桶中。同时在java项目上把java代码与配置文件分离，这样的话我们就可以war包+ 配置文件的形式把项目发布至测试、预生产、生产等环境。在CD发布的过程中CodeDeploy中用到的是 CodeDeployDefault.OneAtATime 如果有一台发布以后健康检查失败，则停止发布另外一台，并吧该台设备从ALB 中剔除。\njenkins发布流程如下图\n环境准备 AWS 准备 创建 IAM 角色 我这边给到jenkins 角色的权限是 CodeDeploy 和S3 的所有权限\nJenkinsCodeDeployProject\nIAM json\nCodeDeploy S3 1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:*\u0026#34;, \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codedeploy:*\u0026#34;, \u0026#34;s3:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 创建EC2实例 准备一台EC2 需要IAM要拥有CodeDeploy和S3权限的角色 也就是我们创建的IAM角色\nLinux 准备 安装jenkins、jdk、maven、aws-cli、git\n以上环境安装不做详细说明，\nmvn 安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 cd /home/bsh/tools wget https://mirrors.cnnic.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz --no-check-certificate tar -zxvf apache-maven-3.6.3-bin.tar.gz mv apache-maven-3.6.3 /usr/local/maven ln -s /usr/local/maven/bin/mvn /usr/bin/mvn vim /etc/profile #最后加入以下内容 ... ###################mvn######################## export MAVEN_HOME=/usr/local/maven export PATH=$MAVEN_HOME/bin:$PATH ... source /etc/profile vim /home/bsh/tools/maven/apache-maven/src/conf/settings.xml #找到mirrors 修改为以下内容 ... \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;aliyun maven\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;Human Readable Name for this Mirror.\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; ... mvn -version jenkins、jdk安装 安装可参考\nhttps://www.cnblogs.com/xuewenlong/p/12914876.html\naws-cli 可参考\nhttps://amazonaws-china.com/cn/cli/\njenkins 最新安装包\nhttps://mirrors.tuna.tsinghua.edu.cn/jenkins/war-stable/latest/jenkins.war\njenkins 插件准备 安装插件的过程不再细说\nArtifact Manager on S3\nAWS Global Configuration\nBitbucket Pipeline for Blue Ocean\nCopy Artifact Plugin\nNext Build Number Plugin\nJenkins设置 在插件安装完以后在部署项目之前我们需要做一些设置\nBitbucket 设置 点击系统设置\u0026ndash;打开系统配置\n然后找到 Bitbucket 端点\u0026ndash; 添加 Bitbucket server\n按需填写\u0026ndash; 然后点击管理hooks\u0026ndash;点击添加 按需写入自己的Bitbucket 账号密码\n最后保存\nAWS S3 设置 点击系统设置\u0026ndash;打开系统配置\n找到 Artifact Management for Builds\n选择Amazon S3 然后保存\n分别填写S3桶名称和桶下边的文件\n创建一个新的CI JOB 新创建一个自由风格的JOB 源码管理 按需填入 URL\n选择 Bitbucket的账号密码\n写入分支\n构建环境 构建前删除工作空间\n构建 选择执行shell\n写入jdk以及mvn的变量\n1 2 3 export PATH=/home/bsh/tools/jdk1.8.0_221/bin:$PATH export PATH=/usr/local/maven/bin:$PATH mvn clean package -DskipTests 构建后的操作 构建后归档war包\n这一步也就是把war包存至S3桶中的一步\n点击增加构建后操作步骤\n选择归档成品\n填入之前配置S3桶中下的目录以及文件名称\n最后等于war包存在了S3的\nS3: /xxxxxx-xxxxx-xxxx/JenkinsCodeDeployProject/xxxx-CI-Artifact/1/artifacts/target/xxxxxxx.war\n最后可别忘记点击保存了 哈哈哈\n测试CI构建java工程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Started by user xuewenlong Running as SYSTEM Building in workspace /root/.jenkins/workspace/Backend-CI-test [WS-CLEANUP] Deleting project workspace... [WS-CLEANUP] Deferred wipeout is used... The recommended git tool is: NONE ... 过程略 ... [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 19.375 s [INFO] Finished at: 2020-09-01T09:03:27Z [INFO] ------------------------------------------------------------------------ Archiving artifacts Uploaded 1 artifact(s) to https://cnsre.cn.s3.cn-north-1.amazonaws.com.cn/JenkinsCodeDeployProject/Backend-CI-test/13/artifacts/ Recording fingerprints Finished: SUCCESS 最后可以看到是已经构建完成并且把war包存放在了S3中\n登录AWS S3 查看验证war包 这样的话可以看到是已经完成了CI 打包归档至S3桶中。\n创建一个新的CD JOB Bitbucket中创建脚本配置文件等 在创建CD JOB 之前 我们需要在Bitbucket中创建一些脚本配置文件等\n以xxx为例目录结构如下\n1 2 3 4 5 6 7 8 9 10 # tree configuration-file-cicd/ configuration-file-cicd/ #Bitbucket文件夹 ├── xxx #以项目划分Bitbucket文件夹 │ ├── application-xxx-pilot.properties #xxx预生产配置文件 │ ├── application-xxx-prod.properties #xxx生产配置文件 │ ├── application-xxx-qa.properties #xxx测试配置文件 │ ├── appspec.yml #CodeDeploy 配置文件 │ └── scripts #CodeDeploy调用的脚本文件 │ ├── apparchive.sh │ └── starttomcat.sh 具体内容展示\n配置文件不再展示\n1 2 3 4 5 6 7 8 9 10 11 12 13 # cat appspec.yml version: 0.0 os: linux files: - source: xxx.war destination: /home/bsh/tools/apache-tomcat-8.5.23/webapps - source: application-backend.properties destination: /home/bsh/tools/apache-tomcat-8.5.23/webapps hooks: BeforeInstall: - location: scripts/apparchive.sh ApplicationStart: - location: scripts/starttomcat.sh 1 2 3 4 [root@ip-10-0-20-89 scripts]# cat apparchive.sh #!/bin/bash mv -f /home/bsh/tools/apache-tomcat-8.5.23/webapps/bshrexxxcipes.war /home/bsh/backup/tomcatback/xxx.war.old mv -f /home/bsh/tools/apache-tomcat-8.5.23/webapps/application-xxx.properties /home/bsh/backup/tomcatback/application-xxx.properties.old 1 2 3 # cat starttomcat.sh #!/bin/bash -x sudo systemctl restart tomcat AWS CodeDeploy 配置 在AWS 中找到CodeDeploy 服务\n选择创建应用程序\n写入应用程序名称，和EC2/本地 点击创建应用程序\n选择刚才创建的应用程序 点击部署组 选择创建部署组\n依次填入相关信息。这个是要在jenkins中用到的\n选择我们一开始创建的IAM 角色\n部署类型选择就地（这边的蓝绿部署的话要用到自动扩展这个对于目前我们的业务来说比较浪费设备）\n选择需要发布的EC2\n部署方式为一台一台发布\n启用负载均衡，发布好一台后做健康检查，如果健康检查不通过则发布停止。\n创建自由风格的CD JOB General 选择 参数化构建过程、然后依次填入一下内容\nBuildNumber 是CI构建时的构建数字\nPackageName 为你的war包名称\n源码管理 选择GIT，这里的URL是java项目配置文件的地址 选择配置文件所在的分支我这边是存放在dev分支\n构建环境 构建前删除工作空间\n构建 选择执行shell\n使用aws cli 命令把S3中的war包拿下来\n1 2 3 4 5 6 #从S3桶中拿取war包 aws s3 cp s3://cnsre.cn/JenkinsCodeDeployProject/Backend-CI-Artifact/${BuildNumber}/artifacts/target/${PackageName} backend/ --region cn-north-1 --no-progress #记录使用 echo \u0026#34;${JOB_NAME}-${BUILD_NUMBER}-Test\u0026#34; \u0026gt; backend/versionfile #配置文件重命名 cp backend/application-backend-prod.properties backend/application-backend.properties 构建后操作 依次填入CodeDeploy以及S3的配置信息\n到现在jenkins CD 已经配置完成 接下来测试下吧\n测试CD发布java工 查看上次CI构建成功的war包\n选择需要发布的BuildNumber 以及填写java 工程的war包名称 然后点击开始构建\n因为在codedeploy 中选择的是一台一台发布，并且启用的负载均衡 所以这个发布的时间会比较长，一般在10分钟以上，但是这种方式也是比较适合生产环境的，因为这样的方式是不停机部署的方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 控制台输出 Started by user SRE Running as SYSTEM Building in workspace /root/.jenkins/workspace/Backend-CD-Prod [WS-CLEANUP] Deleting project workspace... [WS-CLEANUP] Deferred wipeout is used... ... 过程略 ... Uploading zip to s3:/XXXXX/JenkinsCodeDeployProject/Backend-CD-Prod/PROD/#5-Backend-CD-Prod-5-Test.zip Registering revision for application \u0026#39;BackendCICD\u0026#39; Creating deployment with revision at {RevisionType: S3,S3Location: {Bucket: cnsre.cn,Key: JenkinsCodeDeployProject/Backend-CD-Prod/PROD/#5-Backend-CD-Prod-5-Test.zip,BundleType: zip,ETag: 5eb4e102812cc69f9c73084b06fdcfb1},} Finished: SUCCESS 验证CD部署java工程 最好的验证方法就是登录服务器查看一下\n我这边就不做验证了\n截止到此 jenkins 配合AWS codedeploy 的不停机发布已经完成。\n文章链接\nhttps://www.cnsre.cn/posts/210310131550/\n","description":"Jenkins与CodeDeploy相结合做的CICD做的蓝绿发布,CI与CD 是分开的.同时在java项目上把java代码与配置文件分离.","id":97,"section":"posts","tags":["jenkins","aws","codedeploy"],"title":"jenkins AWS CodeDeploy不停机部署","uri":"https://www.cnsre.cn/posts/210310131550/"},{"content":"本文链接\nhttps://www.cnsre.cn/posts/210303161655/\nZabbix架构 在讲Zabbix优化之前，先来看看Zabbix Server 的逻辑架构图：\n对于上图中，架构组件的描述：\nZabbix进程 Self-Monitoring：用于收集Zabbix系统内部的监控信息； Configuration syncer：用于将配置文件中的配置信息同步到内存中缓存； Timer：用于处理触发器中与时间相关的函数和维护模式的进程； History syncer：用于写历史数据表的进程； Escalator：用于处理Action中的步骤的进程； Housekeeper：用于清理过期的历史数据的进程； Db watchdog：用于监视Zabbix系统的数据库状态，当数据库状态变为不可用时，发送警告信息（服务器代理端不支持这类型进程）。 Zabbix Poller Poller：用于普通的被动监控项目的轮询； ICMP pinger：用于定期的进行ICMP PING检查； IPMI poller：用于定期进行IPMI监控项目的检查; Unreachable poller：用于轮询不可达的设备； Proxy poller：用于服务器代理的被动轮询； Trapper：用于处理主动采集、陷入以及分布式节点间或服务器代理的通信； Java poller：用于轮询Java监控项目； Http poller：用于轮询Web类的监控项目； Snmp trapper：用于轮询Snmp/trap类的监控项目； Discovery：用于自动发现设备的进程； Vmware Collector：负责从VMware服务进程中收集数据（服务器代理端不支持这种类型的进程）； Alerter：用于发送报警通知进程。 优化内容 优化Zabbix架构 常用的架构：\n说明：Zabbix最简单的架构，常用于监控主机比较少的情况下。\n分布式架构：\nServer-Proxy-Agentd模式。\n说明：Zabbix分布式架构，常用于监控主机比较多的情况下，使用Zabbix Proxy进行分布式监控，有效的减轻了Zabbix Server端的压力。\nZabbix Server/Zabbix Proxy配置优化 调整配置文件：Zabbix_Server.conf\nZabbix进程参数调整：\n1 2 3 4 5 6 StartPollers=80 StartPingers=10 StartPollersUnreachable=80 StartIPMIPollers=10 StartTrappers=20 StartDBSyncers=8 值得注意的是，当Zabbix的Pollers数量过多时（超过limit默认值1024），需要对系统的limit的参数大小进行修改。\n1 2 3 4 5 shell\u0026gt; vi /etc/security/limit.conf * hard nofile 65536 * soft nofile 65536 * hard nproc 65536 * soft nproc 65536 Zabbix In-Memory Cache参数优化（以下值仅做参考）：\nValueCacheSize=256M HistoryIndexCacheSize = 64M TrendCacheSize=64M HistoryCacheSize=128M CacheSize=128M VMwareCacheSize=64M 优化Zabbix的数据库（MySQL） 调整MySQL配置文件：my.cnf或my.ini，在[mysqld]酌情修改参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 [client] port = 3306 socket = /var/lib/mysql/mysql.sock [mysql] prompt=\u0026#34;\\u@mysqldb \\R:\\m:\\s [\\d]\u0026gt; \u0026#34; no-auto-rehash [mysqld] user= mysql port = 3306 datadir = /data/mysql/ socket = /var/lib/mysql/mysql.sock pid-file = mysqldb.pid character-set-server = utf8mb4 skip_name_resolve = 1 # Timestamp explicit_defaults_for_timestamp = 1 # Connections back_log = 1024 max_connections = 512 max_connect_errors = 5120 innodb_thread_concurrency = 16 # Limit open_files_limit = 65535 innodb_open_files = 65535 # Query Cache query_cache_type = 0 query_cache_size = 0 # binary logs server-id = 3306001 binlog_format = row log-bin = /data/mysql/mysql-binlog expire_logs_days = 7 sync_binlog = 1 max_binlog_size = 1G binlog_cache_size = 4m max_binlog_cache_size = 1G # slow query slow_query_log = 1 long_query_time = 2 slow_query_log_file = /data/mysql/slow.log # Timeout interactive_timeout = 600 wait_timeout = 600 # Engine default-storage-engine = innodb # Buffer key_buffer_size = 32M read_buffer_size = 1M sort_buffer_size = 1M read_rnd_buffer_size = 1M tmp_table_size = 32M join_buffer_size = 16M max_heap_table_size = 32M max_tmp_tables = 48 thread_cache_size = 32 # Time log_timestamps = SYSTEM # Tablespace \u0026amp; File I/O innodb_data_file_path = ibdata1:1G:autoextend innodb_file_per_table = 1 # Redo Log innodb_flush_log_at_trx_commit = 1 innodb_log_file_size = 256M innodb_log_files_in_group = 2 innodb_log_buffer_size = 16M # Innodb innodb_buffer_pool_size = 8G innodb_buffer_pool_instances = 8 innodb_flush_method = O_DIRECT innodb_change_buffer_max_size = 50 innodb_max_dirty_pages_pct = 30 innodb_buffer_pool_load_at_startup = 1 innodb_buffer_pool_dump_at_shutdown = 1 innodb_io_capacity = 500 innodb_io_capacity_max= 1000 innodb_support_xa = 0 innodb_rollback_on_timeout = 1 优化Zabbix监控项 优化监控项的数据采集方式，由被动方式改为主动模式(Passive mode -\u0026gt; Active mode)，主动模式的优势：\n可以用户NAT到设备后面； 数据缓冲； 减轻服务器的负载，Poller轮询零负载； 更加安全，代理端不需要监测任何端口。 降低监控项的轮询时间； 删除无用的监控项。\n本文链接\nhttps://www.cnsre.cn/posts/210303161655/\n原文链接 ","description":"Zabbix系统到底该如何优化？自定义监控模板；调优数据库以获得最佳性能；调优Zabbix Server配置；根据硬件的规格设置合理的HouseKeeper；使用最新的，并且稳定的版本；对Zabbix数据库进行表分区；优化Zabbix的架构","id":98,"section":"posts","tags":["zabbix"],"title":"Zabbix 系统到底应该怎样优化？","uri":"https://www.cnsre.cn/posts/210303161655/"},{"content":"📢 快来留言发表你的看法吧~\n","description":"畅所欲言","id":99,"section":"","tags":null,"title":"留言","uri":"https://www.cnsre.cn/comment/"},{"content":"2020年前 2020年前一直都使用有道云笔记来编写记录文档。\n2020年 2020-04 2020-04-28 入住博客园，发布80多篇文章笔记。\n2020-08 2020-08-03 搭建 Wordpress 站点。\n2021年 2021-02 2021-02-04 购入域名 cdops.cn。\n放弃 wordpress 开始使用 hugo 搭建个人博客。\n2021-03 2021-03-01 购入域名 cnsre.cn，zsre.cn。\n2021-03-08 博客调整完毕，SRE运维博客 正式上线。\n2021-03-17 博客首页 SRE运维博客 被百度首次收录。\n2021-03-17 增加博客 时间线 功能。\n2021-03-31 增强博客评论功能。\n2021-07 2021-07-07 博客转入国内并备案成功。\n2021-07-23 升级 hugo v0.79.0 至 v0.85.0。\n2021-09 2021-09-24 改版博客 时间线功能。\n2021-09-25 博客加入 CDN 功能。\n2021-11 2021-11-11 博客迁移至 kubernetes，并使用 jenkins 自动构建。\n2021-11-25 个人网盘 SRE网盘 上线。\n2022-03 2022-03-04 博客评论系统 Valine 切换至 Waline。\n2022-03-07 博客评论数据迁移完成。\n优化博客部分特效：鼠标点击特效、顶部跑马灯等。\n","description":"关于SRE运维博客,SRE运维博客时间线,SRE运维博客历史,SRE运维博客是一个专注于SRE、DevOps、自动化运维、Kubernetes、Docker的个人运维博客。博客主要有系统运维、脚本编程、监控、devops等内容，本站涵盖Linux系统运维、自动化运维、监控、脚本、容器、运维经验、云计算、虚拟化等内容。","id":100,"section":"","tags":null,"title":"SRE运维博客时间线","uri":"https://www.cnsre.cn/timeline/"},{"content":"👋嗨 ，欢迎来到SRE运维博客网站！我是一名SRE运维工程师，拥有多年的云平台使用和系统架构设计经验，擅长使用各种云计算技术和工具解决故障问题，保证系统的稳定性和可用性。在这里，我将与大家分享我的运维经验和技术。\n关于博主 作为一名运维工程师，我拥有丰富的云平台使用经验，包括 AWS、阿里云、腾讯云等。我熟练掌握各种云计算技术和工具。此外，我还具备快速解决故障问题的能力，并能够使用监控和自动化工具保证系统的稳定性和可用性。\n在运维方面，我具有优秀的系统架构设计能力，能够实施高可用、高效率的云运维环境。另外，我还具备良好的团队协作能力和沟通能力，能够与开发人员、项目经理等团队成员协调配合。\n🧰 常用的工具\n关于博客 我的博客是我在运维领域的经验总结和技术分享的平台。博客框架是基于Hugo v0.85.0构建，目前运行在Kubernetes上，并使用Hugo 主题 Zzo。\n博客最初是基于GitHub Pages创建的，但由于国内访问较慢，我使用了GitHub Pages + Vercel + Cloudflare的方式，并购买了域名备案到了国内，并使用国内的服务器。\n博客发展历史如下：\n2020年04月：入住博客园，发布80多篇文章笔记。 2020年08月：搭建 Wordpress 站点。 2021年02月：购入域名cdops.cn，放弃wordpress开始使用hugo搭建个人博客。 2021年03月：购入域名cnsre.cn，zsre.cn，并正式上线SRE运维博客。 2021年03月：博客首页被百度首次收录，增加博客时间线功能，增强博客评论功能。 2021年07月：博客转入国内并备案成功，升级hugo至 v0.85.0。 2021年09月：改版博客时间线功能，博客加入CDN功能。 2021年11月：博客迁移至kubernetes，并使用jenkins自动构建。个人网盘SRE网盘上线。 2022年03月：博客评论系统Valine切换至Waline，优化博客部分特效，包括鼠标点击特效和顶部跑马灯等。 2023年02月：博客上线Chat-GPT WEB 版本。 2023年05月：更新留留言板页面。 ","description":"SRE运维博客网站的特点和优势介绍。博主的个人简介和背景介绍，帮助读者更好了解博主。","id":101,"section":"","tags":null,"title":"SRE运维博客网站介绍","uri":"https://www.cnsre.cn/about/"},{"content":"友链申请说明 本站属于网络资源分享、技术交流站点，欢迎站长们申请友链添加！ 友链申请优先于技术分享类博客站点友链。 申请链接前请先添加本博链接。 不定时进行回访友链。 如遇到长时间无响应或涉及违规内容将终止友链，熟不另行通知! 申请友链，请在本站 留言板 中留言内容格式如下： 网站名称：SRE运维博客\n网站地址：www.cnsre.cn\n网站说明：专注SRE运维技术分享的博客\n网站Logo：https://www.cnsre.cn/favicon.ico\n友情链接 ","description":"","id":102,"section":"","tags":null,"title":"友情链接","uri":"https://www.cnsre.cn/friends/"}]